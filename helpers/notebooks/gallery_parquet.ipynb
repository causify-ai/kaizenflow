{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895cb286",
   "metadata": {},
   "source": [
    "Show Parquet / Pyarrow API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068d525",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import helpers.hdbg as hdbg\n",
    "import helpers.hio as hio\n",
    "\n",
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ff89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create pandas random data, like:\n",
    "\n",
    "    ```\n",
    "                idx instr  val1  val2\n",
    "    2000-01-01    0     A    99    30\n",
    "    2000-01-02    0     A    54    46\n",
    "    2000-01-03    0     A    85    86\n",
    "    ```\n",
    "    \"\"\"\n",
    "    instruments = \"A B C D E\".split()\n",
    "    \"id stock val1 val2\".split()\n",
    "    df_idx = pd.date_range(\n",
    "        pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-15\"), freq=\"1D\"\n",
    "    )\n",
    "    # print(df_idx)\n",
    "    random.seed(1000)\n",
    "\n",
    "    df = []\n",
    "    for idx, inst in enumerate(instruments):\n",
    "        df_tmp = pd.DataFrame(\n",
    "            {\n",
    "                \"idx\": idx,\n",
    "                \"instr\": inst,\n",
    "                \"val1\": [random.randint(0, 100) for k in range(len(df_idx))],\n",
    "                \"val2\": [random.randint(0, 100) for k in range(len(df_idx))],\n",
    "            },\n",
    "            index=df_idx,\n",
    "        )\n",
    "        # print(df_tmp)\n",
    "        df.append(df_tmp)\n",
    "    df = pd.concat(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8235d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_str(df: pd.DataFrame) -> str:\n",
    "    txt = \"\"\n",
    "    txt += \"# df=\\n%s\" % df.head(3)\n",
    "    txt += \"\\n# df.shape=\\n%s\" % str(df.shape)\n",
    "    txt += \"\\n# df.dtypes=\\n%s\" % str(df.dtypes)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc474b",
   "metadata": {},
   "source": [
    "# Save and load all data in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb399156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "# print(df.head())\n",
    "print(df_to_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "print(\"table=\\n%s\" % table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save.\n",
    "file_name = \"df_in_one_file.pq\"\n",
    "pq.write_table(table, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load.\n",
    "df2 = pq.read_table(file_name)\n",
    "print(df2)\n",
    "\n",
    "df2 = df2.to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098757c",
   "metadata": {},
   "source": [
    "## Read a subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pq.read_table(file_name, columns=[\"idx\", \"val1\"])\n",
    "print(df2)\n",
    "\n",
    "df2 = df2.to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012cebdb",
   "metadata": {},
   "source": [
    "## Partitioned dataset\n",
    "\n",
    "from https://arrow.apache.org/docs/python/dataset.html#reading-partitioned-data\n",
    "\n",
    "- A dataset can exploit a nested structure, where the sub-dir names hold information about which subset of the data is stored in that dir\n",
    "- E.g., \"Hive\" patitioning scheme \"key=vale\" dir names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "print(df_to_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \".\"\n",
    "dir_name = os.path.join(base, \"parquet_dataset_partitioned\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "pq.write_to_dataset(table, dir_name, partition_cols=[\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls parquet_dataset_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64394b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read everything.\n",
    "df2 = dataset.to_table().to_pandas()\n",
    "\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load part of the data.\n",
    "\n",
    "df2 = dataset.to_table(filter=ds.field(\"idx\") == 1).to_pandas()\n",
    "print(df_to_str(df2))\n",
    "\n",
    "df2 = dataset.to_table(filter=ds.field(\"idx\") < 3).to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c27848",
   "metadata": {},
   "source": [
    "## Add year-month partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "df[\"year\"] = df.index.year\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "print(df_to_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "print(\"table=\\n%s\" % table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \".\"\n",
    "dir_name = os.path.join(base, \"pq_partitioned2\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "pq.write_to_dataset(table, dir_name, partition_cols=[\"idx\", \"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844913cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $dir_name/idx=0/year=2000/month=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21148afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "df2 = dataset.to_table(filter=ds.field(\"idx\") == 2).to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could scan manually and create the dirs manually if we don't want to add\n",
    "# add a new dir.\n",
    "base = \".\"\n",
    "dir_name = os.path.join(base, \"parquet_dataset_partitioned2\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "schemas = []\n",
    "\n",
    "schema = pa.Table.from_pandas(df).schema\n",
    "print(schema)\n",
    "# assert 0\n",
    "# idx: int64\n",
    "# instr: string\n",
    "# val1: int64\n",
    "# val2: int64\n",
    "# year: int64\n",
    "# month: int64\n",
    "\n",
    "# grouped = df.groupby(lambda x: x.day)\n",
    "group_by_idx = df.groupby(\"idx\")\n",
    "for idx, df_tmp in group_by_idx:\n",
    "    _LOG.debug(\"idx=%s -> df.shape=%s\", idx, str(df_tmp.shape))\n",
    "    #\n",
    "    group_by_year = df_tmp.groupby(lambda x: x.year)\n",
    "    for year, df_tmp2 in group_by_year:\n",
    "        _LOG.debug(\"year=%s -> df.shape=%s\", year, str(df_tmp2.shape))\n",
    "        #\n",
    "        group_by_month = df_tmp2.groupby(lambda x: x.month)\n",
    "        for month, df_tmp3 in group_by_month:\n",
    "            _LOG.debug(\"month=%s -> df.shape=%s\", month, str(df_tmp3.shape))\n",
    "            # file_name = \"df_in_one_file.pq\"\n",
    "            # pq.write_table(table, file_name)\n",
    "            # /app/data/idx=0/year=2000/month=1/02e3265d515e4fb88ebe1a72a405fc05.parquet\n",
    "            subdir_name = os.path.join(\n",
    "                dir_name, f\"idx={idx}\", f\"year={year}\", f\"month={month}\"\n",
    "            )\n",
    "            table = pa.Table.from_pandas(df_tmp3, schema=schema)\n",
    "            schemas.append(table.schema)\n",
    "            # print(df_tmp3)\n",
    "            # print(table.schema)\n",
    "            #             pq.write_to_dataset(table,\n",
    "            #                     subdir_name, schema=schema)\n",
    "            file_name = os.path.join(subdir_name, \"df_out.pq\")\n",
    "            hio.create_enclosing_dir(file_name)\n",
    "            pq.write_table(table, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas[0] == schemas[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e49f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130cbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $dir_name/idx=0/year=2000/month=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf67ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "# https://github.com/dask/dask/issues/4194\n",
    "# src_dir = f\"{dir_name}/idx=0/year=2000/month=1\"\n",
    "src_dir = f\"{dir_name}/idx=0/year=2000\"\n",
    "dataset = ds.dataset(src_dir, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "df2 = dataset.to_table().to_pandas()\n",
    "# print(df_to_str(df2))\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4111d",
   "metadata": {},
   "source": [
    "## Partition manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b33d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.dataset import DirectoryPartitioning\n",
    "\n",
    "partitioning = DirectoryPartitioning(\n",
    "    pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int8())])\n",
    ")\n",
    "print(partitioning.parse(\"/2009/11/3\"))\n",
    "\n",
    "# partitioning.discover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /app/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/app/data\"\n",
    "\n",
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name, format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read everything.\n",
    "df2 = dataset.to_table().to_pandas()\n",
    "\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e84388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[\"instr\"].unique())\n",
    "print(df2.index)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.11.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
