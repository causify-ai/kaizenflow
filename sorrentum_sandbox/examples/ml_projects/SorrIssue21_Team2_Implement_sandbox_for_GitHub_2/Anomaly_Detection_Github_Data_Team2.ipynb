{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IgzpLf3G-wCe"
   },
   "outputs": [],
   "source": [
    "from extracted_api import *\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TAqlQdvvnLka",
    "outputId": "82088f1e-ca39-4fa5-ce4f-abee69867bc1"
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"host.docker.internal\" to address: nodename nor servname provided, or not known\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Pulling Data from Issues Table-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m issues_check_query\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM github_issues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m issues_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_db_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43missues_check_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPulling The Issues df:\u001b[39m\u001b[38;5;124m\"\u001b[39m,issues_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Pulling Data from Commits Table-\u001b[39;00m\n",
      "File \u001b[0;32m/private/tmp/Sorrentum/sorrentum_sandbox/examples/ml_projects/SorrIssue21_Team2_Implement_sandbox_for_GitHub_2/extracted_api.py:9\u001b[0m, in \u001b[0;36mget_db_connection\u001b[0;34m(query_var)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_db_connection\u001b[39m(query_var) :       \n\u001b[0;32m----> 9\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[43mpsycop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost.docker.internal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                      \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairflow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5532\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     drt_cursor\u001b[38;5;241m=\u001b[39mconnection\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     16\u001b[0m     drt_cursor\u001b[38;5;241m.\u001b[39mexecute(query_var)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not translate host name \"host.docker.internal\" to address: nodename nor servname provided, or not known\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pulling Data from Issues Table-\n",
    "issues_check_query= \"SELECT * FROM github_issues\"\n",
    "issues_df = get_db_connection(issues_check_query)\n",
    "print(\"Pulling The Issues df:\",issues_df.head(2))\n",
    "\n",
    "\n",
    "\n",
    "#Pulling Data from Commits Table-\n",
    "commits_check_query= \"SELECT * FROM github_commits\"\n",
    "commits_df = get_db_connection(commits_check_query)\n",
    "print(\"Pulling the Commits df:\",commits_df.head(2))\n",
    "\n",
    "\n",
    "#Pulling Data from Main Table-\n",
    "main_check_query= \"SELECT * FROM github_main\"\n",
    "main_df = get_db_connection(main_check_query)\n",
    "print(\"Pulling the Main df:\",main_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkH36XTrnLkc",
    "outputId": "f4336355-e6ee-404d-9b68-c9c8e29264c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on : ProspectID\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 23266\n",
      "Total number of null values : 0\n",
      "Working on : lead_creation_date\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1114\n",
      "Total number of null values : 0\n",
      "Working on : program\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 3\n",
      "Total number of null values : 0\n",
      "Working on : activity_date\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 25306\n",
      "Total number of null values : 0\n",
      "Working on : keyid\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 25505\n",
      "Total number of null values : 0\n",
      "Working on : Status\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 18\n",
      "Total number of null values : 0\n",
      "Working on : msource\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 121\n",
      "Total number of null values : 0\n",
      "Working on : mx_City\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1397\n",
      "Total number of null values : 0\n",
      "Working on : mx_Application_Segment\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 4\n",
      "Total number of null values : 0\n",
      "Working on : work_exp\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 4\n",
      "Total number of null values : 0\n",
      "Working on : Vertical\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1\n",
      "Total number of null values : 0\n",
      "Working on : age\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 4\n",
      "Total number of null values : 0\n",
      "Working on : task_type\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 0\n",
      "Total number of null values : 25505\n",
      "Working on : stage_type\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 0\n",
      "Total number of null values : 25505\n",
      "Working on : calling_status2\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 19\n",
      "Total number of null values : 2328\n",
      "Working on : enrolment_datetime_ist\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 41\n",
      "Total number of null values : 25464\n",
      "Working on : activity_month\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 12\n",
      "Total number of null values : 0\n",
      "Working on : lead_age_day\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1006\n",
      "Total number of null values : 0\n",
      "Working on : Task_driven_flag\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1\n",
      "Total number of null values : 0\n",
      "Working on : Past_reactivated\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : Days_Since_Last_Task\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 592\n",
      "Total number of null values : 0\n",
      "Working on : Task_Completed_Per\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 141\n",
      "Total number of null values : 0\n",
      "Working on : Task_Breached_Per\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 234\n",
      "Total number of null values : 0\n",
      "Working on : Activity_Closed_Task\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1\n",
      "Total number of null values : 0\n",
      "Working on : duration_secs\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2311\n",
      "Total number of null values : 2602\n",
      "Working on : Calls_made\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 89\n",
      "Total number of null values : 0\n",
      "Working on : TT(Mins)\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2311\n",
      "Total number of null values : 0\n",
      "Working on : Connected_Calls\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 38\n",
      "Total number of null values : 0\n",
      "Working on : Incoming_Calls\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 30\n",
      "Total number of null values : 0\n",
      "Working on : Incoming_Connected\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 19\n",
      "Total number of null values : 0\n",
      "Working on : Incoming_TT(Mins)\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 979\n",
      "Total number of null values : 0\n",
      "Working on : AHT\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1425\n",
      "Total number of null values : 7955\n",
      "Working on : called_after\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 975\n",
      "Total number of null values : 0\n",
      "Working on : PDE_Done\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : CNC_CBLDone\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : NE_NIDone\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : HWC_Done\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : connected_after\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 735\n",
      "Total number of null values : 12338\n",
      "Working on : App\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : Offer\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : Page_Visits\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 104\n",
      "Total number of null values : 0\n",
      "Working on : enroled_in_days\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 19\n",
      "Total number of null values : 25464\n",
      "Working on : Enrolment_Month\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 10\n",
      "Total number of null values : 25464\n",
      "Working on : Enrolment\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 2\n",
      "Total number of null values : 0\n",
      "Working on : Score_Pre\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 599\n",
      "Total number of null values : 80\n",
      "Working on : EngagementScore_Pre\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 82\n",
      "Total number of null values : 80\n",
      "Working on : Promised_Discount\n",
      "Total number of values : 25505\n",
      "Total number of unique values : 1\n",
      "Total number of null values : 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### code for catagorical data\n",
    "data_file = data.select_dtypes(exclude =np.number)\n",
    "def count_stats(column_data):\n",
    "    total_count = column_data.size\n",
    "    unique_count = column_data.nunique()\n",
    "    null_count = column_data.isnull().sum()\n",
    "\n",
    "    ## printing column counts\n",
    "    print('Total number of values : {}'.format(total_count))\n",
    "    print('Total number of unique values : {}'.format(unique_count))\n",
    "    print('Total number of null values : {}'.format(null_count))\n",
    "\n",
    "    return total_count, unique_count, null_count\n",
    "\n",
    "discrete_dataFrame_columns = ['Column Name', 'total_count', 'unique_count', 'null_count', '% null', 'levels',\n",
    "                                  'value_count', '% counts']\n",
    "discrete_dataFrame = pd.DataFrame(columns=discrete_dataFrame_columns)\n",
    "\n",
    "def count_stats(column_data):\n",
    "    total_count = column_data.size\n",
    "    unique_count = column_data.nunique()\n",
    "    null_count = column_data.isnull().sum()\n",
    "\n",
    "    ## printing column counts\n",
    "    print('Total number of values : {}'.format(total_count))\n",
    "    print('Total number of unique values : {}'.format(unique_count))\n",
    "    print('Total number of null values : {}'.format(null_count))\n",
    "\n",
    "    return total_count, unique_count, null_count\n",
    "for column in data_file.columns:\n",
    "\n",
    "        print(\"Working on : \" + column)\n",
    "\n",
    "        column_data = data_file[column]\n",
    "        total_count, unique_count , null_count = count_stats(column_data)\n",
    "        if(total_count!=0):\n",
    "            null_per = float(null_count)/float(total_count)*100\n",
    "        else:\n",
    "            null_per = 0\n",
    "\n",
    "        #column_type,column_data = get_column_type(column_data)\n",
    "\n",
    "\n",
    "        value_counts = pd.DataFrame(column_data.value_counts())\n",
    "        value_counts = value_counts.reset_index()\n",
    "        value_counts.insert(loc=0,column='Column Name',value=column)\n",
    "        value_count_columns = ['Column Name','levels', 'value_count']\n",
    "        value_counts.columns = value_count_columns\n",
    "        value_counts = value_counts.sort_values(by='value_count', ascending=False)\n",
    "        value_counts_top = value_counts[:10].copy()\n",
    "        top_frequency = value_counts_top.value_count.sum()\n",
    "        others_frequency = value_counts[10:].value_count.sum()\n",
    "\n",
    "\n",
    "        temp_column_names = ['Column Name','total_count','unique_count','null_count','% null']\n",
    "        temp_values = [[column,total_count,unique_count,null_count,null_per]]\n",
    "        temp_dataFrame = pd.DataFrame(columns=temp_column_names,data=temp_values)\n",
    "\n",
    "        ## create value counts dataframe with others column\n",
    "        value_counts_top = value_counts_top.append(pd.DataFrame(columns=value_count_columns,data=[[column,'others',others_frequency]]))\n",
    "        ## add null level\n",
    "        value_counts_top = value_counts_top.append(pd.DataFrame(columns=value_count_columns,data=[[column,'nulls',null_count]]))\n",
    "        ## add % counts\n",
    "\n",
    "        if(total_count!=0):\n",
    "                value_counts_top['% counts'] = value_counts_top.value_count/float(total_count) * 100\n",
    "        else:\n",
    "                value_counts_top['% counts'] = 0\n",
    "\n",
    "            ## merge temp_dataframe and value_counts_top, then insert into discrete_dataframe\n",
    "        temp_discrete_dataFrame = temp_dataFrame.merge(how='left',left_on='Column Name',right_on='Column Name',right=value_counts_top)\n",
    "\n",
    "        discrete_dataFrame = discrete_dataFrame.append(temp_discrete_dataFrame)\n",
    "discrete_dataFrame_cat = discrete_dataFrame.copy()\n",
    "discrete_dataFrame_cat['Column_Type']=['Categorical']*len(discrete_dataFrame_cat)        \n",
    "discrete_dataFrame_cat = discrete_dataFrame_cat[['Column_Type','Column Name', 'total_count', 'unique_count', 'null_count', '% null',\n",
    "       'levels', 'value_count', '% counts']]\n",
    "        \n",
    "#### code for cont. data\n",
    "data_file = data.select_dtypes(include =np.number)\n",
    "def count_stats(column_data):\n",
    "    total_count = column_data.size\n",
    "    unique_count = column_data.nunique()\n",
    "    null_count = column_data.isnull().sum()\n",
    "\n",
    "    ## printing column counts\n",
    "    print('Total number of values : {}'.format(total_count))\n",
    "    print('Total number of unique values : {}'.format(unique_count))\n",
    "    print('Total number of null values : {}'.format(null_count))\n",
    "\n",
    "    return total_count, unique_count, null_count\n",
    "\n",
    "discrete_dataFrame_columns = ['Column Name', 'total_count', 'unique_count', 'null_count', '% null', 'levels',\n",
    "                                  'value_count', '% counts']\n",
    "discrete_dataFrame = pd.DataFrame(columns=discrete_dataFrame_columns)\n",
    "\n",
    "def count_stats(column_data):\n",
    "    total_count = column_data.size\n",
    "    unique_count = column_data.nunique()\n",
    "    null_count = column_data.isnull().sum()\n",
    "\n",
    "    ## printing column counts\n",
    "    print('Total number of values : {}'.format(total_count))\n",
    "    print('Total number of unique values : {}'.format(unique_count))\n",
    "    print('Total number of null values : {}'.format(null_count))\n",
    "\n",
    "    return total_count, unique_count, null_count\n",
    "for column in data_file.columns:\n",
    "\n",
    "        print(\"Working on : \" + column)\n",
    "\n",
    "        column_data = data_file[column]\n",
    "        total_count, unique_count , null_count = count_stats(column_data)\n",
    "        if(total_count!=0):\n",
    "            null_per = float(null_count)/float(total_count)*100\n",
    "        else:\n",
    "            null_per = 0\n",
    "\n",
    "        #column_type,column_data = get_column_type(column_data)\n",
    "\n",
    "\n",
    "        value_counts = pd.DataFrame(column_data.value_counts())\n",
    "        value_counts = value_counts.reset_index()\n",
    "        value_counts.insert(loc=0,column='Column Name',value=column)\n",
    "        value_count_columns = ['Column Name','levels', 'value_count']\n",
    "        value_counts.columns = value_count_columns\n",
    "        value_counts = value_counts.sort_values(by='value_count', ascending=False)\n",
    "        value_counts_top = value_counts[:10].copy()\n",
    "        top_frequency = value_counts_top.value_count.sum()\n",
    "        others_frequency = value_counts[10:].value_count.sum()\n",
    "\n",
    "\n",
    "        temp_column_names = ['Column Name','total_count','unique_count','null_count','% null']\n",
    "        temp_values = [[column,total_count,unique_count,null_count,null_per]]\n",
    "        temp_dataFrame = pd.DataFrame(columns=temp_column_names,data=temp_values)\n",
    "\n",
    "        ## create value counts dataframe with others column\n",
    "        value_counts_top = value_counts_top.append(pd.DataFrame(columns=value_count_columns,data=[[column,'others',others_frequency]]))\n",
    "        ## add null level\n",
    "        value_counts_top = value_counts_top.append(pd.DataFrame(columns=value_count_columns,data=[[column,'nulls',null_count]]))\n",
    "        ## add % counts\n",
    "\n",
    "        if(total_count!=0):\n",
    "                value_counts_top['% counts'] = value_counts_top.value_count/float(total_count) * 100\n",
    "        else:\n",
    "                value_counts_top['% counts'] = 0\n",
    "\n",
    "            ## merge temp_dataframe and value_counts_top, then insert into discrete_dataframe\n",
    "        temp_discrete_dataFrame = temp_dataFrame.merge(how='left',left_on='Column Name',right_on='Column Name',right=value_counts_top)\n",
    "\n",
    "        discrete_dataFrame = discrete_dataFrame.append(temp_discrete_dataFrame)\n",
    "df=data_file.describe(percentiles=[.1,.2,.25,.3,.4,.6,.7,.75,.8,.9]).T.reset_index()\n",
    "df=df.drop(labels=\"count\",axis=1)\n",
    "df2=pd.merge(discrete_dataFrame,df,how='left',left_on='Column Name',right_on='index')\n",
    "Q1 = data_file.quantile(0.25)\n",
    "Q3 = data_file.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers=pd.DataFrame(data=((data_file< (Q1 - 1.5 * IQR)) | (data_file > (Q3 + 1.5 * IQR))).sum(),columns=[\"Outliers_count\"]).reset_index()\n",
    "df3=pd.merge(df2,outliers,how='left',left_on='Column Name',right_on='index')\n",
    "df3=df3.drop(labels=['index_x','index_y'],axis=1)\n",
    "df3['Column_Type']=['Continous']*len(df3)\n",
    "df3=df3[['Column_Type','Column Name', 'total_count', 'unique_count', 'null_count', '% null',\n",
    "       'levels', 'value_count', '% counts', 'mean', 'std', 'min', '10%', '20%',\n",
    "       '25%', '30%', '40%', '50%', '60%', '70%', '75%', '80%', '90%', 'max',\n",
    "       'Outliers_count']]\n",
    "df3=df3.append(discrete_dataFrame_cat)\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
