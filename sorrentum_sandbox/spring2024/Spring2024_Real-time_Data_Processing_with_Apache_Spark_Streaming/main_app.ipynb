{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HoQ-_fX9pUL"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SparkContext and StreamingContext\n",
        "sc = SparkContext(\"local[2]\", \"RandomDataStream\")\n",
        "ssc = StreamingContext(sc, 1)  # Batch interval of 1 second\n",
        "\n",
        "def generate_data():\n",
        "    return [int(np.random.normal(140, 40)) for _ in range(100)]  # Generate 100 random numbers to represent Tweet lengths in characters\n",
        "\n",
        "def process_data(data):\n",
        "    if data.isEmpty():\n",
        "        print(\"All Batches Processed! Run Again for More Data\")\n",
        "        ssc.stop(stopSparkContext=True, stopGraceFully=True)\n",
        "    else:\n",
        "\n",
        "        # Calculate mean\n",
        "        count = data.count()\n",
        "        sum_of_data = data.reduce(lambda x, y: x + y)\n",
        "        mean = sum_of_data / count\n",
        "        print(\"New Batch Arrived!\")\n",
        "        print(\"Average characters per Tweet in this batch:\", mean)\n",
        "\n",
        "        # Calculate min and max\n",
        "        min_val = data.min()\n",
        "        max_val = data.max()\n",
        "        print(f\"Shortest Tweet in this batch: {min_val} characters\")\n",
        "        print(f\"Longest Tweet in this batch: {max_val} characters\\n\")\n",
        "\n",
        "# Create a DStream from a function that generates data\n",
        "stream = ssc.queueStream([sc.parallelize(generate_data()) for _ in range(10)])\n",
        "\n",
        "# Process the stream\n",
        "stream.foreachRDD(process_data)\n",
        "\n",
        "# Start the streaming context\n",
        "print(\"Starting Spark Streaming context...\")\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the termination of the streaming context\n",
        "print(\"Awaiting termination...\")\n",
        "ssc.awaitTermination()"
      ]
    }
  ]
}