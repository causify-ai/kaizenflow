{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izcvRstCdDHu"
   },
   "source": [
    "# HBase Data Modeling and Querying\n",
    "\n",
    "Dakeun Park\n",
    "\n",
    "120462429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkG5B8etdHQg"
   },
   "source": [
    "## 1. Designing the Schema\n",
    "\n",
    "Let's assume we're creating a database for a simple bookstore. We need tables for Books and Authors.\n",
    "\n",
    "- Books Table\n",
    "\n",
    "  Row Key: ISBN (International Standard Book Number)\n",
    "\n",
    "  Column Families:\n",
    "\n",
    "  details: General information about the book.\n",
    "\n",
    "  details:title: The title of the book.\n",
    "\n",
    "  details:author: Author ID (link to Authors table).\n",
    "\n",
    "  stock: Information about book availability.\n",
    "\n",
    "  stock:quantity: Number of copies available.\n",
    "\n",
    "- Authors Table\n",
    "\n",
    "  Row Key: Author ID\n",
    "\n",
    "  Column Families:\n",
    "\n",
    "  info: Information about the author.\n",
    "\n",
    "  info:name: Author's name.\n",
    "\n",
    "  *info*:birthdate: Author's birth date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfEp6mupdPv3"
   },
   "source": [
    "## 2. Creating Tables in HBase\n",
    "\n",
    "Connect to HBase and create tables using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "\n",
    "try:\n",
    "    connection = happybase.Connection('hbase-docker', port=9090)\n",
    "    if connection:\n",
    "        print(\"Connected to HBase.\")\n",
    "    else:\n",
    "        print(\"FAIL\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect HBase:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "yZP1fxuWdTEk",
    "outputId": "ae915fd0-133e-4bef-ec1c-932318314ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tables: ['Authors', 'Books', 'denormalized']\n",
      "'Books' table already exists.\n",
      "'Authors' table already exists.\n",
      "Updated tables list: ['Authors', 'Books', 'denormalized']\n"
     ]
    }
   ],
   "source": [
    "# Connect to HBase\n",
    "connection = happybase.Connection('hbase-docker', port=9090)\n",
    "\n",
    "# List current tables\n",
    "existing_tables = connection.tables()\n",
    "print(\"Existing tables:\", [table.decode('utf-8') for table in existing_tables])\n",
    "\n",
    "# Creating the 'Books' table if not already created\n",
    "if b'Books' not in existing_tables:\n",
    "    connection.create_table(\n",
    "        'Books',\n",
    "        {'details': dict(max_versions=1),\n",
    "         'stock': dict(max_versions=1)}\n",
    "    )\n",
    "    print(\"Created 'Books' table.\")\n",
    "else:\n",
    "    print(\"'Books' table already exists.\")\n",
    "\n",
    "# Creating the 'Authors' table if not already created\n",
    "if b'Authors' not in existing_tables:\n",
    "    connection.create_table(\n",
    "        'Authors',\n",
    "        {'info': dict(max_versions=1)}\n",
    "    )\n",
    "    print(\"Created 'Authors' table.\")\n",
    "else:\n",
    "    print(\"'Authors' table already exists.\")\n",
    "\n",
    "# Print tables to verify\n",
    "updated_tables = connection.tables()\n",
    "print(\"Updated tables list:\", [table.decode('utf-8') for table in updated_tables])\n",
    "\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvGNkv79g63t"
   },
   "source": [
    "## 3. Populating Tables with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m table \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBooks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Insert data into 'Books'\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-3-16-148410-0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails:title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSample Book Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails:author\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock:quantity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m table\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m978-3-16-148410-1\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails:title\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnother Sample Book Title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails:author\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock:quantity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Connect to 'Authors' table\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:463\u001b[0m, in \u001b[0;36mTable.put\u001b[0;34m(self, row, data, timestamp, wal)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, data, timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store data in the table.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    This method stores the data in the `data` argument for the row\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    :param wal bool: whether to write to the WAL (optional)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:137\u001b[0m, in \u001b[0;36mBatch.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:60\u001b[0m, in \u001b[0;36mBatch.send\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending batch for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m mutations on \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_count, \u001b[38;5;28mlen\u001b[39m(bms))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutateRows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmutateRowsTs(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, bms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp, {})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:228\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv\u001b[39m(\u001b[38;5;28mself\u001b[39m, _api):\n\u001b[0;32m--> 228\u001b[0m     fname, mtype, rseqid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iprot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_message_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mtype \u001b[38;5;241m==\u001b[39m TMessageType\u001b[38;5;241m.\u001b[39mEXCEPTION:\n\u001b[1;32m    230\u001b[0m         x \u001b[38;5;241m=\u001b[39m TApplicationException()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/protocol/cybin/cybin.pyx:476\u001b[0m, in \u001b[0;36mcybin.TCyBinaryProtocol.read_message_begin\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/protocol/cybin/cybin.pyx:69\u001b[0m, in \u001b[0;36mcybin.read_i32\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:65\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.c_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/transport/buffered/cybuffered.pyx:69\u001b[0m, in \u001b[0;36mthriftpy2.transport.buffered.cybuffered.TCyBufferedTransport.read_trans\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/transport/cybase.pyx:61\u001b[0m, in \u001b[0;36mthriftpy2.transport.cybase.TCyBuffer.read_trans\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/transport/socket.py:112\u001b[0m, in \u001b[0;36mTSocket.read\u001b[0;34m(self, sz)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         buff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINTR:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "connection = happybase.Connection('hbase-docker', port=9090)\n",
    "\n",
    "# Connect to 'Books' table\n",
    "table = connection.table('Books')\n",
    "\n",
    "# Insert data into 'Books'\n",
    "table.put('978-3-16-148410-0', {'details:title': 'Sample Book Title', 'details:author': '1', 'stock:quantity': '5'})\n",
    "table.put('978-3-16-148410-1', {'details:title': 'Another Sample Book Title', 'details:author': '1', 'stock:quantity': '7'})\n",
    "\n",
    "# Connect to 'Authors' table\n",
    "table = connection.table('Authors')\n",
    "\n",
    "# Insert data into 'Authors'\n",
    "table.put('1', {'info:name': 'John Doe', 'info:birthdate': '1990-01-01'})\n",
    "\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-Row Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data from Books: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:741)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:712)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getRegionLocation(ConnectionImplementation.java:594)\\n\\tat org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation(HRegionLocator.java:72)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:223)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:105)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:386)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:360)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.getRowWithColumnsTs(ThriftServerRunner.java:1137)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.getRowWithColumns(ThriftServerRunner.java:1113)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4088)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4072)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "Failed to fetch data from Authors: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Authors\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:741)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:712)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getRegionLocation(ConnectionImplementation.java:594)\\n\\tat org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation(HRegionLocator.java:72)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:223)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:105)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:386)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:360)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.getRowWithColumnsTs(ThriftServerRunner.java:1137)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.getRowWithColumns(ThriftServerRunner.java:1113)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4088)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4072)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n"
     ]
    }
   ],
   "source": [
    "# Function for querying a single data\n",
    "def fetch_and_print_row(connection, table_name, row_key):\n",
    "    try:\n",
    "        table = connection.table(table_name)\n",
    "        row = table.row(row_key)\n",
    "        # if row with the given key exists\n",
    "        if row:\n",
    "            print(f\"\\nDetails for {table_name}:\")\n",
    "            for key, value in row.items():\n",
    "                print(f\"{key.decode('utf-8')}: {value.decode('utf-8')}\")\n",
    "        else:\n",
    "            print(f\"No data found for row key: {row_key} in table: {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {table_name}: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "connection = happybase.Connection('hbase-docker', port=9090)\n",
    "fetch_and_print_row(connection, 'Books', '978-3-16-148410-0')\n",
    "fetch_and_print_row(connection, 'Authors', '1')\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Row Query Using Scans\n",
    "\n",
    "A basic scan fetches all rows in a table or within a range of row keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning table Books...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m connection \u001b[38;5;241m=\u001b[39m happybase\u001b[38;5;241m.\u001b[39mConnection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhbase-docker\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9090\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Muli-Row query given the key range.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mscan_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-3-16-148410-0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-3-16-148410-9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m connection\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mscan_table\u001b[0;34m(connection, table_name, start_key, end_key)\u001b[0m\n\u001b[1;32m      2\u001b[0m table \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScanning table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRow key: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:414\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     how_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, limit \u001b[38;5;241m-\u001b[39m n_returned)\n\u001b[0;32m--> 414\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerGetList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# scan has finished\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "def scan_table(connection, table_name, start_key=None, end_key=None):\n",
    "    table = connection.table(table_name)\n",
    "    print(f\"Scanning table {table_name}...\")\n",
    "    for key, data in table.scan(row_start=start_key, row_stop=end_key):\n",
    "        print(f\"Row key: {key.decode('utf-8')}\")\n",
    "        for column, value in data.items():\n",
    "            print(f\"  {column.decode('utf-8')}: {value.decode('utf-8')}\")\n",
    "        print(\"\")\n",
    "\n",
    "# Example usage\n",
    "connection = happybase.Connection('hbase-docker', port=9090)\n",
    "# Muli-Row query given the key range.\n",
    "scan_table(connection, 'Books', start_key='978-3-16-148410-0', end_key='978-3-16-148410-9')\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range Query with Filters\n",
    "You can refine scans further using filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning table Books with filter: SingleColumnValueFilter('details', 'author', =, 'binary:1')...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Searching for books with author id 1 \u001b[39;00m\n\u001b[1;32m     13\u001b[0m filter_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingleColumnValueFilter(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, =, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mfiltered_scan_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m connection\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mfiltered_scan_table\u001b[0;34m(connection, table_name, filter_string)\u001b[0m\n\u001b[1;32m      2\u001b[0m table \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScanning table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with filter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRow key: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:414\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     how_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, limit \u001b[38;5;241m-\u001b[39m n_returned)\n\u001b[0;32m--> 414\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerGetList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# scan has finished\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "def filtered_scan_table(connection, table_name, filter_string):\n",
    "    table = connection.table(table_name)\n",
    "    print(f\"Scanning table {table_name} with filter: {filter_string}...\")\n",
    "    for key, data in table.scan(filter=filter_string):\n",
    "        print(f\"Row key: {key.decode('utf-8')}\")\n",
    "        for column, value in data.items():\n",
    "            print(f\"  {column.decode('utf-8')}: {value.decode('utf-8')}\")\n",
    "        print(\"\")\n",
    "\n",
    "# Example usage\n",
    "connection = happybase.Connection('hbase-docker', port=9090)\n",
    "# Searching for books with author id 1 \n",
    "filter_string = \"SingleColumnValueFilter('details', 'author', =, 'binary:1')\"\n",
    "filtered_scan_table(connection, 'Books', filter_string)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRUD operations using python class\n",
    "\n",
    "In Python, we can utilize classes to implement create, read, update, and delete operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HBaseCRUD:\n",
    "    def __init__(self, host, port=9090):\n",
    "        \"\"\"\n",
    "        Initialize connection to the HBase server.\n",
    "        \"\"\"\n",
    "        self.connection = happybase.Connection(host, port)\n",
    "\n",
    "    def create_or_update(self, table_name, row_key, data):\n",
    "        \"\"\"\n",
    "        Create or update data in an HBase table.\n",
    "        \"\"\"\n",
    "        table = self.connection.table(table_name)\n",
    "        table.put(row_key, data)\n",
    "        print(f\"Data inserted/updated in {table_name} for row {row_key}\")\n",
    "\n",
    "    def read(self, table_name, row_key):\n",
    "        \"\"\"\n",
    "        Read data from an HBase table.\n",
    "        \"\"\"\n",
    "        table = self.connection.table(table_name)\n",
    "        data = table.row(row_key)\n",
    "        if data:\n",
    "            print(f\"Data retrieved from {table_name} for row {row_key}:\")\n",
    "            return {k.decode('utf-8'): v.decode('utf-8') for k, v in data.items()}\n",
    "        else:\n",
    "            print(f\"No data found for row {row_key} in table {table_name}\")\n",
    "            return None\n",
    "\n",
    "    def delete(self, table_name, row_key):\n",
    "        \"\"\"\n",
    "        Delete a row from an HBase table.\n",
    "        \"\"\"\n",
    "        table = self.connection.table(table_name)\n",
    "        table.delete(row_key)\n",
    "        print(f\"Row {row_key} deleted from {table_name}\")\n",
    "\n",
    "    def scan_table(self, table_name, start_key=None, end_key=None, filter_string=None):\n",
    "        \"\"\"Scan for rows in a table optionally within a key range and with a filter.\"\"\"\n",
    "        table = self.connection.table(table_name)\n",
    "        print(f\"Scanning table {table_name}...\")\n",
    "        rows = table.scan(row_start=start_key, row_stop=end_key, filter=filter_string)\n",
    "        result = []\n",
    "        for key, data in rows:\n",
    "            decoded_data = {k.decode('utf-8'): v.decode('utf-8') for k, v in data.items()}\n",
    "            result.append((key.decode('utf-8'), decoded_data))\n",
    "        return result\n",
    "\n",
    "    def scan_filtered_table(self, table_name, column, value, comparator='='):\n",
    "        \"\"\"Scan for rows in a table with a column filter, supporting multiple comparison operators.\"\"\"\n",
    "        table = self.connection.table(table_name)\n",
    "        filter_string = (\n",
    "            f\"SingleColumnValueFilter ('{column.split(':')[0]}', '{column.split(':')[1]}', \"\n",
    "            f\"{comparator}, 'binary:{value}', true, true)\"\n",
    "        )\n",
    "        rows = table.scan(filter=filter_string)\n",
    "        result = []\n",
    "        for key, data in rows:\n",
    "            decoded_data = {k.decode('utf-8'): v.decode('utf-8') for k, v in data.items()}\n",
    "            result.append((key.decode('utf-8'), decoded_data))\n",
    "        return result\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"\n",
    "        Close the HBase connection.\n",
    "        \"\"\"\n",
    "        self.connection.close()\n",
    "        print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class implementation\n",
    "\n",
    "Performing various Operations using the class structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Books: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Insert data into 'Books' table\u001b[39;00m\n\u001b[1;32m      4\u001b[0m book_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails:title\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Book Title\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails:author\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock:quantity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43mhbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-3-16-148410-0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbook_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Read data from 'Books' table\u001b[39;00m\n\u001b[1;32m     13\u001b[0m book \u001b[38;5;241m=\u001b[39m hbase\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBooks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m978-3-16-148410-0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mHBaseCRUD.create_or_update\u001b[0;34m(self, table_name, row_key, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mCreate or update data in an HBase table.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData inserted/updated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:463\u001b[0m, in \u001b[0;36mTable.put\u001b[0;34m(self, row, data, timestamp, wal)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, data, timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store data in the table.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    This method stores the data in the `data` argument for the row\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    :param wal bool: whether to write to the WAL (optional)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:137\u001b[0m, in \u001b[0;36mBatch.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:60\u001b[0m, in \u001b[0;36mBatch.send\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending batch for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m mutations on \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_count, \u001b[38;5;28mlen\u001b[39m(bms))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutateRows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmutateRowsTs(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, bms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp, {})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Books: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "hbase = HBaseCRUD('hbase-docker')\n",
    "\n",
    "# Insert data into 'Books' table\n",
    "book_data = {\n",
    "    'details:title': 'Sample Book Title', \n",
    "    'details:author': '1', \n",
    "    'stock:quantity': '5'\n",
    "}\n",
    "\n",
    "hbase.create_or_update('Books', '978-3-16-148410-0', book_data)\n",
    "\n",
    "# Read data from 'Books' table\n",
    "book = hbase.read('Books', '978-3-16-148410-0')\n",
    "print(book)\n",
    "\n",
    "# Update data in 'Books' table\n",
    "update_data = {\n",
    "    'stock:quantity': '10'\n",
    "}\n",
    "hbase.create_or_update('Books', '978-3-16-148410-0', update_data)\n",
    "\n",
    "# Read data from 'Books' table\n",
    "book = hbase.read('Books', '978-3-16-148410-0')\n",
    "print(book)\n",
    "\n",
    "# Delete row from 'Books' table\n",
    "hbase.delete('Books', '978-3-16-148410-0')\n",
    "\n",
    "# Close connection\n",
    "hbase.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create or Update multiple data - Books Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Books: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Inserting the book data into the 'Books' table\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m books_data:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mhbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbook\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrow_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbook\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInserted book with ISBN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Close the connection after operations\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mHBaseCRUD.create_or_update\u001b[0;34m(self, table_name, row_key, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mCreate or update data in an HBase table.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData inserted/updated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:463\u001b[0m, in \u001b[0;36mTable.put\u001b[0;34m(self, row, data, timestamp, wal)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, data, timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store data in the table.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    This method stores the data in the `data` argument for the row\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    :param wal bool: whether to write to the WAL (optional)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:137\u001b[0m, in \u001b[0;36mBatch.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:60\u001b[0m, in \u001b[0;36mBatch.send\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending batch for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m mutations on \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_count, \u001b[38;5;28mlen\u001b[39m(bms))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutateRows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmutateRowsTs(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, bms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp, {})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Books: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "# Example book data entries\n",
    "books_data = [\n",
    "    {\n",
    "        'row_key': '978-0-13-110163-0',\n",
    "        'data': {\n",
    "            'details:title': 'Introduction to Algorithms',\n",
    "            'details:author': '2',  # Assuming author ID '2' is linked in the Authors table\n",
    "            'stock:quantity': '15'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-13-595705-9',\n",
    "        'data': {\n",
    "            'details:title': 'Artificial Intelligence: A Modern Approach',\n",
    "            'details:author': '3',  # Assuming author ID '3'\n",
    "            'stock:quantity': '20'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-201-83595-3',\n",
    "        'data': {\n",
    "            'details:title': 'The C Programming Language',\n",
    "            'details:author': '4',  # Assuming author ID '4'\n",
    "            'stock:quantity': '8'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-596-52068-7',\n",
    "        'data': {\n",
    "            'details:title': 'Learning Python',\n",
    "            'details:author': '5',  # Assuming author ID '5'\n",
    "            'stock:quantity': '12'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-262-03384-8',\n",
    "        'data': {\n",
    "            'details:title': 'Algorithms Unlocked',\n",
    "            'details:author': '2',  # Thomas H. Cormen\n",
    "            'stock:quantity': '10'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-262-53305-8',\n",
    "        'data': {\n",
    "            'details:title': 'Introduction to Autonomous Robots',\n",
    "            'details:author': '3',  # Stuart Russell\n",
    "            'stock:quantity': '7'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-0-13-110362-7',\n",
    "        'data': {\n",
    "            'details:title': 'The UNIX Programming Environment',\n",
    "            'details:author': '4',  # Brian Kernighan\n",
    "            'stock:quantity': '5'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '978-1-59327-708-4',\n",
    "        'data': {\n",
    "            'details:title': 'Python Crash Course',\n",
    "            'details:author': '5',  # Mark Lutz\n",
    "            'stock:quantity': '12'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "# Initialize HBase CRUD operations for the 'Books' table\n",
    "hbase = HBaseCRUD('hbase-docker')\n",
    "\n",
    "# Inserting the book data into the 'Books' table\n",
    "for book in books_data:\n",
    "    hbase.create_or_update('Books', book['row_key'], book['data'])\n",
    "    print(f\"Inserted book with ISBN {book['row_key']}\")\n",
    "\n",
    "# Close the connection after operations\n",
    "hbase.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read using key range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning table Books...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m hbase \u001b[38;5;241m=\u001b[39m HBaseCRUD(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhbase-docker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Scan with a range of ISBNs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m selected_books \u001b[38;5;241m=\u001b[39m \u001b[43mhbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-0-13-110163-0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m978-0-201-83595-3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, data \u001b[38;5;129;01min\u001b[39;00m selected_books:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISBN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mHBaseCRUD.scan_table\u001b[0;34m(self, table_name, start_key, end_key, filter_string)\u001b[0m\n\u001b[1;32m     41\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mscan(row_start\u001b[38;5;241m=\u001b[39mstart_key, row_stop\u001b[38;5;241m=\u001b[39mend_key, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilter_string)\n\u001b[1;32m     42\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 43\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoded_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:414\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     how_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, limit \u001b[38;5;241m-\u001b[39m n_returned)\n\u001b[0;32m--> 414\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerGetList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# scan has finished\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "hbase = HBaseCRUD('hbase-docker')\n",
    "    \n",
    "# Scan with a range of ISBNs\n",
    "selected_books = hbase.scan_table('Books', start_key='978-0-13-110163-0', end_key='978-0-201-83595-3')\n",
    "for key, data in selected_books:\n",
    "    print(f\"ISBN: {key}\")\n",
    "    for column, value in data.items():\n",
    "        print(f\"  {column}: {value}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Close connection after operations\n",
    "hbase.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create or Update Multiple data - Authors table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Authors: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Inserting the authors data into the 'Authors' table\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m authors_data:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mhbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAuthors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrow_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Close connection\u001b[39;00m\n\u001b[1;32m     40\u001b[0m hbase\u001b[38;5;241m.\u001b[39mclose_connection()\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mHBaseCRUD.create_or_update\u001b[0;34m(self, table_name, row_key, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mCreate or update data in an HBase table.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData inserted/updated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:463\u001b[0m, in \u001b[0;36mTable.put\u001b[0;34m(self, row, data, timestamp, wal)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, data, timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store data in the table.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    This method stores the data in the `data` argument for the row\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    :param wal bool: whether to write to the WAL (optional)\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:137\u001b[0m, in \u001b[0;36mBatch.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mand\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/batch.py:60\u001b[0m, in \u001b[0;36mBatch.send\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending batch for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m mutations on \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_count, \u001b[38;5;28mlen\u001b[39m(bms))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutateRows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmutateRowsTs(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname, bms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestamp, {})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: Authors: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1227)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:455)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:553)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRowsTs(ThriftServerRunner.java:1471)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.mutateRows(ThriftServerRunner.java:1407)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4308)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4292)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "# Sample authors data\n",
    "authors_data = [\n",
    "    {\n",
    "        'row_key': '2',\n",
    "        'data': {\n",
    "            'info:name': 'Thomas H. Cormen',\n",
    "            'info:birthdate': '1956-02-24'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '3',\n",
    "        'data': {\n",
    "            'info:name': 'Stuart Russell',\n",
    "            'info:birthdate': '1962-05-03'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '4',\n",
    "        'data': {\n",
    "            'info:name': 'Brian Kernighan',\n",
    "            'info:birthdate': '1942-01-01'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'row_key': '5',\n",
    "        'data': {\n",
    "            'info:name': 'Mark Lutz',\n",
    "            'info:birthdate': '1956-01-01'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "hbase = HBaseCRUD('hbase-docker')\n",
    "\n",
    "# Inserting the authors data into the 'Authors' table\n",
    "for author in authors_data:\n",
    "    hbase.create_or_update('Authors', author['row_key'], author['data'])\n",
    "\n",
    "# Close connection\n",
    "hbase.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read with conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books with Author id 2:\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Author id = 2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBooks with Author id 2:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m filtered_books \u001b[38;5;241m=\u001b[39m \u001b[43mhbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_filtered_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBooks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails:author\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, data \u001b[38;5;129;01min\u001b[39;00m filtered_books:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISBN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mHBaseCRUD.scan_filtered_table\u001b[0;34m(self, table_name, column, value, comparator)\u001b[0m\n\u001b[1;32m     55\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mscan(\u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilter_string)\n\u001b[1;32m     56\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 57\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoded_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:414\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     how_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, limit \u001b[38;5;241m-\u001b[39m n_returned)\n\u001b[0;32m--> 414\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerGetList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# scan has finished\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "hbase = HBaseCRUD('hbase-docker')\n",
    "\n",
    "# Author id = 2\n",
    "print(\"Books with Author id 2:\")\n",
    "filtered_books = hbase.scan_filtered_table('Books', 'details:author', '2', '=')\n",
    "for key, data in filtered_books:\n",
    "    print(f\"ISBN: {key}\")\n",
    "    for column, value in data.items():\n",
    "        print(f\"  {column}: {value}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Make sure the connection is closed properly after operations\n",
    "hbase.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating join operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     books_with_authors \u001b[38;5;241m=\u001b[39m \u001b[43mget_books_with_authors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m books_with_authors:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISBN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISBN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Title: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Author: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor Name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Birthdate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor Birthdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Quantity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mget_books_with_authors\u001b[0;34m(connection)\u001b[0m\n\u001b[1;32m     17\u001b[0m books \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mscan()\n\u001b[1;32m     18\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbooks\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauthor_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails:author\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauthor_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor_birthdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_author_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/happybase/table.py:414\u001b[0m, in \u001b[0;36mTable.scan\u001b[0;34m(self, row_start, row_stop, row_prefix, columns, filter, timestamp, include_timestamp, batch_size, scan_batching, limit, sorted_columns, reverse)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     how_many \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, limit \u001b[38;5;241m-\u001b[39m n_returned)\n\u001b[0;32m--> 414\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscannerGetList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# scan has finished\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:214\u001b[0m, in \u001b[0;36mTClient._req\u001b[0;34m(self, _api, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# wait result only if non-oneway\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneway\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/thriftpy2/thrift.py:248\u001b[0m, in \u001b[0;36mTClient._recv\u001b[0;34m(self, _api)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m v\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# no throws & not void api\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIOError\u001b[0m: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: Books\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:860)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:755)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)\\n\\tat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:191)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)\\n\\tat org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)\\n\\tat org.apache.hadoop.hbase.client.ResultScanner.next(ResultScanner.java:97)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.scannerGetList(ThriftServerRunner.java:1537)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:66)\\n\\tat com.sun.proxy.$Proxy10.scannerGetList(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4707)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$scannerGetList.getResult(Hbase.java:4691)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:293)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')"
     ]
    }
   ],
   "source": [
    "\"\"\"Connect to the HBase server.\"\"\"\n",
    "connection = happybase.Connection('hbase-docker', 9090)\n",
    "\n",
    "def get_author_details(connection, author_id):\n",
    "    \"\"\"Fetch author details by author ID from the Authors table.\"\"\"\n",
    "    table = connection.table('Authors')\n",
    "    row = table.row(author_id.encode('utf-8'))\n",
    "    if row:\n",
    "        author_name = row[b'info:name'].decode('utf-8') if b'info:name' in row else \"Unknown Author\"\n",
    "        author_birthdate = row[b'info:birthdate'].decode('utf-8') if b'info:birthdate' in row else \"Unknown Birthdate\"\n",
    "        return author_name, author_birthdate\n",
    "    return \"Unknown Author\", \"Unknown Birthdate\"\n",
    "\n",
    "def get_books_with_authors(connection):\n",
    "    \"\"\"Fetch all books and enrich them with author details from the Authors table.\"\"\"\n",
    "    table = connection.table('Books')\n",
    "    books = table.scan()\n",
    "    results = []\n",
    "    for key, data in books:\n",
    "        author_id = data[b'details:author'].decode('utf-8')\n",
    "        author_name, author_birthdate = get_author_details(connection, author_id)\n",
    "        book_info = {\n",
    "            'ISBN': key.decode('utf-8'),\n",
    "            'Title': data[b'details:title'].decode('utf-8'),\n",
    "            'Author ID': author_id,\n",
    "            'Author Name': author_name,\n",
    "            'Author Birthdate': author_birthdate,\n",
    "            'Quantity': data[b'stock:quantity'].decode('utf-8')\n",
    "        }\n",
    "        results.append(book_info)\n",
    "    return results\n",
    "\n",
    "try:\n",
    "    books_with_authors = get_books_with_authors(connection)\n",
    "    for book in books_with_authors:\n",
    "        print(f\"ISBN: {book['ISBN']}, Title: {book['Title']}, Author: {book['Author Name']}, Birthdate: {book['Author Birthdate']}, Quantity: {book['Quantity']}\")\n",
    "finally:\n",
    "    \"\"\"Close the connection to the HBase server.\"\"\"\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment with Data Modeling\n",
    "\n",
    "*Denormalization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denormalized_table(hbase_connection):\n",
    "    \"\"\"\n",
    "    Creates a 'denormalized' table in HBase with specified column families,\n",
    "    if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "    hbase_connection (happybase.Connection): The connection to HBase.\n",
    "    \"\"\"\n",
    "    table_name = 'denormalized'\n",
    "    families = ['book_details', 'author_details']\n",
    "    family_options = {\n",
    "        'book_details': dict(max_versions=1),\n",
    "        'author_details': dict(max_versions=1)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Check if the table already exists\n",
    "        if table_name.encode('utf-8') not in hbase_connection.tables():\n",
    "            # Define column families with options\n",
    "            families_dict = {fam: family_options.get(fam, dict()) for fam in families}\n",
    "            # Create the table with specified column families and their options\n",
    "            hbase_connection.create_table(table_name, families_dict)\n",
    "            print(f\"Table '{table_name}' created with families {list(families_dict.keys())}\")\n",
    "        else:\n",
    "            print(f\"Table '{table_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create table '{table_name}': {str(e)}\")\n",
    "\n",
    "def populate_denormalized_table(connection):\n",
    "    table = connection.table('Books')\n",
    "    denormalized_table = connection.table('denormalized')\n",
    "    books = table.scan()\n",
    "    for key, data in books:\n",
    "        isbn = key.decode('utf-8')\n",
    "        author_id = data[b'details:author'].decode('utf-8')\n",
    "        author_name, author_birthdate = get_author_details(connection, author_id)\n",
    "\n",
    "        # Prepare the data to insert into the denormalized table\n",
    "        book_data = {\n",
    "            b'book_details:title': data[b'details:title'],\n",
    "            b'book_details:quantity': data[b'stock:quantity']\n",
    "        }\n",
    "        author_data = {\n",
    "            b'author_details:name': author_name.encode('utf-8'),\n",
    "            b'author_details:birthdate': author_birthdate.encode('utf-8')\n",
    "        }\n",
    "\n",
    "        # Combine book and author data into a single dictionary for insertion\n",
    "        combined_data = {**book_data, **author_data}\n",
    "        denormalized_table.put(isbn.encode('utf-8'), combined_data)\n",
    "\n",
    "    print(\"Populated the denormalized table with book and author details.\")\n",
    "\n",
    "connection = happybase.Connection('hbase-docker', 9090)\n",
    "\n",
    "# Create the denormalized table if it doesn't exist\n",
    "create_denormalized_table(connection)\n",
    "\n",
    "# Populate the denormalized table with data\n",
    "populate_denormalized_table(connection)\n",
    "\n",
    "# Close the connection after operations\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the denormalized table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_denormalized_table(connection):\n",
    "    \"\"\"Reads all entries from the 'denormalized' table and prints them.\"\"\"\n",
    "    table = connection.table('denormalized')\n",
    "    print(\"Reading data from 'denormalized' table...\")\n",
    "    for key, data in table.scan():\n",
    "        isbn = key.decode('utf-8')\n",
    "        title = data.get(b'book_details:title', b'').decode('utf-8')\n",
    "        quantity = data.get(b'book_details:quantity', b'').decode('utf-8')\n",
    "        author_name = data.get(b'author_details:name', b'').decode('utf-8')\n",
    "        author_birthdate = data.get(b'author_details:birthdate', b'').decode('utf-8')\n",
    "\n",
    "        print(f\"ISBN: {isbn}\")\n",
    "        print(f\"  Title: {title}\")\n",
    "        print(f\"  Quantity: {quantity}\")\n",
    "        print(f\"  Author Name: {author_name}\")\n",
    "        print(f\"  Author Birthdate: {author_birthdate}\")\n",
    "        print(\"\")\n",
    "\n",
    "# Usage Example\n",
    "connection = happybase.Connection('hbase-docker', 9090)\n",
    "try:\n",
    "    read_denormalized_table(connection)\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
