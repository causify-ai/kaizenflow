{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Spring2024_Streaming_Word_Count_with_Apache_Spark_Streaming.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1kfEMsZrlHged3z4gbn9yznFHDvi5QGMI\n",
    "\"\"\"\n",
    "### Streaming Word Count with Apache Spark Streaming\n",
    "**This project focuses on real-time word counting using Apache Spark Streaming, a powerful framework for processing and analyzing streaming data. In this scenario, we aim to continuously process incoming text data streams, perform word counting, and visualize the results in real-time. Here's an overview of the functionalities and testing scenarios for this project:**\n",
    "\n",
    "Functionality Overview:\n",
    "1. Data Streaming: Apache Spark Streaming ingests data streams from a source, such as a file system, in mini-batches, allowing for continuous processing.\n",
    "2. Word Counting: The streaming application tokenizes the text data, removes stop words, and calculates the frequency of each word in the stream.\n",
    "3. Real-time Visualization: The word counts are visualized dynamically, enabling users to observe changes in word frequencies as new data arrives.\n",
    "4. Continuous Processing: The streaming application operates indefinitely, processing incoming data streams in real-time without interruption.\n",
    "\n",
    "Testing Rate Limiting:\n",
    "To test rate limiting, we can simulate a high influx of text data by continuously feeding the streaming application with a large volume of text. We can monitor the application's behavior to ensure that it handles the incoming data stream efficiently without overwhelming the system. Additionally, we can introduce delays or throttling mechanisms to observe how the application responds to varying data rates.\n",
    "\n",
    "Database Change Monitoring:\n",
    "While Apache Spark Streaming is primarily designed for processing real-time data streams, we can integrate it with database monitoring systems to track changes in underlying data sources. For example, we can monitor updates to a database table containing text data and trigger the streaming application to reprocess the affected data streams accordingly. This ensures that the streaming application remains synchronized with the underlying data source and reflects any changes in real-time.\n",
    "\n",
    "Additional Features:\n",
    "We can enhance the streaming word count application by incorporating additional features such as fault tolerance, windowed computations, stateful processing, and integration with external systems for data ingestion and output. Experimenting with these features allows us to explore the full capabilities of Apache Spark Streaming and tailor the application to specific use cases and requirements.\n",
    "\n",
    "This below section of code is performing text preprocessing and analysis on lyrics data obtained from a CSV file. It returns these metrics as word_count, average_word_length, most_common_word, most_common_word_count, and unique_word_count.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Function to read CSV file\n",
    "def read_csv_file(file_path):\n",
    "    lyrics = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip header row\n",
    "        for row in csv_reader:\n",
    "            lyrics.append(row[4])  # Assuming lyrics are in the 5th column\n",
    "    return lyrics\n",
    "\n",
    "# Function to preprocess text (tokenize, filter, remove stop words)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Assuming words are separated by whitespace\n",
    "    \n",
    "    # Filter out short words (length < 3 characters)\n",
    "    words = [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    # Remove stop words (you can define your own list of stop words)\n",
    "    stop_words = set(['the', 'and', 'of', 'in', 'to', 'a', 'is', 'that', 'it', 'for', 'on', 'with', 'as'])\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Function to perform word count and additional functionalities\n",
    "def process_text(lyrics):\n",
    "    # Flatten the list of preprocessed lyrics\n",
    "    flattened_lyrics = [word for sublist in lyrics for word in sublist]\n",
    "    \n",
    "    # Word count\n",
    "    word_count = len(flattened_lyrics)\n",
    "    \n",
    "    # Calculate average word length\n",
    "    total_word_length = sum(len(word) for word in flattened_lyrics)\n",
    "    average_word_length = total_word_length / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Find the most common word\n",
    "    word_freq = Counter(flattened_lyrics)\n",
    "    most_common_word, most_common_word_count = word_freq.most_common(1)[0]\n",
    "    \n",
    "    # Count the number of unique words\n",
    "    unique_word_count = len(word_freq)\n",
    "    \n",
    "    return word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count\n",
    "\n",
    "# Read CSV data\n",
    "lyrics = read_csv_file('billboard_lyrics_1964-2015.csv')\n",
    "\n",
    "# Process text\n",
    "preprocessed_lyrics = [preprocess_text(text) for text in lyrics]\n",
    "word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count = process_text(preprocessed_lyrics)\n",
    "\n",
    "# Display results\n",
    "print(\"Total Word Count:\", word_count)\n",
    "print(\"Average Word Length:\", average_word_length)\n",
    "print(\"Most Common Word:\", most_common_word, \"(Count:\", most_common_word_count, \")\")\n",
    "print(\"Number of Unique Words:\", unique_word_count)\n",
    "```\n",
    "\n",
    "This section of the code defines a function visualize_results to create a bar plot visualizing various text analysis metrics.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize the results\n",
    "def visualize_results(word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count):\n",
    "    # Define the data to visualize\n",
    "    data = {\n",
    "        'Total Word Count': word_count,\n",
    "        'Average Word Length': average_word_length,\n",
    "        'Most Common Word': most_common_word_count,\n",
    "        'Unique Word Count': unique_word_count\n",
    "    }\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(data.keys(), data.values(), color='skyblue')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Text Analysis Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_results(word_count, average_word_length, most_common_word, most_common_word_count, unique_word_count)\n",
    "```",
    "\n",
    "**The below section of the code initializes the streaming process:**\n",
    "\n",
    "### Streaming Word Count with Apache Spark Streaming\n",
    "\n",
    "**The below function, we create a DStream (Discretized Stream) by reading text files from a specified directory. The textFileStream method of the StreamingContext (ssc) is used to create the stream, with the directory path provided as the argument. This enables the streaming application to ingest and process data from the text files in real-time, allowing for continuous analysis of the data as it becomes available.**\n",
    "# Create DStream by reading text files from the directory\n",
    "data_dir = \"./data_chunks\"\n",
    "stream = ssc.textFileStream(data_dir)\n",
    "\n",
    "**Followed by the below code, we define a process to handle each RDD (Resilient Distributed Dataset) in the stream. The foreachRDD method is used to apply the process_stream function to each RDD in the stream. This function processes the streaming data, extracting word counts, and visualizing the results. By iterating over each RDD in the stream, we ensure that the processing logic is applied to every batch of data received, enabling real-time analysis of the streaming data.**\n",
    "# Process each RDD in the stream\n",
    "stream.foreachRDD(process_stream)\n",
    "\n",
    "**In the below process, we initiate the streaming process by calling the start method on the StreamingContext object (ssc). Additionally, we use the awaitTermination method to instruct the program to wait until the streaming process is terminated or manually stopped.**\n",
    "# Start streaming\n",
    "ssc.start()\n",
    "# Wait for streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

