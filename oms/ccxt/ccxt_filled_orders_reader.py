"""
Import as:

import oms.ccxt.ccxt_filled_orders_reader as occforre
"""
import logging
import os
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from tqdm.autonotebook import tqdm

import helpers.hdatetime as hdateti
import helpers.hdbg as hdbg
import helpers.hio as hio
import im_v2.common.universe.universe_utils as imvcuunut
import oms.ccxt.abstract_ccxt_broker as ocabccbr

# TODO(Danya): CMTask4420.
# `extra_params` in `oms.Order` contains a dict, which is serialized in CSV
# as a string, which is converted back to dict using `eval`. `extra_params`
# contains time logging info in `datetime.datetime` format, and applying
# `eval` in `load_oms_child_order_df` requires this library to be imported.
import datetime  # pylint: disable=unused-import

_LOG = logging.getLogger(__name__)

# #############################################################################
# CCXT Logs Reader
# #############################################################################


class CcxtLogsReader:
    """
    Read fills and logs generated by a CCXT Broker.

    This includes:
    - Oms.Fills for all the CCXT trades
    - CCXT order responses for all children orders
    - Submitted child orders

    The expected structure of the logs folder is:
    ```
    {log_dir}/
        ccxt_child_order_responses/
            - Common to Broker_v1 and Broker_v2
        oms_child_orders/
            - Common to Broker_v1 and Broker_v2
        oms_parent_orders/
            - Initial input parent orders in the state before submission
        child_order_fills/
            ccxt_fills/
                - Information about closed orders from CCXT
                - Works only for v2
            oms_fills/
                - JSON representations of oms.Fill objects generated
                  by the Broker
                - Works only for v2
            ccxt_trades/
                - Output of CCXT `fetchMyTrades` method which contains the 'fees'
                  and 'realizedPNL' fields
                - Works only for v2
    ```
    """

    def __init__(
        self,
        root_dir: str,
    ):
        """
        :param root_dir: root location of broker logs,
          e.g. /shared_data/system_log_dir_20230315_30minutes/
        """
        #
        self._root_dir = root_dir
        self._get_log_subdirectories()

    def load_all_data(self) -> Dict[str, pd.DataFrame]:
        """
        Load all data.

        :return: dictionary storing the logged data
        """
        all_data = {
            "ccxt_order_responses": self.load_ccxt_order_response_df(),
            "oms_parent_orders": self.load_oms_parent_order_df(),
            "ccxt_trades": self.load_ccxt_trades_df(),
            "oms_child_orders": self.load_oms_child_order_df(),
            "oms_fills": self.load_oms_fills_df(),
            "ccxt_fills": self.load_ccxt_fills_df(),
        }
        return all_data

    def load_ccxt_order_response_df(
        self,
    ) -> pd.DataFrame:
        """
        Load CCXT order responses as a DataFrame.

        The order response is a CCXT order structure, as described in
        https://docs.ccxt.com/#/?id=order-structure.

        E.g., a timeseries from return DataFrame looks like:
        ```
        info                    {'orderId': '7954906695', 'symbol': 'APEUSDT',...
        order                   7954906695
        client_order_id             x-xcKtGhcub89989e55d47273a3610a9
        timestamp                1678898138582
        datetime                 2023-03-15 16:35:38.582000+00:00
        last_trade_timestamp          None
        symbol                  APE/USDT
        order_type                limit
        time_in_force              GTC
        post_only                False
        reduce_only               False
        side                    buy
        order_price               4.12
        stop_price                NaN
        order_amount              10.0
        cost                    0.0
        average                 NaN
        filled                   0.0
        remaining                10.0
        status                  open
        fee                    NaN
        trades                  []
        fees                   []
        order_update_timestamp       1678898138582
        order_update_datetime        1970-01-01 00:27:58.898138582+00:00
        ```
        """
        # Get the files.
        files = self._get_files(self._ccxt_order_responses_dir)
        # Read all the files.
        ccxt_order_responses = []
        for path in tqdm(
            files, desc=f"Loading files from '{self._ccxt_order_responses_dir}'"
        ):
            data = hio.from_json(path, use_types=True)
            ccxt_order_responses.append(data)
        # Assemble the output df.
        ccxt_order_responses = self._convert_ccxt_order_structures_to_dataframe(
            ccxt_order_responses
        )
        return ccxt_order_responses

    def load_oms_parent_order_df(self) -> pd.DataFrame:
        """
        Load and normalize "parent" orders.
        """
        # Get the files.
        files = self._get_files(self._oms_parent_orders_dir)
        # Read all the files.
        parent_orders = []
        # Load individual orders as pd.Series.
        for path in tqdm(
            files, desc=f"Loading files from '{self._oms_parent_orders_dir}'"
        ):
            # Load parent order files from JSON format.
            data = hio.from_json(path, use_types=True)
            parent_orders.extend(data)
        # Assemble the output df.
        oms_parent_order_df = self._convert_oms_parent_orders_to_dataframe(
            parent_orders
        )
        return oms_parent_order_df

    def load_oms_child_order_df(
        self, unpack_extra_params: bool = False
    ) -> pd.DataFrame:
        """
        Load and normalize "child" orders.

        :param unpack_extra_params: if True, unpack `extra_params` field into output DataFrame columns

        Example of returned data:
        ```
                creation_timestamp              asset_id    type_  \
        order_id
        20       2023-03-15 16:35:37.825835+00:00  6051632686  limit
        21       2023-03-15 16:35:38.718960+00:00  8717633868  limit

                start_timestamp               end_timestamp  \
        order_id
        20       2023-03-15 16:35:37.825835+00:00 2023-03-15 16:36:37.825835+00:00
        21       2023-03-15 16:35:38.718960+00:00 2023-03-15 16:36:38.718960+00:00

                curr_num_shares  diff_num_shares  tz            passivity_factor    \
        order_id
        20       0.0          10.0        America/New_York   0.55
        21       0.0          -1.0        America/New_York    0.55

                latest_bid_price  latest_ask_price  bid_price_mean  ask_price_mean  \
        order_id
        20       4.120             4.121    4.125947    4.126974
        21       15.804            15.805   15.818162   15.819432

                used_bid_price    used_ask_price  limit_price      ccxt_id  \
        order_id
        20       latest_bid_price  latest_ask_price    4.12045   7954906695
        21       latest_bid_price  latest_ask_price    15.80455  14412582631
        ```
        """
        # Get the files.
        files = self._get_files(self._oms_child_orders_dir)
        # Read all the files.
        child_orders = []
        # Load individual orders as pd.Series.
        for path in tqdm(
            files, desc=f"Loading files from '{self._oms_child_orders_dir}'"
        ):
            # Load individual orders in pd.Series format.
            data = hio.from_json(path, use_types=True)
            child_orders.append(data)
        # Assemble the output df.
        oms_child_orders_df = self._convert_oms_child_orders_to_dataframe(
            child_orders,
            unpack_extra_params=unpack_extra_params,
        )
        return oms_child_orders_df

    def load_ccxt_trades_df(self) -> pd.DataFrame:
        """
        Load trades as DataFrame.

        Example of one entity of input JSON data:
        ```
        {
        "info": {
            "symbol": "APEUSDT",
            "id": "356819245",
            "orderId": "8077704766",
            "side": "SELL",
            "price": "4.0400",
            "qty": "6",
            "realizedPnl": 0.0,
            "marginAsset": "USDT",
            "quoteQty": "24.2400",
            "commission": "0.00969600",
            "commissionAsset": "USDT",
            "time": "1679560540879",
            "positionSide": "BOTH",
            "buyer": False,
            "maker": False
        },
            "timestamp": 1679560540879,
            "datetime": "Timestamp('2023-03-23 08:35:40.879000+0000', tz='UTC')",
            "symbol": "APE/USDT",
            "asset_id": 6051632686,
            "id": 356819245,
            "order": 8077704766,
            "type": None,
            "side": "sell",
            "takerOrMaker": "taker",
            "price": 4.04,
            "amount": 6.0,
            "cost": 24.24,
            "fee": {
                "cost": 0.009696,
                "currency": "USDT"
            },
            "fees": [
                {
                    "currency": "USDT",
                    "cost": 0.009696
                }
            ]
        }
        ```

        Example of returned data:
        ```
                                timestamp    symbol         id       order  side  \
        0 2022-09-29 16:46:39.509000+00:00  APE/USDT  282773274  5772340563  sell
        1 2022-09-29 16:51:58.567000+00:00  APE/USDT  282775654  5772441841   buy
        2 2022-09-29 16:57:00.267000+00:00  APE/USDT  282779084  5772536135   buy
        3 2022-09-29 17:02:00.329000+00:00  APE/USDT  282780259  5772618089  sell
        4 2022-09-29 17:07:03.097000+00:00  APE/USDT  282781536  5772689853   buy

        takerOrMaker  price  amount    cost      fees fees_currency realized_pnl
        0        taker  5.427     5.0  27.135  0.010854          USDT            0
        1        taker  5.398     6.0  32.388  0.012955          USDT   0.14500000
        2        taker  5.407     3.0  16.221  0.006488          USDT            0
        3        taker  5.395     9.0  48.555  0.019422          USDT  -0.03900000
        4        taker  5.381     8.0  43.048  0.017219          USDT   0.07000000
        ```
        """
        # Get the files.
        files = self._get_files(self._ccxt_trades_dir)
        # Read all the files.
        ccxt_child_order_trades = []
        for path in tqdm(
            files, desc=f"Loading files from '{self._ccxt_trades_dir}'"
        ):
            data = hio.from_json(path, use_types=True)
            ccxt_child_order_trades.extend(data)
        # Convert fills to DataFrame.
        ccxt_child_order_trades = self._convert_ccxt_trades_json_to_dataframe(
            ccxt_child_order_trades
        )
        # Remove full duplicates for fills.
        # Note: generally fills are loaded in bulk via CCXT `fetchMyTrades()`
        # method, which allows only for queries based on time range and full
        # symbol. In some cases, there is an overlap between queries, which leads
        # to full duplicates appearing in the resulting DataFrame.
        ccxt_child_order_trades_deduped = ccxt_child_order_trades.drop_duplicates(
            subset=["timestamp", "id", "order"], keep="last"
        )
        # TODO(pp): Use hdbg.to_perc to get a better output.
        _LOG.debug(
            "%s duplicates were removed from original data.",
            ccxt_child_order_trades.shape[0]
            - ccxt_child_order_trades_deduped.shape[0],
        )
        return ccxt_child_order_trades_deduped

    def load_oms_fills_df(self) -> pd.DataFrame:
        """
        Load the oms fills representation from JSON.

        Example of input data:
        ```
        {
            "asset_id": 5115052901,
            "fill_id": 6,
            "timestamp": "2023-05-23T11:58:50.201000+00:00",
            "num_shares": 93.0,
            "price": 69.5733
        }
        ```

        Example of returned data:
        ```
           asset_id         fill_id  timestamp                      num_shares    price
        0  8717633868        2  2023-05-23T11:58:49.172000+00:00       1.000  14.7540
        1  1467591036        4  2023-05-23T11:58:49.687000+00:00       0.001  27.3304
        2  5115052901        6  2023-05-23T11:58:50.201000+00:00      93.000  69.5733
        3  3401245610        7  2023-05-23T11:58:50.456000+00:00       3.000   6.5460
        ```

        Works only for version v2.
        """
        files = self._get_files(self._oms_fills_dir)
        oms_child_order_fills = []
        for path in tqdm(
            files, desc=f"Loading files from '{self._oms_fills_dir}'"
        ):
            data = hio.from_json(path, use_types=True)
            oms_child_order_fills.extend(data)
        oms_child_order_fills = pd.DataFrame(oms_child_order_fills)
        return oms_child_order_fills

    def load_ccxt_fills_df(self) -> pd.DataFrame:
        """
        Load the CCXT filled order representations from JSON.

        These representations correspond to closed orders.

        Example of one entity of input JSON data:
        ```
        {
        "info": {
            "orderId": "2187830797",
            "symbol": "CTKUSDT",
            "status": "FILLED",
            "clientOrderId": "x-xcKtGhcuda5dde8563a5c1568a5893",
            "price": "0.74810",
            "avgPrice": "0.74810",
            "origQty": "93",
            "executedQty": "93",
            "cumQuote": "69.57330",
            "timeInForce": "GTC",
            "type": "LIMIT",
            "reduceOnly": False,
            "closePosition": False,
            "side": "BUY",
            "positionSide": "BOTH",
            "stopPrice": "0",
            "workingType": "CONTRACT_PRICE",
            "priceProtect": False,
            "origType": "LIMIT",
            "time": "1684843130201",
            "updateTime": 1684843130201
        },
        "id": 2187830797,
        "clientOrderId": "x-xcKtGhcuda5dde8563a5c1568a5893",
        "timestamp": 1684843130201,
        "datetime": "Timestamp('2023-05-23 11:58:50.201000+0000', tz='UTC')",
        "lastTradeTimestamp": None,
        "symbol": "CTK/USDT",
        "type": "limit",
        "timeInForce": "GTC",
        "postOnly": False,
        "reduceOnly": False,
        "side": "buy",
        "price": 0.7481,
        "stopPrice": "nan",
        "amount": 93.0,
        "cost": 69.5733,
        "average": 0.7481,
        "filled": 93.0,
        "remaining": 0.0,
        "status": "closed",
        "fee": "nan",
        "trades": [],
        "fees": []
        }
        ```

        E.g., a timeseries from return DataFrame looks like:
        ```
        info                      {'orderId': '14901643572', 'symbol': 'AVAXUSDT...
        order                      14901643572
        client_order_id                x-xcKtGhcu8261da0e4a2533b528e0e2
        timestamp                   1684843129172
        datetime                    2023-05-23 11:58:49.172000+00:00
        last_trade_timestamp             None
        symbol                     AVAX/USDT
        order_type                   limit
        time_in_force                 GTC
        post_only                   False
        reduce_only                  False
        side                       buy
        order_price                  14.761
        stop_price                   NaN
        order_amount                 1.0
        cost                      14.754
        average                    14.754
        filled                      1.0
        remaining                   0.0
        status                     closed
        fee                       NaN
        trades                     []
        fees                      []
        order_update_timestamp          1684843129172
        order_update_datetime           1970-01-01 00:28:04.843129172+00:00
        ```

        Works only for version v2.
        """
        files = self._get_files(self._ccxt_fills_dir)
        ccxt_fills = []
        for path in tqdm(
            files, desc=f"Loading `{self._ccxt_fills_dir}` files..."
        ):
            data = hio.from_json(path, use_types=True)
            ccxt_fills.extend(data)
        ccxt_fills = self._convert_ccxt_order_structures_to_dataframe(ccxt_fills)
        return ccxt_fills

    @staticmethod
    def _get_files(log_dir: str) -> List[str]:
        """
        Get a list of files from the directory.
        """
        pattern = "*"
        only_files = True
        use_relative_paths = False
        files: List[str] = hio.listdir(
            log_dir, pattern, only_files, use_relative_paths
        )
        files.sort()
        return files

    @staticmethod
    def _convert_ccxt_order_structures_to_dataframe(
        ccxt_order_structures: List[ocabccbr.CcxtData],
    ) -> pd.DataFrame:
        """
        Convert a list of CCXT order structures to a DataFrame.

        `child_order_response` and `ccxt_fill` are exactly the same data
        structures (see http://docs.ccxt.com/#/?id=order-structure) that
        we encode as `ocabccbr.CcxtData` (`Dict[str, Any]` under the
        hood). Thus, we refer to the common data structure as
        `ccxt_order_structure` and use this function for both data
        structures.
        """
        ccxt_order_structures_series_list = []
        for ccxt_order_structure in ccxt_order_structures:
            # Skip the response JSON if it is empty.
            if not ccxt_order_structure:
                continue
            srs = pd.Series(
                ccxt_order_structure.values(), ccxt_order_structure.keys()
            )
            # Get order update Unix timestamp from the exchange.
            info_timestamp = ccxt_order_structure["info"]["updateTime"]
            srs["info_timestamp"] = info_timestamp
            # Convert to datetime.
            srs["info_datetime"] = pd.to_datetime(
                info_timestamp, unit="ms", utc=True
            )
            ccxt_order_structures_series_list.append(srs)
        ccxt_order_df = pd.concat(ccxt_order_structures_series_list, axis=1).T
        # Transform timestamp columns to UTC.
        ccxt_order_df = ccxt_order_df.applymap(
            lambda x: x.tz_convert("UTC") if isinstance(x, pd.Timestamp) else x
        )
        #  Check the timestamp logs in `CcxtBroker_v2` and update the conversion accordingly.
        # Rename columns.
        ccxt_order_df = ccxt_order_df.rename(
            columns={
                "id": "order",
                "clientOrderId": "client_order_id",
                "lastTradeTimestamp": "last_trade_timestamp",
                "type": "order_type",
                "timeInForce": "time_in_force",
                "postOnly": "post_only",
                "reduceOnly": "reduce_only",
                "price": "order_price",
                "stopPrice": "stop_price",
                "amount": "order_amount",
                "info_timestamp": "order_update_timestamp",
                "info_datetime": "order_update_datetime",
            }
        )
        return ccxt_order_df

    @staticmethod
    def _normalize_fills_dataframe(fills_df: pd.DataFrame) -> pd.DataFrame:
        """
        Validate a fills DataFrame and normalize for chaining
        `_aggregate_fills()`.

        Ensure `df` is a DataFrame with certain columns and restricted values.

        :param df: a fills DataFrame as returned by
            `convert_fills_json_to_dataframe()`
        """
        # Sanity-check the DataFrame.
        hdbg.dassert_isinstance(fills_df, pd.DataFrame)
        required_cols = [
            "timestamp",
            "datetime",
            "side",
            "takerOrMaker",
            "price",
            "amount",
            "cost",
            "transaction_cost",
        ]
        hdbg.dassert_is_subset(required_cols, fills_df.columns)
        # Ensure categoricals lie in expected sets.
        hdbg.dassert_is_subset(fills_df["side"].unique(), ["buy", "sell"])
        hdbg.dassert_is_subset(
            fills_df["takerOrMaker"].unique(), ["taker", "maker", np.nan]
        )
        # Check self-consistency of prices/costs/amounts.
        hdbg.dassert_lte(
            ((fills_df["cost"] / fills_df["amount"]) - fills_df["price"]).sum(),
            1e-6,
        )
        #
        fills_df = fills_df.copy()
        fills_df["datetime"] = pd.to_datetime(fills_df["datetime"])
        # Assign additional datetime columns.
        fills_df["first_timestamp"] = fills_df["timestamp"]
        fills_df["last_timestamp"] = fills_df["timestamp"]
        fills_df["first_datetime"] = fills_df["datetime"]
        fills_df["last_datetime"] = fills_df["datetime"]
        # Accumulate buy/sell counts.
        fills_df["buy_count"] = (fills_df["side"] == "buy").astype(int)
        fills_df["sell_count"] = (fills_df["side"] == "sell").astype(int)
        # Accumulate taker/maker counts.
        fills_df["taker_count"] = (fills_df["takerOrMaker"] == "taker").astype(
            int
        )
        fills_df["maker_count"] = (fills_df["takerOrMaker"] == "maker").astype(
            int
        )
        return fills_df

    @staticmethod
    def _convert_oms_fills_to_dataframe(
        oms_fills: List[ocabccbr.CcxtData],
    ) -> pd.DataFrame:
        """
        Convert a list of OMS fill dicts into a DataFrame.
        """
        oms_fills_df = pd.DataFrame(oms_fills)
        #
        expected_columns = [
            "asset_id",
            "fill_id",
            "timestamp",
            "num_shares",
            "price",
        ]
        hdbg.dassert_set_eq(oms_fills_df.columns, expected_columns)
        return oms_fills_df

    def _convert_oms_parent_orders_to_dataframe(
        self, oms_parent_orders: List[ocabccbr.CcxtData]
    ) -> pd.DataFrame:
        """
        Convert a list of OMS parent orders into a single DataFrame and
        normalize.

        Example of input OMS parent orders:
        [OrderedDict([('order_id', 0),
        ('creation_timestamp',
        Timestamp('2023-06-21 10:15:49.589872-0400',
        tz='America/New_York')),                         ('asset_id',
        3065029174),                         ('type_', 'price@twap'),
        ('start_timestamp',
        Timestamp('2023-06-21 10:15:49.589872-0400',
        tz='America/New_York')),
        ('end_timestamp',                         Timestamp('2023-06-21
        10:20:49.589872-0400', tz='America/New_York')),
        ('curr_num_shares', 0.0),
        ('diff_num_shares', -900.0),                         ('tz',
        <DstTzInfo 'America/New_York' EST-1 day, 19:00:00 STD>),
        ('extra_params', {})]),             OrderedDict([('order_id',
        1),                         ('creation_timestamp',
        Timestamp('2023-06-21 10:15:49.590399-0400',
        tz='America/New_York')),                         ('asset_id',
        6051632686),                         ('type_', 'limit'),
        ('start_timestamp',
        Timestamp('2023-06-21 10:15:49.590399-0400',
        tz='America/New_York')),
        ('end_timestamp',                         Timestamp('2023-06-21
        10:20:49.590399-0400', tz='America/New_York')),
        ('curr_num_shares', 0.0),
        ('diff_num_shares', 33.0),                         ('tz',
        <DstTzInfo 'America/New_York' EST-1 day, 19:00:00 STD>),
        ('extra_params', {})])]         Example of output:         ```
        creation_timestamp    asset_id       type_
        start_timestamp                    end_timestamp
        curr_num_shares  diff_num_shares               tz
        extra_params order_id 0        2023-06-22 08:44:21.852271+00:00
        3065029174  price@twap 2023-06-22 08:44:21.852271+00:00
        2023-06-22 08:49:21.852271+00:00              0.0
        -900.0  America/New_York           {} 1        2023-06-22
        08:44:21.852754+00:00  6051632686       limit 2023-06-22
        08:44:21.852754+00:00 2023-06-22 08:49:21.852754+00:00
        0.0             33.0  America/New_York           {}         ```
        """
        # Combine into a single DataFrame.
        oms_parent_orders_df = pd.DataFrame(oms_parent_orders)
        oms_parent_orders_df = oms_parent_orders_df.set_index("order_id")
        # Transform timestamp columns to UTC.
        oms_parent_orders_df = oms_parent_orders_df.applymap(
            lambda x: x.tz_convert("UTC") if isinstance(x, pd.Timestamp) else x
        )
        return oms_parent_orders_df

    def _convert_oms_child_orders_to_dataframe(
        self,
        oms_child_orders: List[ocabccbr.CcxtData],
        *,
        unpack_extra_params: bool = False,
    ) -> pd.DataFrame:
        """
        Convert list of OMS child order Series into a DataFrame and normalize.

        Example of input OMS child order:
        ```
        {
            "order_id": 20,
            "creation_timestamp": "Timestamp('2023-03-15 12:35:37.825835-0400', tz='pytz.FixedOffset(-240)')",
            "asset_id": 6051632686,
            "type_": "limit",
            "start_timestamp": "Timestamp('2023-03-15 12:35:37.825835-0400', tz='pytz.FixedOffset(-240)')",
            "end_timestamp": "Timestamp('2023-03-15 12:36:37.825835-0400', tz='pytz.FixedOffset(-240)')",
            "curr_num_shares": 0.0,
            "diff_num_shares": 10.0,
            "tz": "America/New_York",
            "passivity_factor": 0.55,
            "latest_bid_price": 4.12,
            "latest_ask_price": 4.121,
            "bid_price_mean": 4.125947368421053,
            "ask_price_mean": 4.126973684210526,
            "used_bid_price": "latest_bid_price",
            "used_ask_price": "latest_ask_price",
            "limit_price": 4.12045,
            "ccxt_id": 7954906695
        }
        ```

        E.g., a timeseries from return DataFrame:
        ```
        order_id                 20
        creation_timestamp          2023-03-15 12:35:37.825835-04:00
        asset_id                 6051632686
        type_                   limit
        start_timestamp            2023-03-15 12:35:37.825835-04:00
        end_timestamp             2023-03-15 12:36:37.825835-04:00
        curr_num_shares            0.0
        diff_num_shares            10.0
        tz                     America/New_York
        passivity_factor            0.55                                                                                                                 latest_bid_price                                                   4.12
        latest_ask_price            4.121
        bid_price_mean             4.125947368421053
        ask_price_mean             4.126973684210526                                                                                                                 used_bid_price                                         latest_bid_price
        used_ask_price             latest_ask_price                                                                                                                 limit_price                                                     4.12045
        ccxt_id                  7954906695
        extra_params              {'order_generated_timestamp': datetime.datetim...
        ```
        """
        # Convert to DataFrame.
        oms_child_orders_series_list = []
        for oms_child_order_srs in oms_child_orders:
            order_id = oms_child_order_srs["order_id"]
            # Name each series according to internal order_id.
            oms_child_order_srs["name"] = order_id
            oms_child_orders_series_list.append(oms_child_order_srs)
        oms_child_orders_df = pd.DataFrame(oms_child_orders_series_list)
        oms_child_orders_df = oms_child_orders_df.set_index("order_id")
        # Transform timestamp columns to UTC.
        oms_child_orders_df = oms_child_orders_df.applymap(
            lambda x: x.tz_convert("UTC") if isinstance(x, pd.Timestamp) else x
        )
        if unpack_extra_params:
            # Unpack 'extra params' dictionary into a DataFrame.
            extra_params_df = oms_child_orders_df["extra_params"].apply(pd.Series)
            # Add extra_params columns to the child orders DataFrame.
            oms_child_orders_df = pd.concat(
                [oms_child_orders_df, extra_params_df], axis=1
            )
            # Remove the extra_params column to avoid data duplication.
            oms_child_orders_df = oms_child_orders_df.drop(
                ["extra_params"], axis=1
            )
        return oms_child_orders_df

    def _convert_ccxt_trades_json_to_dataframe(
        self, trades_json: List[ocabccbr.CcxtData]
    ) -> pd.DataFrame:
        """
        Convert JSON-format trades into a DataFrame.

        - Unpack nested values;
        - Convert unix epoch to pd.Timestamp;
        - Remove duplicated information;
        """
        hdbg.dassert_lte(1, len(trades_json))
        trades = pd.DataFrame(trades_json)
        hdbg.dassert_in("asset_id", trades.columns)
        # Extract nested values.
        # Note: `transaction_cost` is extracted from the `fee`
        #  value for the base/quote currency.
        #  See https://docs.ccxt.com/#/?id=trade-structure.
        #
        trades["transaction_cost"] = [fee["cost"] for fee in trades["fee"]]
        trades["fees_currency"] = [fee["currency"] for fee in trades["fee"]]
        trades["realized_pnl"] = [info["realizedPnl"] for info in trades["info"]]
        # Force conversion of PnL to float.
        # PnL is extracted from `info` field, which stores all values as strings.
        trades["realized_pnl"] = trades["realized_pnl"].astype(float)
        # Replace unix epoch with a timestamp.
        trades["timestamp"] = trades["timestamp"].apply(
            hdateti.convert_unix_epoch_to_timestamp
        )
        # Set columns.
        columns = [
            "timestamp",
            "datetime",
            "symbol",
            "asset_id",
            "id",
            "order",
            "side",
            "takerOrMaker",
            "price",
            "amount",
            "cost",
            "transaction_cost",
            "fees_currency",
            "realized_pnl",
        ]
        trades = trades[columns]
        # Normalize data types.
        trades = self._normalize_fills_dataframe(trades)
        # Set timestamp index.
        trades = trades.set_index("timestamp", drop=False)
        return trades

    def _get_log_subdirectories(self) -> None:
        """
        Generate and store subdirectories with Broker logs.

        The directory is expected to follow the same structure, with
        fills and orders stored in '{root_dir}/'
        """
        hdbg.dassert_path_exists(self._root_dir)
        child_order_fills_dir = os.path.join(self._root_dir, "child_order_fills")
        hdbg.dassert_path_exists(child_order_fills_dir)
        # Verify that the fills dir conforms to expected structure.
        # If there are no subdirectories, the reader expects to find child order fill logs.
        # If there are subdirectories, the reader expect them to conform to the structure outlined in the constructor.
        child_order_fills_dir_contents = os.scandir(child_order_fills_dir)
        has_subdirectories = any(
            [path.is_dir() for path in child_order_fills_dir_contents]
        )
        if has_subdirectories:
            # Get a subdirectory for CCXT fill representations.
            # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/ccxt_fills'.
            oms_fills_dir = os.path.join(child_order_fills_dir, "ccxt_fills")
            hdbg.dassert_path_exists(oms_fills_dir)
            self._ccxt_fills_dir = oms_fills_dir
            # Get a subdirectory for CCXT trades.
            # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/ccxt_trades'.
            ccxt_child_order_trades_dir = os.path.join(
                child_order_fills_dir, "ccxt_trades"
            )
            hdbg.dassert_path_exists(ccxt_child_order_trades_dir)
            self._ccxt_trades_dir = ccxt_child_order_trades_dir
            # Get a subdirectory for OMS fills.
            # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/oms_fills'.
            oms_fills_dir = os.path.join(child_order_fills_dir, "oms_fills")
            hdbg.dassert_path_exists(oms_fills_dir)
            self._oms_fills_dir = oms_fills_dir
        else:
            # Get subdirectory for CCXT representations child order fills.
            # E.g. 'system_log_dir_20230315_30minutes/child_order_fills'.
            self._ccxt_trades_dir = child_order_fills_dir
        # Get subdirectory for CCXT order responses.
        # E.g. 'system_log_dir_20230315_30minutes/ccxt_child_order_responses'.
        ccxt_order_responses_dir = os.path.join(
            self._root_dir, "ccxt_child_order_responses"
        )
        hdbg.dassert_path_exists(ccxt_order_responses_dir)
        self._ccxt_order_responses_dir = ccxt_order_responses_dir
        # Get subdirectory for submitted child orders.
        # E.g. 'system_log_dir_20230315_30minutes/oms_child_orders'.
        oms_child_orders_dir = os.path.join(self._root_dir, "oms_child_orders")
        hdbg.dassert_path_exists(oms_child_orders_dir)
        self._oms_child_orders_dir = oms_child_orders_dir
        # Get subdirectory for parent orders.
        # E.g. 'system_log_dir_20230315_30minutes/oms_parent_orders/'.
        oms_parent_orders_dir = os.path.join(self._root_dir, "oms_parent_orders")
        hdbg.dassert_path_exists(oms_parent_orders_dir)
        self._oms_parent_orders_dir = oms_parent_orders_dir


# #############################################################################
# Ccxt order response processing
# #############################################################################

# TODO(gp): Move this in different files.


def aggregate_ccxt_orders_by_bar(
    df: pd.DataFrame,
    freq: str,
) -> pd.DataFrame:
    """
    Summarize ccxt order responses by bar and instrument.

    :param df: result of `load_ccxt_order_response_df()`
    :param freq: bar duration as a string, e.g, `"5T"`
    :return: DataFrame with 2 col levels, with various order stats
        organized by asset and by bar (a timestamp index)
    """
    df = df.set_index("order")
    df["bar_start_datetime"] = df["order_update_datetime"].dt.floor(freq)
    df["bar_end_datetime"] = df["order_update_datetime"].dt.ceil(freq)
    #
    df["full_symbol"] = (
        "binance" + "::" + df["symbol"].apply(lambda x: x.replace("/", "_"))
    )
    df_symbols = df["full_symbol"].unique()
    asset_id_to_full_symbol = imvcuunut.build_numerical_to_string_id_mapping(
        df_symbols
    )
    full_symbol_to_asset_id = {v: k for k, v in asset_id_to_full_symbol.items()}
    df["asset_id"] = df["full_symbol"].apply(lambda x: full_symbol_to_asset_id[x])
    #
    buy_orders = df[df["side"] == "buy"]
    sell_orders = df[df["side"] == "sell"]
    #
    buy_orders["buy_notional"] = (
        buy_orders["order_amount"] * buy_orders["order_price"]
    )
    sell_orders["sell_notional"] = (
        sell_orders["order_amount"] * sell_orders["order_price"]
    )
    #
    buy_groupby = buy_orders.groupby(["bar_end_datetime", "asset_id"])
    sell_groupby = sell_orders.groupby(["bar_end_datetime", "asset_id"])
    #
    buy_order_count = (
        buy_groupby["client_order_id"]
        .count()
        .unstack()
        .fillna(0)
        .astype(np.int64)
    )
    sell_order_count = (
        sell_groupby["client_order_id"]
        .count()
        .unstack()
        .fillna(0)
        .astype(np.int64)
    )
    order_count = buy_order_count + sell_order_count
    #
    buy_limit_twap = buy_groupby["order_price"].mean().unstack()
    sell_limit_twap = sell_groupby["order_price"].mean().unstack()
    #
    buy_amount = buy_groupby["order_amount"].sum().unstack()
    sell_amount = sell_groupby["order_amount"].sum().abs().unstack()
    #
    buy_notional = buy_groupby["buy_notional"].sum().unstack()
    sell_notional = sell_groupby["sell_notional"].sum().unstack()
    #
    buy_limit_vwap = buy_notional / buy_amount
    sell_limit_vwap = sell_notional / sell_amount
    #
    out_df = {
        "order_count": order_count,
        "buy_order_count": buy_order_count,
        "buy_limit_twap": buy_limit_twap,
        "buy_limit_vwap": buy_limit_vwap,
        "buy_amount": buy_amount,
        "buy_notional": buy_notional,
        "sell_order_count": sell_order_count,
        "sell_limit_twap": sell_limit_twap,
        "sell_limit_vwap": sell_limit_vwap,
        "sell_amount": sell_amount,
        "sell_notional": sell_notional,
    }
    out_df = pd.concat(out_df, axis=1)
    return out_df


def aggregate_child_limit_orders_by_bar(
    df: pd.DataFrame,
    freq: str,
) -> pd.DataFrame:
    """
    Summarize OMS child orders by bar and instrument.

    This is similar to `aggregate_ccxt_orders_by_bar()`, is a separate function
    due to different column name conventions and expected fields.

    :param df: result of `load_child_order_df()`
    :param freq: bar duration as a string, e.g, `"5T"`
    :return: DataFrame with 2 col levels, with various order stats
        organized by asset and by bar (a timestamp index)
    """
    # Don't modify the DataFrame.
    df = df.copy()
    #
    df["bar_start_datetime"] = df["start_timestamp"].dt.floor(freq)
    df["bar_end_datetime"] = df["start_timestamp"].dt.ceil(freq)
    #
    buy_orders = df[df["diff_num_shares"] > 0]
    sell_orders = df[df["diff_num_shares"] < 0]
    #
    buy_orders["buy_notional"] = (
        buy_orders["diff_num_shares"] * buy_orders["limit_price"]
    )
    sell_orders["sell_notional"] = (
        sell_orders["diff_num_shares"].abs() * sell_orders["limit_price"]
    )
    #
    buy_groupby = buy_orders.groupby(["bar_end_datetime", "asset_id"])
    sell_groupby = sell_orders.groupby(["bar_end_datetime", "asset_id"])
    #
    buy_order_count = (
        buy_groupby["creation_timestamp"]
        .count()
        .unstack()
        .fillna(0)
        .astype(np.int64)
    )
    sell_order_count = (
        sell_groupby["creation_timestamp"]
        .count()
        .unstack()
        .fillna(0)
        .astype(np.int64)
    )
    order_count = pd.concat([buy_order_count, sell_order_count], axis=1)
    #
    buy_limit_twap = buy_groupby["limit_price"].mean().unstack()
    sell_limit_twap = sell_groupby["limit_price"].mean().unstack()
    #
    buy_amount = buy_groupby["diff_num_shares"].sum().unstack()
    sell_amount = sell_groupby["diff_num_shares"].sum().abs().unstack()
    #
    buy_notional = buy_groupby["buy_notional"].sum().unstack()
    sell_notional = sell_groupby["sell_notional"].sum().unstack()
    #
    buy_limit_vwap = buy_notional / buy_amount
    sell_limit_vwap = sell_notional / sell_amount
    #
    out_df = {
        "order_count": order_count,
        "buy_order_count": buy_order_count,
        "buy_limit_twap": buy_limit_twap,
        "buy_limit_vwap": buy_limit_vwap,
        "buy_amount": buy_amount,
        "buy_notional": buy_notional,
        "sell_order_count": sell_order_count,
        "sell_limit_twap": sell_limit_twap,
        "sell_limit_vwap": sell_limit_vwap,
        "sell_amount": sell_amount,
        "sell_notional": sell_notional,
    }
    out_df = pd.concat(out_df, axis=1)
    return out_df


# #############################################################################
# Fill processing
# #############################################################################


def aggregate_fills_by_bar(
    df: pd.DataFrame,
    freq: str,
    *,
    offset: str = "0T",
    groupby_id_col: str = "asset_id",
) -> pd.DataFrame:
    """
    Aggregate a fills DataFrame by order id, then by bar at `freq`.

    :param df: a fills DataFrame as returned by
        `convert_fills_json_to_dataframe()`
    :param freq: bar frequency
    """
    hdbg.dassert_in(groupby_id_col, ["asset_id", "symbol"])
    df = aggregate_fills_by_order(df)
    df["bar_start_datetime"] = df["first_datetime"].dt.floor(freq) + pd.Timedelta(
        offset
    )
    df["bar_end_datetime"] = df["first_datetime"].dt.ceil(freq) + pd.Timedelta(
        offset
    )
    # If, for a given instruments, only buys or sells occur within any given
    #  bar, then a single aggregation makes sense. Otherwise, we need to
    #  explicitly separate buys and sells.
    aggregated = df.groupby(
        ["bar_end_datetime", groupby_id_col], group_keys=True
    ).apply(_aggregate_fills)
    return aggregated


def compute_buy_sell_prices_by_bar(
    df: pd.DataFrame,
    freq: str,
    *,
    offset: str = "0T",
    groupby_id_col: str = "asset_id",
) -> pd.DataFrame:
    """
    Compute buy/sell trade prices by symbol and bar.

    :param df: a fills DataFrame as returned by
        `convert_fills_json_to_dataframe()`
    :param freq: bar frequency
    :return: DataFrame indexed by bar end datetimes and multilevel columns;
        outer level consists of "buy_trade_price", "sell_trade_price", and
        inner level consists of symbols
    """
    hdbg.dassert_in(groupby_id_col, ["asset_id", "symbol"])
    df = aggregate_fills_by_order(df)
    df["bar_end_datetime"] = df["first_datetime"].dt.ceil(freq) + pd.Timedelta(
        offset
    )
    #
    buys = df[df["buy_count"] > 0]
    sells = df[df["sell_count"] > 0]
    hdbg.dassert(buys.index.intersection(sells.index).empty)
    bar_buys = buys.groupby(
        ["bar_end_datetime", groupby_id_col], group_keys=True
    ).apply(_aggregate_fills)
    bar_sells = sells.groupby(
        ["bar_end_datetime", groupby_id_col], group_keys=True
    ).apply(_aggregate_fills)
    dict_ = {
        "buy_trade_price": bar_buys.unstack()["price"],
        "sell_trade_price": bar_sells.unstack()["price"],
    }
    df_out = pd.concat(dict_, axis=1)
    return df_out


def aggregate_fills_by_order(df: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregate a fills DataFrame by order id.

    :param df: a fills DataFrame as returned by
        `convert_fills_json_to_dataframe()`
    """
    aggregated = (
        df.set_index(["order", "id"])
        .groupby("order", group_keys=True)
        .apply(_aggregate_fills)
    )
    return aggregated


def _aggregate_fills(df: pd.DataFrame) -> pd.Series:
    # Extract first and last fill timestamps.
    first_timestamp = df["first_timestamp"].min()
    last_timestamp = df["last_timestamp"].max()
    # Extract first and last fill datetimes.
    first_datetime = df["first_datetime"].min()
    last_datetime = df["last_datetime"].max()
    # Get symbol.
    symbols = df["symbol"].unique()
    hdbg.dassert_eq(len(symbols), 1)
    symbol = symbols[0]
    #
    asset_ids = df["asset_id"].unique()
    hdbg.dassert_eq(len(asset_ids), 1)
    asset_id = asset_ids[0]
    # Accumulate buys and sells.
    buy_count = df["buy_count"].sum()
    sell_count = df["sell_count"].sum()
    # Accumulate taker/maker counts.
    taker_count = df["taker_count"].sum()
    maker_count = df["maker_count"].sum()
    # Accumulate share counts and costs.
    amount = df["amount"].sum()
    cost = df["cost"].sum()
    transaction_cost = df["transaction_cost"].sum()
    realized_pnl = df["realized_pnl"].sum()
    # Compute average price.
    price = cost / amount
    # Package the results and return.
    aggregated = pd.Series(
        {
            "first_timestamp": first_timestamp,
            "last_timestamp": last_timestamp,
            "first_datetime": first_datetime,
            "last_datetime": last_datetime,
            "symbol": symbol,
            "asset_id": asset_id,
            "buy_count": buy_count,
            "sell_count": sell_count,
            "taker_count": taker_count,
            "maker_count": maker_count,
            "price": price,
            "amount": amount,
            "cost": cost,
            "transaction_cost": transaction_cost,
            "realized_pnl": realized_pnl,
        }
    )
    return aggregated


# #############################################################################
# TCA
# #############################################################################


def compute_execution_quality(
    df: pd.DataFrame,
    buy_trade_reference_price_col: str,
    sell_trade_reference_price_col: str,
    buy_trade_price_col: str,
    sell_trade_price_col: str,
) -> pd.DataFrame:
    """
    Compute buy/sell slippage in notional and bps wrt reference price.

    Analogous to `cofinanc.compute_execution_quality()` but with reference
    prices that are not necessarily bid/ask prices.

    :param df: DataFrame, possibly with multiple column levels (assets in
        inner level)
    :param buy_trade_reference_price_col: name of col with reference (e.g.,
        TWAP, VWAP) price for buy orders
    :param sell_trade_reference_price_col: name of col with reference price
        for sell orders; may be the same as `buy_trade_reference_price_col`
    :param buy_trade_price_col: name of col with buy trade price
    :param sell_trade_price_col: name of col with sell trade price
    :return: DataFrame, possibly with multiple column levels, of notional and
        relative slippage
    """
    # Compute buy trade slippage.
    buy_trade_slippage_notional = (
        df[buy_trade_price_col] - df[buy_trade_reference_price_col]
    )
    buy_trade_slippage_bps = 1e4 * buy_trade_slippage_notional.divide(
        df[buy_trade_reference_price_col]
    )
    # Compute sell trade slippage.
    sell_trade_slippage_notional = (
        df[sell_trade_reference_price_col] - df[sell_trade_price_col]
    )
    sell_trade_slippage_bps = 1e4 * sell_trade_slippage_notional.divide(
        df[sell_trade_reference_price_col]
    )
    dict_ = {
        "buy_trade_slippage_notional": buy_trade_slippage_notional,
        "buy_trade_slippage_bps": buy_trade_slippage_bps,
        "sell_trade_slippage_notional": sell_trade_slippage_notional,
        "sell_trade_slippage_bps": sell_trade_slippage_bps,
    }
    df_out = pd.concat(dict_, axis=1)
    return df_out


# #############################################################################
# Child order handling.
# #############################################################################


def get_limit_order_price(
    orders: pd.DataFrame, *, freq: str = "1s"
) -> pd.DataFrame:
    """
    Get limit order prices from orders.
    """
    # Generate DataFrame of buy limit orders.
    buys = orders[orders["diff_num_shares"] > 0]
    buys = buys.pivot(
        index="creation_timestamp", columns="asset_id", values="limit_price"
    )
    buys = buys.resample(freq, closed="right", label="right").mean()
    # Generate DataFrame of sell limit orders.
    sells = orders[orders["diff_num_shares"] < 0]
    sells = sells.pivot(
        index="creation_timestamp", columns="asset_id", values="limit_price"
    )
    sells = sells.resample(freq, closed="right", label="right").mean()
    #
    col_set = buys.columns.union(sells.columns)
    col_set = col_set.sort_values()
    buys = buys.reindex(columns=col_set)
    sells = sells.reindex(columns=col_set)
    #
    dict_ = {
        "buy_limit_order_price": buys,
        "sell_limit_order_price": sells,
    }
    df = pd.concat(dict_, axis=1)
    return df


def align_ccxt_orders_and_fills(
    child_order_response_df: pd.DataFrame,
    fills_df: pd.DataFrame,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    cols = [
        "order",
        "order_type",
        "time_in_force",
        "post_only",
        "reduce_only",
        "side",
        "order_price",
        "stop_price",
        "order_amount",
        "order_update_timestamp",
        "order_update_datetime",
    ]
    order_responses = child_order_response_df[cols].set_index("order")
    order_fills = aggregate_fills_by_order(fills_df)
    filled_orders = pd.merge(
        order_responses,
        order_fills,
        how="inner",
        left_index=True,
        right_index=True,
    )
    # TODO(Paul): Compute price improvement for filled orders.
    unfilled_orders = order_responses.loc[
        order_responses.index.difference(order_fills.index)
    ]
    return filled_orders, unfilled_orders


def compute_filled_order_execution_quality(
    df: pd.DataFrame,
    tick_decimals: int,
) -> pd.DataFrame:
    """
    Compute price improvement and fill rate for filled orders.

    :param df: a `filled_orders` DataFrame, from
        `align_ccxt_orders_and_fills()`
    :param tick_decimals: number of decimals of rounding for order quantities.
        This helps suppress machine precision artifacts.
    :return: DataFrame indexed by order id with the following columns:
      - direction (buy or sell as +1 or -1, respectively)
      - price_improvement_notional (with respect to limit price)
      - price_improvement_bps
      - underfill_quantity
      - underfill_pct
      - underfill_notional_at_limit_price
      - underfill_notional_at_transaction_price
    """
    hdbg.dassert_isinstance(df, pd.DataFrame)
    required_cols = [
        "amount",
        "order_amount",
        "order_price",
        "price",
        "side",
    ]
    hdbg.dassert_is_subset(required_cols, df.columns)
    #
    srs_list = []
    is_buy = df["side"] == "buy"
    #
    direction = np.round(2 * is_buy.astype(int) - 1).rename("direction")
    srs_list.append(direction)
    #
    price_improvement_notional = (
        direction.multiply(df["order_price"] - df["price"])
        .round(tick_decimals)
        .replace(-0.0, 0.0)
        .rename("price_improvement_notional")
    )
    srs_list.append(price_improvement_notional)
    #
    price_improvement_bps = 1e4 * (
        price_improvement_notional / df["order_price"]
    ).rename("price_improvement_bps")
    srs_list.append(price_improvement_bps)
    #
    underfill_quantity = (df["order_amount"] - df["amount"]).rename(
        "underfill_quantity"
    )
    srs_list.append(underfill_quantity)
    #
    underfill_pct = (underfill_quantity / df["order_amount"]).rename(
        "underfill_pct"
    )
    srs_list.append(underfill_pct)
    #
    underfill_notional_at_limit_price = (
        underfill_quantity * df["order_price"]
    ).rename("underfill_notional_at_limit_price")
    srs_list.append(underfill_notional_at_limit_price)
    #
    underfill_notional_at_transaction_price = (
        underfill_quantity * df["price"]
    ).rename("underfill_notional_at_transaction_price")
    srs_list.append(underfill_notional_at_transaction_price)
    #
    return pd.concat(srs_list, axis=1)
