{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df822e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T23:23:12.653335Z",
     "start_time": "2024-02-27T23:23:12.637054Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d491129",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Import as:\n",
    "\n",
    "import oms.broker.ccxt.ccxt_logger as obcccclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9105752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T23:23:15.794443Z",
     "start_time": "2024-02-27T23:23:15.715979Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import core.config as cconfig\n",
    "import helpers.hdatetime as hdateti\n",
    "import helpers.hdbg as hdbg\n",
    "import helpers.hio as hio\n",
    "import helpers.hpickle as hpickle\n",
    "import helpers.hprint as hprint\n",
    "import helpers.hwall_clock_time as hwacltim\n",
    "import oms.fill as omfill\n",
    "import oms.order.order as oordorde\n",
    "\n",
    "# TODO(Danya): Redefining to avoid circular dependency with `abstract_ccxt_broker`.\n",
    "CcxtData = Dict[str, Any]\n",
    "\n",
    "_LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_oms_fills(logs_dir: str) -> List[List[omfill.Fill]]:\n",
    "    \"\"\"\n",
    "    Load OMS `Fill`.\n",
    "\n",
    "    :param logs_dir: same as in `CcxtLogger`\n",
    "    :return: OMS `Fill`, where the inner-most list contains per-asset `Fill`\n",
    "        and the outer-most list contains per-bar `Fill`\n",
    "    \"\"\"\n",
    "    # Initialize logger.\n",
    "    logger = CcxtLogger(logs_dir, mode=\"read\")\n",
    "    # Read OMS fills and OMS parent orders from logs.\n",
    "    oms_fills_file_paths = logger._get_files(logger._oms_fills_dir)\n",
    "    oms_parent_orders_file_paths = logger._get_files(\n",
    "        logger._oms_parent_orders_dir, file_extension=\"json\"\n",
    "    )\n",
    "    # `Fills` and `Orders` should have equal number of file paths.\n",
    "    hdbg.dassert_eq(len(oms_fills_file_paths), len(oms_parent_orders_file_paths))\n",
    "    oms_fills = []\n",
    "    for order_path, fill_path in zip(\n",
    "        oms_parent_orders_file_paths, oms_fills_file_paths\n",
    "    ):\n",
    "        orders_data = hio.from_json(order_path, use_types=True)\n",
    "        fills_data = hio.from_json(fill_path, use_types=True)\n",
    "        for order_data, fill_data in zip(orders_data, fills_data):\n",
    "            # Get OMS `Order`.\n",
    "            order = oordorde.Order(\n",
    "                order_data[\"creation_timestamp\"],\n",
    "                order_data[\"asset_id\"],\n",
    "                order_data[\"type_\"],\n",
    "                order_data[\"start_timestamp\"],\n",
    "                order_data[\"end_timestamp\"],\n",
    "                order_data[\"curr_num_shares\"],\n",
    "                order_data[\"diff_num_shares\"],\n",
    "                order_id=order_data[\"order_id\"],\n",
    "                extra_params=order_data[\"extra_params\"],\n",
    "            )\n",
    "            # Get OMS `Fill`.\n",
    "            fill = omfill.Fill(\n",
    "                order,\n",
    "                fill_data[\"timestamp\"],\n",
    "                fill_data[\"num_shares\"],\n",
    "                fill_data[\"price\"],\n",
    "            )\n",
    "            oms_fills.append([fill])\n",
    "    return oms_fills\n",
    "\n",
    "\n",
    "class CcxtLogger:\n",
    "    \"\"\"\n",
    "    Write and read logs for the CcxtBroker.\n",
    "\n",
    "    The logs include:\n",
    "    - `oms.Fills` for all the CCXT trades\n",
    "    - CCXT order responses for all children orders\n",
    "    - Submitted child orders\n",
    "\n",
    "    For more info on logs structure, see `docs/trade_execution/ck.ccxt_broker_logs_schema.reference.md`\n",
    "    \"\"\"\n",
    "\n",
    "    # Default locations of log files.\n",
    "    CCXT_CHILD_ORDER_FILLS = \"child_order_fills\"\n",
    "    CCXT_FILLS = os.path.join(CCXT_CHILD_ORDER_FILLS, \"ccxt_fills\")\n",
    "    CCXT_CHILD_ORDER_TRADES = os.path.join(CCXT_CHILD_ORDER_FILLS, \"ccxt_trades\")\n",
    "    OMS_FILLS = os.path.join(CCXT_CHILD_ORDER_FILLS, \"oms_fills\")\n",
    "    CCXT_CHILD_ORDER_RESPONSE = \"ccxt_child_order_responses\"\n",
    "    OMS_CHILD_ORDERS = \"oms_child_orders\"\n",
    "    OMS_PARENT_ORDERS = \"oms_parent_orders\"\n",
    "    BID_ASK = \"bid_ask\"\n",
    "    POSITIONS = \"positions\"\n",
    "    EXCHANGE_MARKETS = \"exchange_markets\"\n",
    "    LEVERAGE_INFO = \"leverage_info\"\n",
    "    BALANCES = \"balances\"\n",
    "    BROKER_CONFIG = \"broker_config.json\"\n",
    "    ARGS_FILE = \"args.json\"\n",
    "\n",
    "    def __init__(self, log_dir: str, *, mode: str = \"read\"):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        :param log_dir: target directory to put logs in,\n",
    "          e.g. \"/shared_data/system_log_dir_20230315_30minutes/\".\n",
    "        :param mode: For now there are two modes:\n",
    "                    - write: the logger will write log files in `log_dir`\n",
    "                    - read: the logger will initialise functions to read\n",
    "                        log files from `log_dir`\n",
    "        \"\"\"\n",
    "        self._log_dir = log_dir\n",
    "        hdbg.dassert_is_not(self._log_dir, None)\n",
    "        if mode == \"read\":\n",
    "            fields = [\n",
    "                \"args\",\n",
    "                \"broker_config\",\n",
    "                \"ccxt_order_responses\",\n",
    "                \"oms_parent_orders\",\n",
    "                \"ccxt_trades\",\n",
    "                \"oms_child_orders\",\n",
    "                \"oms_fills\",\n",
    "                \"ccxt_fills\",\n",
    "                \"exchange_markets\",\n",
    "                \"leverage_info\",\n",
    "                \"bid_ask_files\",\n",
    "                \"positions\",\n",
    "                \"balances\",\n",
    "                \"reduce_only_order_responses\",\n",
    "                \"reduce_only_child_orders\",\n",
    "            ]\n",
    "            self._has_data = {field: False for field in fields}\n",
    "            self._init_log_subdirectories()\n",
    "        # We aim to log the initial exchange position once and avoid redundant\n",
    "        # position logging. This strategy simplifies the process of mock/replay\n",
    "        # for the exchange, as position updates are based on the executed\n",
    "        # orders rather than continuous logging. By reducing the frequency of\n",
    "        # position logging, we enhance logging efficiency and streamline\n",
    "        # debugging when replaying exchange scenarios.\n",
    "        self._enable_positions_logging = True\n",
    "\n",
    "    # #########################################################################\n",
    "    # Write logs\n",
    "    # #########################################################################\n",
    "\n",
    "    def log_broker_config(self, broker_configuration: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Log the broker configuration. The config is saved as a JSON file in the\n",
    "        root log directory, e.g. `{log_dir}/broker_config.json`\n",
    "\n",
    "        :param broker_configuration: broker config as dict, e.g.\n",
    "        ```\n",
    "        {'bid_ask_lookback': '60S',\n",
    "        'child_order_quantity_computer': {'object_type': 'StaticSchedulingChildOrderQuantityComputer'},\n",
    "        'limit_price_computer': {'max_deviation': 0.01,\n",
    "                                'object_type': 'LimitPriceComputerUsingSpread',\n",
    "                                'passivity_factor': 0.55},\n",
    "        'log_dir': '/app/system_log_dir',\n",
    "        'raw_data_reader_signature': 'realtime.airflow.downloaded_200ms.postgres.bid_ask.futures.v7.ccxt.binance.v1_0_0',\n",
    "        'secret_identifier': 'binance.preprod.trading.4',\n",
    "        'stage': 'preprod',\n",
    "        'universe_version': 'v7.4'}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self._log_dir, self.BROKER_CONFIG)\n",
    "        hio.to_json(file_path, broker_configuration)\n",
    "\n",
    "    # TODO(Danya): Refactor to accept lists of OMS / CCXT child orders.\n",
    "    # TODO(Danya): Pass get_wall_clock_time as the logger attribute.\n",
    "    def log_child_order(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        oms_child_order: oordorde.Order,\n",
    "        ccxt_child_order_response: CcxtData,\n",
    "        extra_info: CcxtData,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log a child order with CCXT order info and additional parameters.\n",
    "\n",
    "        :param log_dir: dir to store logs in\n",
    "            - OMS child order information is saved into a CSV file, while the\n",
    "            corresponding order response from CCXT is saved into a JSON dir, in a\n",
    "            format like:\n",
    "                ```\n",
    "                {log_dir}/oms_child_orders/...\n",
    "                {log_dir}/ccxt_child_order_responses/...\n",
    "                ```\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param oms_child_order: an order to be logged\n",
    "        :param ccxt_child_order_response: CCXT order structure from the exchange,\n",
    "            corresponding to the child order\n",
    "        :param extra_info: values to include into the logged order, for example\n",
    "            `{'bid': 0.277, 'ask': 0.279}`\n",
    "        \"\"\"\n",
    "        # Save reduce-only order in a separate subdirectory.\n",
    "        # This is done to separate market orders used for flattening the account\n",
    "        # from child orders generated by the system.\n",
    "        if \"reduce_only\" in oms_child_order.extra_params:\n",
    "            child_orders_log_dir = os.path.join(self._log_dir, \"reduce_only\")\n",
    "        else:\n",
    "            child_orders_log_dir = self._log_dir\n",
    "        # Add extra_info.\n",
    "        logged_oms_child_order = oms_child_order.to_dict()\n",
    "        hdbg.dassert_not_intersection(\n",
    "            logged_oms_child_order.keys(),\n",
    "            extra_info.keys(),\n",
    "            msg=\"There should be no overlapping keys\",\n",
    "        )\n",
    "        logged_oms_child_order.update(extra_info)\n",
    "        # Add CCXT id to the child order, -1 if there was no response.\n",
    "        logged_oms_child_order[\"ccxt_id\"] = logged_oms_child_order[\n",
    "            \"extra_params\"\n",
    "        ].get(\"ccxt_id\", -1)\n",
    "        # Generate file name.\n",
    "        wall_clock_time_str = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        bar_timestamp = hwacltim.get_current_bar_timestamp(\n",
    "            as_str=True, include_msec=True\n",
    "        )\n",
    "        order_asset_id = logged_oms_child_order[\"asset_id\"]\n",
    "        # 1) Save OMS child orders.\n",
    "        incremental = True\n",
    "        oms_order_log_dir = os.path.join(\n",
    "            child_orders_log_dir, self.OMS_CHILD_ORDERS\n",
    "        )\n",
    "        hio.create_dir(oms_order_log_dir, incremental)\n",
    "        oms_order_file_name = (\n",
    "            f\"{order_asset_id}_{bar_timestamp}.{wall_clock_time_str}.json\"\n",
    "        )\n",
    "        oms_order_file_name = os.path.join(oms_order_log_dir, oms_order_file_name)\n",
    "        hio.to_json(oms_order_file_name, logged_oms_child_order, use_types=True)\n",
    "        _LOG.debug(\n",
    "            \"Saved OMS child orders log file %s\",\n",
    "            hprint.to_str(\"oms_order_file_name\"),\n",
    "        )\n",
    "        # 2) Save CCXT order response.\n",
    "        ccxt_log_dir = os.path.join(\n",
    "            child_orders_log_dir, self.CCXT_CHILD_ORDER_RESPONSE\n",
    "        )\n",
    "        hio.create_dir(ccxt_log_dir, incremental)\n",
    "        response_file_name = (\n",
    "            f\"{order_asset_id}_{bar_timestamp}.{wall_clock_time_str}.json\"\n",
    "        )\n",
    "        response_file_name = os.path.join(ccxt_log_dir, response_file_name)\n",
    "        hio.to_json(response_file_name, ccxt_child_order_response, use_types=True)\n",
    "        _LOG.debug(\n",
    "            \"Saved CCXT child order response log file %s\",\n",
    "            hprint.to_str(\"response_file_name\"),\n",
    "        )\n",
    "\n",
    "    # TODO(gp): P0, @all can we remove the extra dir `child_order_fills`?\n",
    "    def log_ccxt_fills(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        ccxt_fills: List[CcxtData],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save fills and trades to separate files.\n",
    "\n",
    "        :param log_dir: dir to store logs in. The data structure looks like:\n",
    "            ```\n",
    "            {log_dir}/child_order_fills/ccxt_fills/ccxt_fills_20230515-112313.json\n",
    "            ```\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param ccxt_fills: list of CCXT fills loaded from CCXT\n",
    "            - The CCXT objects correspond to closed order (i.e., not in execution\n",
    "              on the exchange any longer)\n",
    "            - Represent the cumulative fill for the closed orders across all\n",
    "              trades\n",
    "        \"\"\"\n",
    "        fills_log_dir = os.path.join(self._log_dir, self.CCXT_CHILD_ORDER_FILLS)\n",
    "        hio.create_dir(fills_log_dir, incremental=True)\n",
    "        # Save CCXT fills, e.g.,\n",
    "        # log_dir/child_order_fills/ccxt_fills/ccxt_fills_20230511-114405.json\n",
    "        timestamp_str = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        ccxt_fills_file_name = os.path.join(\n",
    "            self._log_dir, self.CCXT_FILLS, f\"ccxt_fills_{timestamp_str}.json\"\n",
    "        )\n",
    "        _LOG.debug(hprint.to_str(\"ccxt_fills_file_name\"))\n",
    "        hio.to_json(ccxt_fills_file_name, ccxt_fills, use_types=True)\n",
    "\n",
    "    def log_ccxt_trades(\n",
    "        self,\n",
    "        wall_clock_time: Callable,\n",
    "        ccxt_trades: List[CcxtData],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save fills and trades to separate files.\n",
    "\n",
    "        :param log_dir: dir to store logs in. The data structure looks like:\n",
    "            ```\n",
    "            {log_dir}/child_order_fills/ccxt_trades/ccxt_trades_20230515-112313.json\n",
    "            ```\n",
    "        :param wall_clock_time: the actual wall clock time of the running system\n",
    "            for accounting\n",
    "        :param ccxt_trades: list of CCXT trades corresponding to the CCXT fills\n",
    "        \"\"\"\n",
    "        fills_log_dir = os.path.join(self._log_dir, self.CCXT_CHILD_ORDER_FILLS)\n",
    "        hio.create_dir(fills_log_dir, incremental=True)\n",
    "        timestamp_str = hdateti.timestamp_to_str(wall_clock_time())\n",
    "        # Save CCXT trades, e.g.,\n",
    "        # log_dir/child_order_fills/ccxt_trades/ccxt_trades_20230511-114405.json\n",
    "        ccxt_trades_file_name = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.CCXT_CHILD_ORDER_TRADES,\n",
    "            f\"ccxt_trades_{timestamp_str}.json\",\n",
    "        )\n",
    "        _LOG.debug(hprint.to_str(\"ccxt_trades_file_name\"))\n",
    "        hio.to_json(ccxt_trades_file_name, ccxt_trades, use_types=True)\n",
    "\n",
    "    def log_oms_fills(\n",
    "        self, get_wall_clock_time: Callable, oms_fills: List[omfill.Fill]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log OMS fills into a separate file.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param oms_fills: list of `oms.Fill` objects\n",
    "            - The OMS fill objects correspond to `oms.Fill` objects before they\n",
    "              are submitted upstream to the Portfolio (e.g., before child\n",
    "              orders are merged into parent orders for accounting by the\n",
    "              `Portfolio`).\n",
    "        \"\"\"\n",
    "        if self._log_dir is None:\n",
    "            _LOG.warning(\"No log dir provided.\")\n",
    "            return\n",
    "        # Save OMS fills, e.g.,\n",
    "        # log_dir/child_order_fills/oms_fills/oms_fills_20230511-114405.json\n",
    "        if oms_fills:\n",
    "            oms_fills = [fill.to_dict() for fill in oms_fills]\n",
    "        timestamp_str = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        oms_fills_file_name = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.OMS_FILLS,\n",
    "            f\"oms_fills_{timestamp_str}.json\",\n",
    "        )\n",
    "        _LOG.debug(hprint.to_str(\"oms_fills_file_name\"))\n",
    "        hio.to_json(oms_fills_file_name, oms_fills, use_types=True)\n",
    "\n",
    "    # TODO(gp): Reorganize the format to be a bit regular\n",
    "    # 1) always OMS data before than CCXT\n",
    "    # 2) flatten the directory structure\n",
    "    # 3) use the same format with `bar_timestamp` and `get_wall_clock_time`\n",
    "    #    f\"XYZ.{bar_timestamp}.{get_wall_clock_time}.json\"`\n",
    "    # 4) clarify when orders are parent or child\n",
    "\n",
    "    def log_oms_parent_orders(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        oms_parent_orders: List[oordorde.Order],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log OMS parent orders before dividing them into child orders.\n",
    "\n",
    "        The orders are logged before they are submitted, in the same format\n",
    "        as they are passed from `TargetPositionAndOrderGenerator`.\n",
    "\n",
    "        :param log_dir: dir to store logs in. The data structure looks like:\n",
    "            ```\n",
    "            {log_dir}/oms_parent_orders/oms_parent_orders_{bar_timestamp}.json.{wallclock_time}\n",
    "            # E.g.,\n",
    "            {log_dir}/oms_parent_orders/oms_parent_orders_20230622-084000.json.20230622-084421\n",
    "            ```\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param oms_parent_orders: list of OMS parent orders\n",
    "        \"\"\"\n",
    "        # Generate file name based on the bar timestamp.\n",
    "        # TODO(gp): P1, @all Factor out the logic to format the timestamps.\n",
    "        wall_clock_time = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        bar_timestamp = hwacltim.get_current_bar_timestamp(\n",
    "            as_str=True, include_msec=True\n",
    "        )\n",
    "        # Create enclosing dir.\n",
    "        oms_parent_orders_log_filename = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.OMS_PARENT_ORDERS,\n",
    "            f\"oms_parent_orders_{bar_timestamp}.json\",\n",
    "        )\n",
    "        # Create enclosing dir.\n",
    "        hio.create_enclosing_dir(oms_parent_orders_log_filename, incremental=True)\n",
    "        # Check if there is a parent order log for previous wave of child orders.\n",
    "        # The parent order log contains information which is updated\n",
    "        # with each new wave, e.g. CCXT IDs of bound child orders.\n",
    "        hio.rename_file_if_exists(\n",
    "            oms_parent_orders_log_filename,\n",
    "            wall_clock_time,\n",
    "            before_extension=False,\n",
    "        )\n",
    "        # Serialize and save parent orders as JSON file.\n",
    "        oms_parent_orders = [order.to_dict() for order in oms_parent_orders]\n",
    "        hio.to_json(\n",
    "            oms_parent_orders_log_filename, oms_parent_orders, use_types=True\n",
    "        )\n",
    "        _LOG.debug(hprint.to_str(\"oms_parent_orders_log_filename\"))\n",
    "\n",
    "    def log_exchange_markets(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        exchange_markets: CcxtData,\n",
    "        leverage_info: CcxtData,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log exchange info on markets and leverage info from CCXT Exchange as\n",
    "        JSON.\n",
    "\n",
    "        The use-case `ReplayedCcxtExchange` where these information assist in\n",
    "        replaying an experiment perfectly.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param exchange_markets: info on markets provided by CCXT Exchange\n",
    "            Download exchange_markets. See more about the output format:\n",
    "            https://docs.ccxt.com/#/README?id=market-structure.\n",
    "        :param leverage_info: leverage info returned for the desired `symbol` by\n",
    "            CCXT Exchange.\n",
    "            Download leverage information. See more about the output format:\n",
    "            https://docs.ccxt.com/#/README?id=leverage-tiers-structure.\n",
    "        \"\"\"\n",
    "        # Generate file name based on the bar timestamp.\n",
    "        wall_clock_time = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        # Create enclosing dir.\n",
    "        exchange_market_log_filename = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.EXCHANGE_MARKETS,\n",
    "            f\"exchange_markets.{wall_clock_time}.json\",\n",
    "        )\n",
    "        leverage_info_log_filename = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.LEVERAGE_INFO,\n",
    "            f\"leverage_info.{wall_clock_time}.json\",\n",
    "        )\n",
    "        # Create enclosing dir.\n",
    "        hio.to_json(\n",
    "            exchange_market_log_filename, exchange_markets, use_types=True\n",
    "        )\n",
    "        hio.to_json(leverage_info_log_filename, leverage_info, use_types=True)\n",
    "        _LOG.debug(hprint.to_str(\"exchange_market_log_filename\"))\n",
    "\n",
    "    def log_bid_ask_data(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        bid_ask_data: pd.DataFrame,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log bid_ask data for replay the CCXT exchange. The logging happens\n",
    "        immediately after data is fetched via `RawDataReader` prior to any\n",
    "        transformations.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param bid_ask_data: data frame received from `RawDataReader`\n",
    "        \"\"\"\n",
    "        # Generate file name based on the bar timestamp.\n",
    "        wall_clock_time = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        bar_timestamp = hwacltim.get_current_bar_timestamp(\n",
    "            as_str=True, include_msec=True\n",
    "        )\n",
    "        bid_ask_log_filename = os.path.join(\n",
    "            self._log_dir,\n",
    "            self.BID_ASK,\n",
    "            f\"{bar_timestamp}.{wall_clock_time}.csv\",\n",
    "        )\n",
    "        # Create enclosing dir.\n",
    "        hio.create_enclosing_dir(bid_ask_log_filename, incremental=True)\n",
    "        # Bid ask data has timestamp index which is also needed.\n",
    "        bid_ask_data.to_csv(bid_ask_log_filename, index=True)\n",
    "\n",
    "    def log_positions(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        positions: CcxtData,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log current positions from CCXT Exchange as JSON.\n",
    "\n",
    "        Using replayed exchange, It is possible to reconstruct the\n",
    "        portfolio at any given moment using the CCXT order responses,\n",
    "        but the missing piece of information is the initial USDT\n",
    "        balance.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param positions: info current positions provided by CCXT\n",
    "            Exchange Download positions. See more about the output\n",
    "            format: https://docs.ccxt.com/#/README?id=positions.\n",
    "        \"\"\"\n",
    "        dir_name = self.POSITIONS\n",
    "        file_name_tag = \"positions\"\n",
    "        if self._enable_positions_logging:\n",
    "            self._log_raw_data(\n",
    "                dir_name, file_name_tag, get_wall_clock_time, positions\n",
    "            )\n",
    "            self._enable_positions_logging = False\n",
    "\n",
    "    def log_balance(\n",
    "        self,\n",
    "        get_wall_clock_time: Callable,\n",
    "        balance: Dict[str, any],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log current balance from CCXT Exchange as JSON.\n",
    "\n",
    "        Using replayed exchange, It is possible to reconstruct the portfolio at\n",
    "        any given moment using the CCXT order responses, but the missing piece\n",
    "        of information is the initial USDT balance which will now be obtained\n",
    "        using this balance log.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        :param balance: info current balance provided by CCXT Exchange\n",
    "            `fetchBalance`. See more about the output format:\n",
    "            https://docs.ccxt.com/#/README?id=balance-structure.\n",
    "        \"\"\"\n",
    "        dir_name = self.BALANCES\n",
    "        file_name_tag = \"balance\"\n",
    "        self._log_raw_data(dir_name, file_name_tag, get_wall_clock_time, balance)\n",
    "\n",
    "    # #########################################################################\n",
    "    # Read logs\n",
    "    # #########################################################################\n",
    "\n",
    "    def load_all_data(\n",
    "        self,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load all data.\n",
    "\n",
    "        :param convert_to_dataframe: loads the data into a Pandas DataFrame\n",
    "            instead of the default Dict.\n",
    "        :param abort_on_missing_data: allow to continue or abort when some data\n",
    "            (e.g., data corresponding to `ccxt_order_responses`) is missing.\n",
    "            Typically, we assume that all the data should be present. This param is\n",
    "            useful when we want to read data from previous runs that don't have all\n",
    "            the information after updating the format, or incomplete data.\n",
    "        :return: dictionary storing the logged data\n",
    "        \"\"\"\n",
    "        all_data = {\n",
    "            \"args\": self.load_args(abort_on_missing_data=abort_on_missing_data),\n",
    "            \"broker_config\": self.load_broker_config(\n",
    "                abort_on_missing_data=abort_on_missing_data\n",
    "            ),\n",
    "            \"ccxt_order_responses\": self.load_ccxt_order_response(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"oms_parent_orders\": self.load_oms_parent_order(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"ccxt_trades\": self.load_ccxt_trades(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"oms_child_orders\": self.load_oms_child_order(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"oms_fills\": self.load_oms_fills(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"ccxt_fills\": self.load_ccxt_fills(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"exchange_markets\": self.load_exchange_markets(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"leverage_info\": self.load_leverage_info(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"bid_ask_files\": self.load_bid_ask_files(\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"positions\": self.load_positions(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            \"balances\": self.load_balances(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "            ),\n",
    "            # Load reduce-only orders response placed for account flattening.\n",
    "            \"reduce_only_order_responses\": self.load_ccxt_order_response(\n",
    "                convert_to_dataframe=convert_to_dataframe,\n",
    "                abort_on_missing_data=abort_on_missing_data,\n",
    "                reduce_only=True,\n",
    "            ),\n",
    "        }\n",
    "        return all_data\n",
    "\n",
    "    def load_broker_config(\n",
    "        self, *, abort_on_missing_data: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Load the broker configuration.\n",
    "\n",
    "        For example of the data see `log_broker_configuration`.\n",
    "\n",
    "        :return: broker configuration as dict\n",
    "        \"\"\"\n",
    "        data_key = \"broker_config\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return {}\n",
    "        file_name = \"broker_config.json\"\n",
    "        file_path = os.path.join(self._log_dir, file_name)\n",
    "        broker_config = hio.from_json(file_path)\n",
    "        return broker_config\n",
    "\n",
    "    def load_ccxt_order_response(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "        reduce_only: bool = False,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load CCXT order responses from the JSON files in the log directory as\n",
    "        Dict or DataFrame.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "        :param reduce_only parameter if True, only reduce_only orders are loaded.\n",
    "\n",
    "        The order response is a CCXT order structure, as described in\n",
    "        https://docs.ccxt.com/#/?id=order-structure.\n",
    "\n",
    "        E.g., a timeseries from return DataFrame looks like:\n",
    "        ```\n",
    "        info                    {'orderId': '7954906695', 'symbol': 'APEUSDT',...\n",
    "        order                   7954906695\n",
    "        client_order_id         x-xcKtGhcub89989e55d47273a3610a9\n",
    "        timestamp               1678898138582\n",
    "        datetime                2023-03-15 16:35:38.582000+00:00\n",
    "        last_trade_timestamp    None\n",
    "        symbol                  APE/USDT\n",
    "        order_type              limit\n",
    "        time_in_force           GTC\n",
    "        post_only               False\n",
    "        reduce_only             False\n",
    "        side                    buy\n",
    "        order_price             4.12\n",
    "        stop_price              NaN\n",
    "        order_amount            10.0\n",
    "        cost                    0.0\n",
    "        average                 NaN\n",
    "        filled                  0.0\n",
    "        remaining               10.0\n",
    "        status                  open\n",
    "        fee                     NaN\n",
    "        trades                  []\n",
    "        fees                    []\n",
    "        order_update_timestamp  1678898138582\n",
    "        order_update_datetime   1970-01-01 00:27:58.898138582+00:00\n",
    "        ```\n",
    "        \"\"\"\n",
    "        # Get the files.\n",
    "        if reduce_only:\n",
    "            data_key = \"reduce_only_order_responses\"\n",
    "            if not self._has_data[data_key]:\n",
    "                self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "                return []\n",
    "            dir_name = self._reduce_only_order_responses_dir\n",
    "        else:\n",
    "            data_key = \"ccxt_order_responses\"\n",
    "            if not self._has_data[data_key]:\n",
    "                self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "                return []\n",
    "            dir_name = self._ccxt_order_responses_dir\n",
    "        ccxt_order_responses = self._load_raw_data(dir_name)\n",
    "        if convert_to_dataframe:\n",
    "            ccxt_order_responses = (\n",
    "                self._convert_ccxt_order_structures_to_dataframe(\n",
    "                    ccxt_order_responses\n",
    "                )\n",
    "            )\n",
    "        return ccxt_order_responses\n",
    "\n",
    "    def load_oms_parent_order(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load parent orders from the JSON files in the log directory as Dict or\n",
    "        DataFrame.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as\n",
    "            `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        # Get the files.\n",
    "        # Specify the file extension as JSON, as OMS parent order folder\n",
    "        # contains multiple copies of the same parent orders with extra\n",
    "        # params updated at each wave of child orders.\n",
    "        # Latest OMS parent orders have the file name pattern:\n",
    "        # `oms_parent_orders_{bar_timestamp}.json`.\n",
    "        # Outdated OMS parent order for previous waves:\n",
    "        # `oms_parent_orders_{bar_timestamp}.json.{wall_clock_time}`.\n",
    "        # `wall_clock_time` indicates the point up to which these logs were relevant.\n",
    "        data_key = \"oms_parent_orders\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        files = self._get_files(\n",
    "            self._oms_parent_orders_dir, file_extension=\"json\"\n",
    "        )\n",
    "        # Read all the files.\n",
    "        parent_orders = []\n",
    "        # Load individual orders as pd.Series.\n",
    "        for path in tqdm(\n",
    "            files, desc=f\"Loading files from '{self._oms_parent_orders_dir}'\"\n",
    "        ):\n",
    "            # Load parent order files from JSON format.\n",
    "            data = hio.from_json(path, use_types=True)\n",
    "            parent_orders.extend(data)\n",
    "        if convert_to_dataframe:\n",
    "            parent_orders = self._convert_oms_parent_orders_to_dataframe(\n",
    "                parent_orders\n",
    "            )\n",
    "        return parent_orders\n",
    "\n",
    "    def load_oms_child_order(\n",
    "        self,\n",
    "        *,\n",
    "        unpack_extra_params: bool = False,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "        reduce_only: bool = False,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load child orders from the JSON files in the log directory as Dict or\n",
    "        DataFrame.\n",
    "\n",
    "        :param unpack_extra_params: if True, unpack `extra_params` field into\n",
    "            output DataFrame columns\n",
    "        :param convert_to_dataframe: same interface as `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "        :param reduce_only parameter if True, only reduce_only orders are loaded.\n",
    "\n",
    "        Example of data returned as DataFrame:\n",
    "        ```\n",
    "                creation_timestamp              asset_id    type_  \\\n",
    "        order_id\n",
    "        20       2023-03-15 16:35:37.825835+00:00  6051632686  limit\n",
    "        21       2023-03-15 16:35:38.718960+00:00  8717633868  limit\n",
    "\n",
    "                start_timestamp               end_timestamp  \\\n",
    "        order_id\n",
    "        20       2023-03-15 16:35:37.825835+00:00 2023-03-15 16:36:37.825835+00:00\n",
    "        21       2023-03-15 16:35:38.718960+00:00 2023-03-15 16:36:38.718960+00:00\n",
    "\n",
    "                curr_num_shares  diff_num_shares  tz            passivity_factor    \\\n",
    "        order_id\n",
    "        20       0.0          10.0        America/New_York   0.55\n",
    "        21       0.0          -1.0        America/New_York    0.55\n",
    "\n",
    "                latest_bid_price  latest_ask_price  bid_price_mean  ask_price_mean  \\\n",
    "        order_id\n",
    "        20       4.120             4.121    4.125947    4.126974\n",
    "        21       15.804            15.805   15.818162   15.819432\n",
    "\n",
    "                used_bid_price    used_ask_price  limit_price      ccxt_id  \\\n",
    "        order_id\n",
    "        20       latest_bid_price  latest_ask_price    4.12045   7954906695\n",
    "        21       latest_bid_price  latest_ask_price    15.80455  14412582631\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the files.\n",
    "        if reduce_only:\n",
    "            data_key = \"reduce_only_child_orders\"\n",
    "            if not self._has_data[data_key]:\n",
    "                self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "                return []\n",
    "            dir_name = self._reduce_only_child_order_dir\n",
    "        else:\n",
    "            data_key = \"oms_child_orders\"\n",
    "            if not self._has_data[data_key]:\n",
    "                self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "                return []\n",
    "            dir_name = self._oms_child_orders_dir\n",
    "        child_orders = self._load_raw_data(dir_name)\n",
    "        if convert_to_dataframe:\n",
    "            child_orders = self._convert_oms_child_orders_to_dataframe(\n",
    "                child_orders,\n",
    "                unpack_extra_params=unpack_extra_params,\n",
    "            )\n",
    "        return child_orders\n",
    "\n",
    "    def load_ccxt_trades(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load trades from the JSON files in the log directory as Dict or\n",
    "        DataFrame.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "\n",
    "        Example of data returned as Dict:\n",
    "        ```\n",
    "        {\n",
    "        \"info\": {\n",
    "            \"symbol\": \"APEUSDT\",\n",
    "            \"id\": \"356819245\",\n",
    "            \"orderId\": \"8077704766\",\n",
    "            \"side\": \"SELL\",\n",
    "            \"price\": \"4.0400\",\n",
    "            \"qty\": \"6\",\n",
    "            \"realizedPnl\": 0.0,\n",
    "            \"marginAsset\": \"USDT\",\n",
    "            \"quoteQty\": \"24.2400\",\n",
    "            \"commission\": \"0.00969600\",\n",
    "            \"commissionAsset\": \"USDT\",\n",
    "            \"time\": \"1679560540879\",\n",
    "            \"positionSide\": \"BOTH\",\n",
    "            \"buyer\": False,\n",
    "            \"maker\": False\n",
    "        },\n",
    "            \"timestamp\": 1679560540879,\n",
    "            \"datetime\": \"Timestamp('2023-03-23 08:35:40.879000+0000', tz='UTC')\",\n",
    "            \"symbol\": \"APE/USDT\",\n",
    "            \"asset_id\": 6051632686,\n",
    "            \"id\": 356819245,\n",
    "            \"order\": 8077704766,\n",
    "            \"type\": None,\n",
    "            \"side\": \"sell\",\n",
    "            \"takerOrMaker\": \"taker\",\n",
    "            \"price\": 4.04,\n",
    "            \"amount\": 6.0,\n",
    "            \"cost\": 24.24,\n",
    "            \"fee\": {\n",
    "                \"cost\": 0.009696,\n",
    "                \"currency\": \"USDT\"\n",
    "            },\n",
    "            \"fees\": [\n",
    "                {\n",
    "                    \"currency\": \"USDT\",\n",
    "                    \"cost\": 0.009696\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        Example of data returned as DataFrame:\n",
    "        ```\n",
    "                                timestamp    symbol         id       order  side  \\\n",
    "        0 2022-09-29 16:46:39.509000+00:00  APE/USDT  282773274  5772340563  sell\n",
    "        1 2022-09-29 16:51:58.567000+00:00  APE/USDT  282775654  5772441841   buy\n",
    "        2 2022-09-29 16:57:00.267000+00:00  APE/USDT  282779084  5772536135   buy\n",
    "        3 2022-09-29 17:02:00.329000+00:00  APE/USDT  282780259  5772618089  sell\n",
    "        4 2022-09-29 17:07:03.097000+00:00  APE/USDT  282781536  5772689853   buy\n",
    "\n",
    "        takerOrMaker  price  amount    cost      fees fees_currency realized_pnl\n",
    "        0        taker  5.427     5.0  27.135  0.010854          USDT            0\n",
    "        1        taker  5.398     6.0  32.388  0.012955          USDT   0.14500000\n",
    "        2        taker  5.407     3.0  16.221  0.006488          USDT            0\n",
    "        3        taker  5.395     9.0  48.555  0.019422          USDT  -0.03900000\n",
    "        4        taker  5.381     8.0  43.048  0.017219          USDT   0.07000000\n",
    "        ```\n",
    "        \"\"\"\n",
    "        data_key = \"ccxt_trades\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._ccxt_trades_dir\n",
    "        ccxt_child_order_trades = self._load_raw_data(dir_name, append_list=False)\n",
    "        if convert_to_dataframe:\n",
    "            # Convert fills to DataFrame.\n",
    "            ccxt_child_order_trades_duped = (\n",
    "                self._convert_ccxt_trades_json_to_dataframe(\n",
    "                    ccxt_child_order_trades\n",
    "                )\n",
    "            )\n",
    "            # Remove full duplicates for fills.\n",
    "            # Note: generally fills are loaded in bulk via CCXT `fetchMyTrades()`\n",
    "            # method, which allows only for queries based on time range and full\n",
    "            # symbol. In some cases, there is an overlap between queries, which leads\n",
    "            # to full duplicates appearing in the resulting DataFrame.\n",
    "            # Duplicates are determined via timestamp, `id` of the trade and `order` id.\n",
    "            ccxt_child_order_trades = (\n",
    "                ccxt_child_order_trades_duped.drop_duplicates(\n",
    "                    subset=[\"timestamp\", \"id\", \"order\"], keep=\"last\"\n",
    "                )\n",
    "            )\n",
    "            # TODO(pp): Use hdbg.to_perc to get a better output.\n",
    "            _LOG.debug(\n",
    "                \"%s duplicates were removed from original data.\",\n",
    "                ccxt_child_order_trades_duped.shape[0]\n",
    "                - ccxt_child_order_trades.shape[0],\n",
    "            )\n",
    "        return ccxt_child_order_trades\n",
    "\n",
    "    def load_oms_fills(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the oms fills from the JSON files in the log directory as Dict or\n",
    "        DataFrame.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "\n",
    "        Example of data returned as Dict:\n",
    "        ```\n",
    "        {\n",
    "            \"asset_id\": 5115052901,\n",
    "            \"fill_id\": 6,\n",
    "            \"timestamp\": \"2023-05-23T11:58:50.201000+00:00\",\n",
    "            \"num_shares\": 93.0,\n",
    "            \"price\": 69.5733\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        Example of data returned as DataFrame:\n",
    "        ```\n",
    "           asset_id         fill_id  timestamp                      num_shares    price\n",
    "        0  8717633868        2  2023-05-23T11:58:49.172000+00:00       1.000  14.7540\n",
    "        1  1467591036        4  2023-05-23T11:58:49.687000+00:00       0.001  27.3304\n",
    "        2  5115052901        6  2023-05-23T11:58:50.201000+00:00      93.000  69.5733\n",
    "        3  3401245610        7  2023-05-23T11:58:50.456000+00:00       3.000   6.5460\n",
    "        ```\n",
    "        \"\"\"\n",
    "        data_key = \"oms_fills\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._oms_fills_dir\n",
    "        oms_child_order_fills = self._load_raw_data(dir_name, append_list=False)\n",
    "        if convert_to_dataframe:\n",
    "            oms_child_order_fills = pd.DataFrame(oms_child_order_fills)\n",
    "        return oms_child_order_fills\n",
    "\n",
    "    def load_ccxt_fills(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the CCXT filled order from the JSON files in the log directory as\n",
    "        Dict or DataFrame.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "\n",
    "        These representations correspond to closed orders.\n",
    "\n",
    "        Example of one entity of input JSON data:\n",
    "        ```\n",
    "        {\n",
    "        \"info\": {\n",
    "            \"orderId\": \"2187830797\",\n",
    "            \"symbol\": \"CTKUSDT\",\n",
    "            \"status\": \"FILLED\",\n",
    "            \"clientOrderId\": \"x-xcKtGhcuda5dde8563a5c1568a5893\",\n",
    "            \"price\": \"0.74810\",\n",
    "            \"avgPrice\": \"0.74810\",\n",
    "            \"origQty\": \"93\",\n",
    "            \"executedQty\": \"93\",\n",
    "            \"cumQuote\": \"69.57330\",\n",
    "            \"timeInForce\": \"GTC\",\n",
    "            \"type\": \"LIMIT\",\n",
    "            \"reduceOnly\": False,\n",
    "            \"closePosition\": False,\n",
    "            \"side\": \"BUY\",\n",
    "            \"positionSide\": \"BOTH\",\n",
    "            \"stopPrice\": \"0\",\n",
    "            \"workingType\": \"CONTRACT_PRICE\",\n",
    "            \"priceProtect\": False,\n",
    "            \"origType\": \"LIMIT\",\n",
    "            \"time\": \"1684843130201\",\n",
    "            \"updateTime\": 1684843130201\n",
    "        },\n",
    "        \"id\": 2187830797,\n",
    "        \"clientOrderId\": \"x-xcKtGhcuda5dde8563a5c1568a5893\",\n",
    "        \"timestamp\": 1684843130201,\n",
    "        \"datetime\": \"Timestamp('2023-05-23 11:58:50.201000+0000', tz='UTC')\",\n",
    "        \"lastTradeTimestamp\": None,\n",
    "        \"symbol\": \"CTK/USDT\",\n",
    "        \"type\": \"limit\",\n",
    "        \"timeInForce\": \"GTC\",\n",
    "        \"postOnly\": False,\n",
    "        \"reduceOnly\": False,\n",
    "        \"side\": \"buy\",\n",
    "        \"price\": 0.7481,\n",
    "        \"stopPrice\": \"nan\",\n",
    "        \"amount\": 93.0,\n",
    "        \"cost\": 69.5733,\n",
    "        \"average\": 0.7481,\n",
    "        \"filled\": 93.0,\n",
    "        \"remaining\": 0.0,\n",
    "        \"status\": \"closed\",\n",
    "        \"fee\": \"nan\",\n",
    "        \"trades\": [],\n",
    "        \"fees\": []\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        E.g., a timeseries from return DataFrame looks like:\n",
    "        ```\n",
    "        info                      {'orderId': '14901643572', 'symbol': 'AVAXUSDT...\n",
    "        order                      14901643572\n",
    "        client_order_id                x-xcKtGhcu8261da0e4a2533b528e0e2\n",
    "        timestamp                   1684843129172\n",
    "        datetime                    2023-05-23 11:58:49.172000+00:00\n",
    "        last_trade_timestamp             None\n",
    "        symbol                     AVAX/USDT\n",
    "        order_type                   limit\n",
    "        time_in_force                 GTC\n",
    "        post_only                   False\n",
    "        reduce_only                  False\n",
    "        side                       buy\n",
    "        order_price                  14.761\n",
    "        stop_price                   NaN\n",
    "        order_amount                 1.0\n",
    "        cost                      14.754\n",
    "        average                    14.754\n",
    "        filled                      1.0\n",
    "        remaining                   0.0\n",
    "        status                     closed\n",
    "        fee                       NaN\n",
    "        trades                     []\n",
    "        fees                      []\n",
    "        order_update_timestamp          1684843129172\n",
    "        order_update_datetime           1970-01-01 00:28:04.843129172+00:00\n",
    "        ```\n",
    "        \"\"\"\n",
    "        data_key = \"ccxt_fills\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._ccxt_fills_dir\n",
    "        ccxt_fills = self._load_raw_data(dir_name, append_list=False)\n",
    "        if convert_to_dataframe:\n",
    "            ccxt_fills = self._convert_ccxt_order_structures_to_dataframe(\n",
    "                ccxt_fills\n",
    "            )\n",
    "        return ccxt_fills\n",
    "\n",
    "    def load_bid_ask_files(self, *, abort_on_missing_data: bool = True) -> List:\n",
    "        \"\"\"\n",
    "        Load the list of bid ask file paths.\n",
    "\n",
    "        Bid ask files are in CSV format, these contain the raw data logged\n",
    "        directly from `RawDataReader` prior to any transformations.\n",
    "\n",
    "        :param abort_on_missing_data: same interface as `load_all_data()`.\n",
    "\n",
    "        Example of data returned:\n",
    "        [\n",
    "        \"log/bid_ask/20231009_212500.20231009-172500.csv\",\n",
    "        \"log/bid_ask/20231009_212500.20231009-172600.csv\",\n",
    "        \"log/bid_ask/20231009_212500.20231009-172700.csv\",\n",
    "        \"log/bid_ask/20231009_212500.20231009-172800.csv\",\n",
    "        ]\n",
    "        \"\"\"\n",
    "        data_key = \"bid_ask_files\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        files = self._get_files(self._bid_ask_dir)\n",
    "        return files\n",
    "\n",
    "    def load_exchange_markets(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the exchange markets data from the JSON files in the log directory\n",
    "        as Dict.\n",
    "\n",
    "        The exchange markets has the exchange structure, as described in\n",
    "        https://docs.ccxt.com/#/?id=exchange-structure.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as\n",
    "            `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        data_key = \"exchange_markets\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._exchange_markets_dir\n",
    "        exchange_markets = self._load_raw_data(dir_name)\n",
    "        if convert_to_dataframe:\n",
    "            exchange_markets = self._convert_raw_data_to_dataframe(\n",
    "                exchange_markets\n",
    "            )\n",
    "        return exchange_markets\n",
    "\n",
    "    def load_leverage_info(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the leverage info data from JSON files in the log directory as\n",
    "        Dict.\n",
    "\n",
    "        The leverage info has the leverage tiers structure, as described\n",
    "        in\n",
    "        https://docs.ccxt.com/#/?id=leverage-tiers-structure.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as\n",
    "            `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        data_key = \"leverage_info\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._leverage_info_dir\n",
    "        leverage_info = self._load_raw_data(dir_name)\n",
    "        if convert_to_dataframe:\n",
    "            leverage_info = self._convert_raw_data_to_dataframe(leverage_info)\n",
    "        return leverage_info\n",
    "\n",
    "    def load_positions(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the positions data from JSON.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as\n",
    "            `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        data_key = \"positions\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._positions_dir\n",
    "        positions = self._load_raw_data(dir_name, append_list=False)\n",
    "        # Convert data type str to float.\n",
    "        for position in positions:\n",
    "            position[\"info\"][\"positionAmt\"] = float(\n",
    "                position[\"info\"][\"positionAmt\"]\n",
    "            )\n",
    "        if convert_to_dataframe:\n",
    "            positions = pd.DataFrame(positions)\n",
    "        return positions\n",
    "\n",
    "    def load_balances(\n",
    "        self,\n",
    "        *,\n",
    "        convert_to_dataframe: bool = False,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the balance data from JSON.\n",
    "\n",
    "        The balance JSON files has the balance structure, as described\n",
    "        in\n",
    "        https://docs.ccxt.com/#/README?id=balance-structure.\n",
    "\n",
    "        :param convert_to_dataframe: same interface as\n",
    "            `load_all_data()`.\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        data_key = \"balances\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return []\n",
    "        dir_name = self._balances_dir\n",
    "        balances = self._load_raw_data(dir_name, append_list=True)\n",
    "        if convert_to_dataframe:\n",
    "            balances = pd.DataFrame(balances)\n",
    "        return balances\n",
    "\n",
    "    def load_args(\n",
    "        self,\n",
    "        *,\n",
    "        abort_on_missing_data: bool = True,\n",
    "    ) -> Union[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Load the arguments used for the run from JSON.\n",
    "\n",
    "        The args JSON files has the following structure,\n",
    "        ```\n",
    "        {\n",
    "            \"secret_id\": 4,\n",
    "            \"max_parent_order_notional\": 100,\n",
    "            \"randomize_orders\": true,\n",
    "            \"log_dir\": \"/shared_data/sameep/daily_experiments/20231205_experiment3\",\n",
    "            \"clean_up_before_run\": true,\n",
    "            \"clean_up_after_run\": true,\n",
    "            \"parent_order_duration_in_min\": 5,\n",
    "            \"num_parent_orders_per_bar\": 5,\n",
    "            \"num_bars\": 6,\n",
    "            \"close_positions_using_twap\": false,\n",
    "            \"db_stage\": \"prod\",\n",
    "            \"child_order_execution_freq\": \"10S\",\n",
    "            \"incremental\": false,\n",
    "            \"log_level\": \"DEBUG\"\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        :param abort_on_missing_data: same interface as\n",
    "            `load_all_data()`.\n",
    "        \"\"\"\n",
    "        data_key = \"args\"\n",
    "        if not self._has_data[data_key]:\n",
    "            self._fatal_missing_data(data_key, abort_on_missing_data)\n",
    "            return {}\n",
    "        file_name = self.ARGS_FILE\n",
    "        file_path = os.path.join(self._log_dir, file_name)\n",
    "        args = hio.from_json(file_path)\n",
    "        return args\n",
    "\n",
    "    # #########################################################################\n",
    "    # Private methods\n",
    "    # #########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_files(\n",
    "        log_dir: str, *, file_extension: Optional[str] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get a list of files from the directory.\n",
    "\n",
    "        :param file_extension: added to the file search pattern, e.g.\n",
    "            \"json\". Can be used with or without the initial \".\"\n",
    "        \"\"\"\n",
    "        pattern = \"*\"\n",
    "        if file_extension:\n",
    "            pattern += file_extension\n",
    "        only_files = True\n",
    "        use_relative_paths = False\n",
    "        files: List[str] = hio.listdir(\n",
    "            log_dir, pattern, only_files, use_relative_paths\n",
    "        )\n",
    "        files.sort()\n",
    "        return files\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_ccxt_order_structures_to_dataframe(\n",
    "        ccxt_order_structures: List[CcxtData],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a list of CCXT order structures to a DataFrame.\n",
    "\n",
    "        `child_order_response` and `ccxt_fill` are exactly the same data\n",
    "        structures (see http://docs.ccxt.com/#/?id=order-structure) that\n",
    "        we encode as `obcaccbr.CcxtData` (`Dict[str, Any]` under the\n",
    "        hood). Thus, we refer to the common data structure as\n",
    "        `ccxt_order_structure` and use this function for both data\n",
    "        structures.\n",
    "        \"\"\"\n",
    "        ccxt_order_structures_series_list = []\n",
    "        for ccxt_order_structure in ccxt_order_structures:\n",
    "            # Skip the response JSON if it is empty.\n",
    "            if (\n",
    "                not ccxt_order_structure\n",
    "                or ccxt_order_structure.get(\"empty\") == True\n",
    "            ):\n",
    "                continue\n",
    "            srs = pd.Series(\n",
    "                ccxt_order_structure.values(), ccxt_order_structure.keys()\n",
    "            )\n",
    "            # Get order update Unix timestamp from the exchange.\n",
    "            info_timestamp = ccxt_order_structure[\"info\"][\"updateTime\"]\n",
    "            srs[\"info_timestamp\"] = info_timestamp\n",
    "            # Convert to datetime.\n",
    "            # Note: using 'coerce' since we expect the exchange to return a\n",
    "            # correct timestamp, and raising would block the loading of the\n",
    "            # DataFrame altogether.\n",
    "            srs[\"info_datetime\"] = pd.to_datetime(\n",
    "                pd.to_numeric(info_timestamp, errors=\"coerce\"),\n",
    "                unit=\"ms\",\n",
    "                utc=True,\n",
    "            )\n",
    "            ccxt_order_structures_series_list.append(srs)\n",
    "        ccxt_order_df = pd.concat(ccxt_order_structures_series_list, axis=1).T\n",
    "        # Transform timestamp columns to UTC.\n",
    "        ccxt_order_df = ccxt_order_df.map(\n",
    "            lambda x: x.tz_convert(\"UTC\") if isinstance(x, pd.Timestamp) else x\n",
    "        )\n",
    "        #  Check the timestamp logs in `CcxtBroker` and update the conversion accordingly.\n",
    "        # Rename columns.\n",
    "        ccxt_order_df = ccxt_order_df.rename(\n",
    "            columns={\n",
    "                \"id\": \"order\",\n",
    "                \"clientOrderId\": \"client_order_id\",\n",
    "                \"lastTradeTimestamp\": \"last_trade_timestamp\",\n",
    "                \"type\": \"order_type\",\n",
    "                \"timeInForce\": \"time_in_force\",\n",
    "                \"postOnly\": \"post_only\",\n",
    "                \"reduceOnly\": \"reduce_only\",\n",
    "                \"price\": \"order_price\",\n",
    "                \"stopPrice\": \"stop_price\",\n",
    "                \"amount\": \"order_amount\",\n",
    "                \"info_timestamp\": \"order_update_timestamp\",\n",
    "                \"info_datetime\": \"order_update_datetime\",\n",
    "            }\n",
    "        )\n",
    "        # Convert id data type to int.\n",
    "        ccxt_order_df[\"order\"] = ccxt_order_df[\"order\"].astype(int)\n",
    "        return ccxt_order_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_fills_dataframe(fills_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Validate a fills DataFrame and normalize for chaining\n",
    "        `obccagfu._aggregate_fills()`.\n",
    "\n",
    "        Ensure `df` is a DataFrame with certain columns and restricted values.\n",
    "\n",
    "        :param df: a fills DataFrame as returned by\n",
    "            `convert_fills_json_to_dataframe()`\n",
    "        \"\"\"\n",
    "        # Sanity-check the DataFrame.\n",
    "        hdbg.dassert_isinstance(fills_df, pd.DataFrame)\n",
    "        required_cols = [\n",
    "            \"timestamp\",\n",
    "            \"datetime\",\n",
    "            \"side\",\n",
    "            \"takerOrMaker\",\n",
    "            \"price\",\n",
    "            \"amount\",\n",
    "            \"cost\",\n",
    "            \"transaction_cost\",\n",
    "        ]\n",
    "        hdbg.dassert_is_subset(required_cols, fills_df.columns)\n",
    "        # Ensure categoricals lie in expected sets.\n",
    "        hdbg.dassert_is_subset(fills_df[\"side\"].unique(), [\"buy\", \"sell\"])\n",
    "        hdbg.dassert_is_subset(\n",
    "            fills_df[\"takerOrMaker\"].unique(), [\"taker\", \"maker\", np.nan]\n",
    "        )\n",
    "        # Check self-consistency of prices/costs/amounts.\n",
    "        hdbg.dassert_lte(\n",
    "            ((fills_df[\"cost\"] / fills_df[\"amount\"]) - fills_df[\"price\"]).sum(),\n",
    "            1e-6,\n",
    "        )\n",
    "        #\n",
    "        fills_df = fills_df.copy()\n",
    "        fills_df[\"datetime\"] = pd.to_datetime(fills_df[\"datetime\"])\n",
    "        # Assign additional datetime columns.\n",
    "        fills_df[\"first_timestamp\"] = fills_df[\"timestamp\"]\n",
    "        fills_df[\"last_timestamp\"] = fills_df[\"timestamp\"]\n",
    "        fills_df[\"first_datetime\"] = fills_df[\"datetime\"]\n",
    "        fills_df[\"last_datetime\"] = fills_df[\"datetime\"]\n",
    "        # Accumulate buy/sell counts.\n",
    "        fills_df[\"buy_count\"] = (fills_df[\"side\"] == \"buy\").astype(int)\n",
    "        fills_df[\"sell_count\"] = (fills_df[\"side\"] == \"sell\").astype(int)\n",
    "        # Accumulate taker/maker counts.\n",
    "        fills_df[\"taker_count\"] = (fills_df[\"takerOrMaker\"] == \"taker\").astype(\n",
    "            int\n",
    "        )\n",
    "        fills_df[\"maker_count\"] = (fills_df[\"takerOrMaker\"] == \"maker\").astype(\n",
    "            int\n",
    "        )\n",
    "        # Process fills data for volume.\n",
    "        fills_df[\"buy_volume\"] = np.where(\n",
    "            fills_df[\"side\"] == \"buy\", fills_df[\"amount\"], 0\n",
    "        )\n",
    "        fills_df[\"sell_volume\"] = np.where(\n",
    "            fills_df[\"side\"] == \"sell\", fills_df[\"amount\"], 0\n",
    "        )\n",
    "        fills_df[\"taker_volume\"] = np.where(\n",
    "            fills_df[\"takerOrMaker\"] == \"taker\", fills_df[\"amount\"], 0\n",
    "        )\n",
    "        fills_df[\"maker_volume\"] = np.where(\n",
    "            fills_df[\"takerOrMaker\"] == \"maker\", fills_df[\"amount\"], 0\n",
    "        )\n",
    "        # Process fills data for notional.\n",
    "        fills_df[\"buy_notional\"] = np.where(\n",
    "            fills_df[\"side\"] == \"buy\", fills_df[\"cost\"], 0\n",
    "        )\n",
    "        fills_df[\"sell_notional\"] = np.where(\n",
    "            fills_df[\"side\"] == \"sell\", fills_df[\"cost\"], 0\n",
    "        )\n",
    "        fills_df[\"taker_notional\"] = np.where(\n",
    "            fills_df[\"takerOrMaker\"] == \"taker\", fills_df[\"cost\"], 0\n",
    "        )\n",
    "        fills_df[\"maker_notional\"] = np.where(\n",
    "            fills_df[\"takerOrMaker\"] == \"maker\", fills_df[\"cost\"], 0\n",
    "        )\n",
    "        return fills_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_oms_fills_to_dataframe(\n",
    "        oms_fills: List[CcxtData],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a list of OMS fill dicts into a DataFrame.\n",
    "        \"\"\"\n",
    "        oms_fills_df = pd.DataFrame(oms_fills)\n",
    "        #\n",
    "        expected_columns = [\n",
    "            \"asset_id\",\n",
    "            \"fill_id\",\n",
    "            \"timestamp\",\n",
    "            \"num_shares\",\n",
    "            \"price\",\n",
    "        ]\n",
    "        hdbg.dassert_set_eq(oms_fills_df.columns, expected_columns)\n",
    "        return oms_fills_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_raw_data_to_dataframe(\n",
    "        raw_data_list: List[Dict[str, Any]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a list of raw data dicts into a DataFrame.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame()\n",
    "        for raw_data in raw_data_list:\n",
    "            raw_data = pd.DataFrame.from_dict(raw_data, orient=\"index\")\n",
    "            df = pd.concat([df, raw_data])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _path_exists(path: str) -> bool:\n",
    "        exists = os.path.exists(path)\n",
    "        if not exists:\n",
    "            txt = f\"Path '{path}' doesn't exist!\"\n",
    "            _LOG.warning(txt)\n",
    "        return exists\n",
    "\n",
    "    @staticmethod\n",
    "    def _fatal_missing_data(data_key: str, abort_on_missing_data: bool) -> None:\n",
    "        if abort_on_missing_data:\n",
    "            raise FileNotFoundError(\n",
    "                \"Missing required data for '%s': returning empty data\", data_key\n",
    "            )\n",
    "        else:\n",
    "            _LOG.warning(\n",
    "                \"Missing required data for '%s': continuing as per user request\",\n",
    "                data_key,\n",
    "            )\n",
    "\n",
    "    def _init_log_subdirectories(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate and store subdirectories with Broker logs.\n",
    "\n",
    "        The directory is expected to follow the same structure, with\n",
    "        fills and orders stored in '{log_dir}/'\n",
    "        \"\"\"\n",
    "        # TODO(Sameep): Update the init to support the previous versions of the logs.\n",
    "        # Will be added in CmTask5749.\n",
    "        if self._log_dir is None:\n",
    "            _LOG.info(\n",
    "                \"No log directory provided, loading log files is impossible.\"\n",
    "            )\n",
    "            return\n",
    "        hdbg.dassert_path_exists(self._log_dir)\n",
    "        # Check if the broker config is saved.\n",
    "        # The presence of the config is not strictly enforced since earlier\n",
    "        # experiments did not dump the config data.\n",
    "        broker_config_file = os.path.join(self._log_dir, self.BROKER_CONFIG)\n",
    "        if self._path_exists(broker_config_file):\n",
    "            self._has_data[\"broker_config\"] = True\n",
    "            self._broker_config_file = broker_config_file\n",
    "        args_file = os.path.join(self._log_dir, self.ARGS_FILE)\n",
    "        if self._path_exists(args_file):\n",
    "            self._has_data[\"args\"] = True\n",
    "            self._args_file = args_file\n",
    "        child_order_fills_dir = os.path.join(\n",
    "            self._log_dir, self.CCXT_CHILD_ORDER_FILLS\n",
    "        )\n",
    "        # Verify that the fills dir conforms to expected structure.\n",
    "        # If there are no subdirectories, the reader expects to find child order\n",
    "        # fill logs. If there are subdirectories, the reader expect them to\n",
    "        # conform to the structure outlined in the constructor.\n",
    "        if self._path_exists(child_order_fills_dir):\n",
    "            child_order_fills_dir_contents = os.scandir(child_order_fills_dir)\n",
    "            has_subdirectories = any(\n",
    "                [path.is_dir() for path in child_order_fills_dir_contents]\n",
    "            )\n",
    "            if has_subdirectories:\n",
    "                # Get a subdirectory for CCXT fill representations.\n",
    "                # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/ccxt_fills'.\n",
    "                ccxt_fills_dir = os.path.join(self._log_dir, self.CCXT_FILLS)\n",
    "                if self._path_exists(ccxt_fills_dir):\n",
    "                    self._has_data[\"ccxt_fills\"] = True\n",
    "                    self._ccxt_fills_dir = ccxt_fills_dir\n",
    "                # Get a subdirectory for CCXT trades.\n",
    "                # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/ccxt_trades'.\n",
    "                ccxt_child_order_trades_dir = os.path.join(\n",
    "                    self._log_dir, self.CCXT_CHILD_ORDER_TRADES\n",
    "                )\n",
    "                if self._path_exists(ccxt_child_order_trades_dir):\n",
    "                    self._has_data[\"ccxt_trades\"] = True\n",
    "                    self._ccxt_trades_dir = ccxt_child_order_trades_dir\n",
    "                # Get a subdirectory for OMS fills.\n",
    "                # E.g. 'system_log_dir_20230315_30minutes/child_order_fills/oms_fills'.\n",
    "                oms_fills_dir = os.path.join(self._log_dir, self.OMS_FILLS)\n",
    "                if self._path_exists(oms_fills_dir):\n",
    "                    self._has_data[\"oms_fills\"] = True\n",
    "                    self._oms_fills_dir = oms_fills_dir\n",
    "            else:\n",
    "                # Get subdirectory for CCXT representations child order fills.\n",
    "                # E.g. 'system_log_dir_20230315_30minutes/child_order_fills'.\n",
    "                self._ccxt_trades_dir = child_order_fills_dir\n",
    "        # Get subdirectory for CCXT order responses.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/ccxt_child_order_responses'.\n",
    "        ccxt_order_responses_dir = os.path.join(\n",
    "            self._log_dir, self.CCXT_CHILD_ORDER_RESPONSE\n",
    "        )\n",
    "        if self._path_exists(ccxt_order_responses_dir):\n",
    "            self._has_data[\"ccxt_order_responses\"] = True\n",
    "            self._ccxt_order_responses_dir = ccxt_order_responses_dir\n",
    "        # Get subdirectory for submitted child orders.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/oms_child_orders'.\n",
    "        oms_child_orders_dir = os.path.join(self._log_dir, self.OMS_CHILD_ORDERS)\n",
    "        if self._path_exists(oms_child_orders_dir):\n",
    "            self._has_data[\"oms_child_orders\"] = True\n",
    "            self._oms_child_orders_dir = oms_child_orders_dir\n",
    "        # Get subdirectory for parent orders.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/oms_parent_orders/'.\n",
    "        oms_parent_orders_dir = os.path.join(\n",
    "            self._log_dir, self.OMS_PARENT_ORDERS\n",
    "        )\n",
    "        if self._path_exists(oms_parent_orders_dir):\n",
    "            self._has_data[\"oms_parent_orders\"] = True\n",
    "            self._oms_parent_orders_dir = oms_parent_orders_dir\n",
    "        # Get subdirectory for bid ask data.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/bid_ask/'.\n",
    "        bid_ask_dir = os.path.join(self._log_dir, self.BID_ASK)\n",
    "        if self._path_exists(bid_ask_dir):\n",
    "            self._has_data[\"bid_ask_files\"] = True\n",
    "            self._bid_ask_dir = bid_ask_dir\n",
    "        # Get subdirectory for exchange markets.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/exchange_markets/'.\n",
    "        exchange_markets_dir = os.path.join(self._log_dir, self.EXCHANGE_MARKETS)\n",
    "        if self._path_exists(exchange_markets_dir):\n",
    "            self._has_data[\"exchange_markets\"] = True\n",
    "            self._exchange_markets_dir = exchange_markets_dir\n",
    "        # Get subdirectory for leverage info.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/leverage_info/'.\n",
    "        leverage_info_dir = os.path.join(self._log_dir, self.LEVERAGE_INFO)\n",
    "        if self._path_exists(leverage_info_dir):\n",
    "            self._has_data[\"leverage_info\"] = True\n",
    "            self._leverage_info_dir = leverage_info_dir\n",
    "        # Get subdirectory for positions.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/log/positions/'.\n",
    "        positions_dir = os.path.join(self._log_dir, self.POSITIONS)\n",
    "        if self._path_exists(positions_dir):\n",
    "            self._has_data[\"positions\"] = True\n",
    "            self._positions_dir = positions_dir\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/log/balances/'.\n",
    "        balances_dir = os.path.join(self._log_dir, self.BALANCES)\n",
    "        if self._path_exists(balances_dir):\n",
    "            self._has_data[\"balances\"] = True\n",
    "            self._balances_dir = balances_dir\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/log/reduce_only/ccxt_child_order_responses'.\n",
    "        reduce_only_order_responses_dir = os.path.join(\n",
    "            self._log_dir, \"reduce_only\", self.CCXT_CHILD_ORDER_RESPONSE\n",
    "        )\n",
    "        if self._path_exists(reduce_only_order_responses_dir):\n",
    "            self._has_data[\"reduce_only_order_responses\"] = True\n",
    "            self._reduce_only_order_responses_dir = (\n",
    "                reduce_only_order_responses_dir\n",
    "            )\n",
    "        # TODO(Sameep): Replace this subdirectory-based reduce-only shift with\n",
    "        # one based on order parameters.\n",
    "        # E.g. 'system_log_dir_20230315_30minutes/log/reduce_only/ccxt_child_order_responses'.\n",
    "        reduce_only_child_order_dir = os.path.join(\n",
    "            self._log_dir, \"reduce_only\", self.OMS_CHILD_ORDERS\n",
    "        )\n",
    "        if self._path_exists(reduce_only_child_order_dir):\n",
    "            self._has_data[\"reduce_only_child_orders\"] = True\n",
    "            self._reduce_only_child_order_dir = reduce_only_child_order_dir\n",
    "\n",
    "    def _convert_oms_parent_orders_to_dataframe(\n",
    "        self, oms_parent_orders: List[CcxtData]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a list of OMS parent orders into a single DataFrame and\n",
    "        normalize.\n",
    "\n",
    "        Example of input OMS parent orders:\n",
    "        [OrderedDict([('order_id', 0),\n",
    "        ('creation_timestamp',\n",
    "        Timestamp('2023-06-21 10:15:49.589872-0400',\n",
    "        tz='America/New_York')),                         ('asset_id',\n",
    "        3065029174),                         ('type_', 'price@twap'),\n",
    "        ('start_timestamp',\n",
    "        Timestamp('2023-06-21 10:15:49.589872-0400',\n",
    "        tz='America/New_York')),\n",
    "        ('end_timestamp',                         Timestamp('2023-06-21\n",
    "        10:20:49.589872-0400', tz='America/New_York')),\n",
    "        ('curr_num_shares', 0.0),\n",
    "        ('diff_num_shares', -900.0),                         ('tz',\n",
    "        <DstTzInfo 'America/New_York' EST-1 day, 19:00:00 STD>),\n",
    "        ('extra_params', {})]),             OrderedDict([('order_id',\n",
    "        1),                         ('creation_timestamp',\n",
    "        Timestamp('2023-06-21 10:15:49.590399-0400',\n",
    "        tz='America/New_York')),                         ('asset_id',\n",
    "        6051632686),                         ('type_', 'limit'),\n",
    "        ('start_timestamp',\n",
    "        Timestamp('2023-06-21 10:15:49.590399-0400',\n",
    "        tz='America/New_York')),\n",
    "        ('end_timestamp',                         Timestamp('2023-06-21\n",
    "        10:20:49.590399-0400', tz='America/New_York')),\n",
    "        ('curr_num_shares', 0.0),\n",
    "        ('diff_num_shares', 33.0),                         ('tz',\n",
    "        <DstTzInfo 'America/New_York' EST-1 day, 19:00:00 STD>),\n",
    "        ('extra_params', {})])]         Example of output:         ```\n",
    "        creation_timestamp    asset_id       type_\n",
    "        start_timestamp                    end_timestamp\n",
    "        curr_num_shares  diff_num_shares               tz\n",
    "        extra_params order_id 0        2023-06-22 08:44:21.852271+00:00\n",
    "        3065029174  price@twap 2023-06-22 08:44:21.852271+00:00\n",
    "        2023-06-22 08:49:21.852271+00:00              0.0\n",
    "        -900.0  America/New_York           {} 1        2023-06-22\n",
    "        08:44:21.852754+00:00  6051632686       limit 2023-06-22\n",
    "        08:44:21.852754+00:00 2023-06-22 08:49:21.852754+00:00\n",
    "        0.0             33.0  America/New_York           {}         ```\n",
    "        \"\"\"\n",
    "        # Combine into a single DataFrame.\n",
    "        oms_parent_orders_df = pd.DataFrame(oms_parent_orders)\n",
    "        oms_parent_orders_df[\"order_id\"] = oms_parent_orders_df[\n",
    "            \"order_id\"\n",
    "        ].astype(int)\n",
    "        oms_parent_orders_df = oms_parent_orders_df.set_index(\"order_id\")\n",
    "        # Transform timestamp columns to UTC.\n",
    "        oms_parent_orders_df = oms_parent_orders_df.map(\n",
    "            lambda x: x.tz_convert(\"UTC\") if isinstance(x, pd.Timestamp) else x\n",
    "        )\n",
    "        return oms_parent_orders_df\n",
    "\n",
    "    def _convert_oms_child_orders_to_dataframe(\n",
    "        self,\n",
    "        oms_child_orders: List[CcxtData],\n",
    "        *,\n",
    "        unpack_extra_params: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert list of OMS child order Series into a DataFrame and normalize.\n",
    "\n",
    "        Example of input OMS child order:\n",
    "        ```\n",
    "        {\n",
    "            \"order_id\": 20,\n",
    "            \"creation_timestamp\": \"Timestamp('2023-03-15 12:35:37.825835-0400', tz='pytz.FixedOffset(-240)')\",\n",
    "            \"asset_id\": 6051632686,\n",
    "            \"type_\": \"limit\",\n",
    "            \"start_timestamp\": \"Timestamp('2023-03-15 12:35:37.825835-0400', tz='pytz.FixedOffset(-240)')\",\n",
    "            \"end_timestamp\": \"Timestamp('2023-03-15 12:36:37.825835-0400', tz='pytz.FixedOffset(-240)')\",\n",
    "            \"curr_num_shares\": 0.0,\n",
    "            \"diff_num_shares\": 10.0,\n",
    "            \"tz\": \"America/New_York\",\n",
    "            \"passivity_factor\": 0.55,\n",
    "            \"latest_bid_price\": 4.12,\n",
    "            \"latest_ask_price\": 4.121,\n",
    "            \"bid_price_mean\": 4.125947368421053,\n",
    "            \"ask_price_mean\": 4.126973684210526,\n",
    "            \"used_bid_price\": \"latest_bid_price\",\n",
    "            \"used_ask_price\": \"latest_ask_price\",\n",
    "            \"limit_price\": 4.12045,\n",
    "            \"ccxt_id\": 7954906695\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        E.g., a timeseries from return DataFrame:\n",
    "        ```\n",
    "        order_id                 20\n",
    "        creation_timestamp          2023-03-15 12:35:37.825835-04:00\n",
    "        asset_id                 6051632686\n",
    "        type_                   limit\n",
    "        start_timestamp            2023-03-15 12:35:37.825835-04:00\n",
    "        end_timestamp             2023-03-15 12:36:37.825835-04:00\n",
    "        curr_num_shares            0.0\n",
    "        diff_num_shares            10.0\n",
    "        tz                     America/New_York\n",
    "        passivity_factor            0.55                                                                                                                 latest_bid_price                                                   4.12\n",
    "        latest_ask_price            4.121\n",
    "        bid_price_mean             4.125947368421053\n",
    "        ask_price_mean             4.126973684210526                                                                                                                 used_bid_price                                         latest_bid_price\n",
    "        used_ask_price             latest_ask_price                                                                                                                 limit_price                                                     4.12045\n",
    "        ccxt_id                  7954906695\n",
    "        extra_params              {'order_generated_timestamp': datetime.datetim...\n",
    "        ```\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame.\n",
    "        oms_child_orders_series_list = []\n",
    "        for oms_child_order_srs in oms_child_orders:\n",
    "            order_id = oms_child_order_srs[\"order_id\"]\n",
    "            # Name each series according to internal order_id.\n",
    "            oms_child_order_srs[\"name\"] = order_id\n",
    "            oms_child_orders_series_list.append(oms_child_order_srs)\n",
    "        oms_child_orders_df = pd.DataFrame(oms_child_orders_series_list)\n",
    "        oms_child_orders_df[\"order_id\"] = oms_child_orders_df[\"order_id\"].astype(\n",
    "            int\n",
    "        )\n",
    "        oms_child_orders_df = oms_child_orders_df.set_index(\"order_id\")\n",
    "        # Transform timestamp columns to UTC.\n",
    "        oms_child_orders_df = oms_child_orders_df.map(\n",
    "            lambda x: x.tz_convert(\"UTC\") if isinstance(x, pd.Timestamp) else x\n",
    "        )\n",
    "        # Unpack ccxt_id from the list format.\n",
    "        # E.g. [12028516372] -> 12028516372.\n",
    "        hdbg.dassert_in(\"ccxt_id\", oms_child_orders_df.columns)\n",
    "        # Make sure that all CCXT ID lists have length of 1.\n",
    "        hdbg.dassert_eq_all(set(oms_child_orders_df[\"ccxt_id\"].str.len()), {1})\n",
    "        oms_child_orders_df[\"ccxt_id\"] = oms_child_orders_df[\"ccxt_id\"].apply(\n",
    "            lambda x: x[0]\n",
    "        )\n",
    "        # Convert ccxt id data type to int.\n",
    "        oms_child_orders_df[\"ccxt_id\"] = oms_child_orders_df[\"ccxt_id\"].astype(\n",
    "            int\n",
    "        )\n",
    "        # Extract number of submission attempts.\n",
    "        # Verify that `stats` is present in all child order `extra_params`.\n",
    "        stats_are_present = all(\n",
    "            [\n",
    "                \"stats\" in extra_params\n",
    "                for extra_params in oms_child_orders_df[\"extra_params\"].to_list()\n",
    "            ]\n",
    "        )\n",
    "        hdbg.dassert(\n",
    "            stats_are_present,\n",
    "            msg=\"`stats` not present in all child orders `extra_params`.\",\n",
    "        )\n",
    "        # Get the key for the number of the successful submission attempt.\n",
    "        # TODO(Danya): This will break if the calling function ever changes.\n",
    "        # A better solution would be to either log `attempt_num` separately from\n",
    "        # `stats` (like ccxt_id) or find the key by prefix.\n",
    "        attempt_num_stats_key = \"_submit_single_order_to_ccxt::attempt_num\"\n",
    "        attempt_num_is_present = all(\n",
    "            [\n",
    "                attempt_num_stats_key in extra_params[\"stats\"]\n",
    "                for extra_params in oms_child_orders_df[\"extra_params\"].to_list()\n",
    "            ]\n",
    "        )\n",
    "        hdbg.dassert(\n",
    "            attempt_num_is_present,\n",
    "            msg=\"`attempt_num_present_cond` not present in all `extra_params['stats']`.\",\n",
    "        )\n",
    "        oms_child_orders_df[\"attempt_num\"] = oms_child_orders_df[\n",
    "            \"extra_params\"\n",
    "        ].apply(lambda x: x[\"stats\"][attempt_num_stats_key])\n",
    "        if unpack_extra_params:\n",
    "            # Unpack 'extra params' dictionary into a DataFrame.\n",
    "            extra_params_df = pd.json_normalize(\n",
    "                oms_child_orders_df[\"extra_params\"], sep=\"_\"\n",
    "            ).set_index(oms_child_orders_df.index)\n",
    "            # Remove duplicated columns that were unpacked earlier.\n",
    "            # The columns are `ccxt_id` and `attempt_num`, which are logged\n",
    "            # in the child order `extra_params['stats']` during submission.\n",
    "            extra_params_df = extra_params_df.drop(\n",
    "                [\"ccxt_id\", \"stats_\" + attempt_num_stats_key], axis=1\n",
    "            )\n",
    "            # Add extra_params columns to the child orders DataFrame.\n",
    "            oms_child_orders_df = pd.concat(\n",
    "                [oms_child_orders_df, extra_params_df], axis=1\n",
    "            )\n",
    "            # Remove the extra_params column to avoid data duplication.\n",
    "            oms_child_orders_df = oms_child_orders_df.drop(\n",
    "                [\"extra_params\"], axis=1\n",
    "            )\n",
    "        return oms_child_orders_df\n",
    "\n",
    "    def _convert_ccxt_trades_json_to_dataframe(\n",
    "        self, trades_json: List[CcxtData]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert JSON-format trades into a DataFrame.\n",
    "\n",
    "        - Unpack nested values;\n",
    "        - Convert unix epoch to pd.Timestamp;\n",
    "        - Remove duplicated information;\n",
    "        \"\"\"\n",
    "        hdbg.dassert_lte(1, len(trades_json))\n",
    "        trades = pd.DataFrame(trades_json)\n",
    "        hdbg.dassert_in(\"asset_id\", trades.columns)\n",
    "        # Extract nested values.\n",
    "        # Note: `transaction_cost` is extracted from the `fee`\n",
    "        #  value for the base/quote currency.\n",
    "        #  See https://docs.ccxt.com/#/?id=trade-structure.\n",
    "        #\n",
    "        trades[\"transaction_cost\"] = [fee[\"cost\"] for fee in trades[\"fee\"]]\n",
    "        trades[\"fees_currency\"] = [fee[\"currency\"] for fee in trades[\"fee\"]]\n",
    "        trades[\"realized_pnl\"] = [info[\"realizedPnl\"] for info in trades[\"info\"]]\n",
    "        # Force conversion of PnL to float.\n",
    "        # PnL is extracted from `info` field, which stores all values as strings.\n",
    "        trades[\"realized_pnl\"] = trades[\"realized_pnl\"].astype(float)\n",
    "        # Convert order ID and trade ID to int.\n",
    "        trades[\"order\"] = trades[\"order\"].astype(int)\n",
    "        trades[\"id\"] = trades[\"id\"].astype(int)\n",
    "        # Replace unix epoch with a timestamp.\n",
    "        trades[\"timestamp\"] = trades[\"timestamp\"].apply(\n",
    "            hdateti.convert_unix_epoch_to_timestamp\n",
    "        )\n",
    "        # Set columns.\n",
    "        columns = [\n",
    "            \"timestamp\",\n",
    "            \"datetime\",\n",
    "            \"symbol\",\n",
    "            \"asset_id\",\n",
    "            \"id\",\n",
    "            \"order\",\n",
    "            \"side\",\n",
    "            \"takerOrMaker\",\n",
    "            \"price\",\n",
    "            \"amount\",\n",
    "            \"cost\",\n",
    "            \"transaction_cost\",\n",
    "            \"fees_currency\",\n",
    "            \"realized_pnl\",\n",
    "        ]\n",
    "        trades = trades[columns]\n",
    "        # Normalize data types.\n",
    "        trades = self._normalize_fills_dataframe(trades)\n",
    "        # Set timestamp index.\n",
    "        trades = trades.set_index(\"timestamp\", drop=False)\n",
    "        return trades\n",
    "\n",
    "    def _log_raw_data(\n",
    "        self,\n",
    "        dir_name: str,\n",
    "        file_name_tag: str,\n",
    "        get_wall_clock_time: Callable,\n",
    "        data: CcxtData,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log raw data from CCXT Exchange as JSON.\n",
    "\n",
    "        :param get_wall_clock_time: retrieve the current wall clock time\n",
    "        \"\"\"\n",
    "        # Generate file name based on the bar timestamp.\n",
    "        wall_clock_time = hdateti.timestamp_to_str(get_wall_clock_time())\n",
    "        # Create enclosing dir.\n",
    "        log_filename = os.path.join(\n",
    "            self._log_dir,\n",
    "            dir_name,\n",
    "            f\"{file_name_tag}.{wall_clock_time}.json\",\n",
    "        )\n",
    "        # Create enclosing dir.\n",
    "        hio.to_json(log_filename, data, use_types=True)\n",
    "        _LOG.debug(hprint.to_str(\"log_filename\"))\n",
    "\n",
    "    def _load_raw_data(\n",
    "        self,\n",
    "        dir_name: str,\n",
    "        *,\n",
    "        append_list: bool = True,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load raw data from the JSON files in the log directory.\n",
    "\n",
    "        :param append_list: Set to True for DataFrame.extend() or False\n",
    "            for DataFrame.append().\n",
    "        \"\"\"\n",
    "        files = self._get_files(dir_name)\n",
    "        data_list = []\n",
    "        for path in tqdm(files, desc=f\"Loading `{dir_name}` files...\"):\n",
    "            data = hio.from_json(path, use_types=True)\n",
    "            if append_list:\n",
    "                data_list.append(data)\n",
    "            else:\n",
    "                data_list.extend(data)\n",
    "        return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d6bbb",
   "metadata": {},
   "source": [
    "Config loading\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042bc434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T23:23:19.642440Z",
     "start_time": "2024-02-27T23:23:19.631578Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_config_for_execution_analysis(system_log_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the configuration used for execution analysis.\n",
    "\n",
    "    Receive a full path to the log directory for a full system or broker-only\n",
    "    experiment, e.g.\n",
    "    '/shared_data/ecs/test/.../system_log_dir.manual/process_forecasts'\n",
    "\n",
    "    :param system_log_dir: dir used in the execution analysis notebook.\n",
    "    If ends with 'process_forecasts', the function expects the config file\n",
    "    to be located a level above the input directory.\n",
    "    \"\"\"\n",
    "    #\n",
    "    config_file_path = system_log_dir.rstrip(\"/\")\n",
    "    # Return to a level higher for the full system log dir.\n",
    "    if config_file_path.endswith(\"process_forecasts\"):\n",
    "        config_file_path = config_file_path.rstrip(\"process_forecasts\")\n",
    "    # Get a full path to the config pickle file.\n",
    "    file_name = \"system_config.output.values_as_strings.pkl\"\n",
    "    config_file_path = os.path.join(config_file_path, file_name)\n",
    "    _LOG.info(\"Loading config from %s\", config_file_path)\n",
    "    # Show a warning if the file does not exist.\n",
    "    # This could indicate an error or a broker-only run.\n",
    "    if not os.path.exists(config_file_path):\n",
    "        _LOG.warning(\"Config file %s does not exist\", config_file_path)\n",
    "        config = None\n",
    "    # Load the config as a string.\n",
    "    else:\n",
    "        config_pkl = hpickle.from_pickle(config_file_path)\n",
    "        config = cconfig.Config.from_dict(config_pkl)\n",
    "        config = config.to_string(\"only_values\").replace(\"\\\\n\", \"\\n\")\n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5504f6",
   "metadata": {},
   "source": [
    "Child order timestamp processing\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16e0155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T23:23:23.153419Z",
     "start_time": "2024-02-27T23:23:23.124399Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_timestamps_and_prices_for_a_single_order(\n",
    "    child_order: pd.Series, fills_df: pd.DataFrame, resample_freq: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Organize submission event and CCXT trade timestamps and related prices.\n",
    "\n",
    "    Apply `process_child_order_execution_timestamps` and\n",
    "    `process_child_order_trades_timestamps` for a single child order,\n",
    "    and add multilevel index on `asset_id`.\n",
    "\n",
    "    :param child_order: a single child order from `oms_child_order_df`\n",
    "        Example:\n",
    "\n",
    "        creation_timestamp                         2023-08-29 12:35:00.939164+00:00\n",
    "        asset_id                                                         1030828978\n",
    "        type_                                                                 limit\n",
    "        start_timestamp                            2023-08-29 12:35:00.939164+00:00\n",
    "        end_timestamp                                     2023-08-29 12:36:00+00:00\n",
    "        curr_num_shares                                                         0.0\n",
    "        diff_num_shares                                                      -162.0\n",
    "        tz                                                         America/New_York\n",
    "        extra_params              {'stats': {'_submit_twap_child_order::wave_id'...\n",
    "        passivity_factor                                                       0.55\n",
    "        latest_bid_price                                                     0.1552\n",
    "        latest_ask_price                                                     0.1553\n",
    "        bid_price_mean                                                       0.1552\n",
    "        ask_price_mean                                                       0.1553\n",
    "        used_bid_price                                             latest_bid_price\n",
    "        used_ask_price                                             latest_ask_price\n",
    "        exchange_timestamp                         2023-08-29 12:34:58.561000+00:00\n",
    "        knowledge_timestamp                        2023-08-29 12:34:58.710276+00:00\n",
    "        end_download_timestamp                     2023-08-29 12:34:58.685410+00:00\n",
    "        limit_price                                                        0.155255\n",
    "        ccxt_id                                                         12049653207\n",
    "        name                                                                      9\n",
    "        Name: 9, dtype: object\n",
    "    :param fills_df: output of `load_ccxt_trades_df()`\n",
    "    :param resample_freq: frequency to resample timestamps to. Should be equal\n",
    "    to the doubled order book sampling frequency, e.g.,\n",
    "    for \"200ms\" order book sampling -> \"100ms\".\n",
    "    :return: DataFrame of event timestamps and prices for a single order, e.g.:\n",
    "                                                                                event limit_price signed_quantity snapshot_ask_price snapshot_bid_price\n",
    "                                                                            1030828978  1030828978      1030828978         1030828978         1030828978\n",
    "    timestamp\n",
    "    2023-09-01 10:34:58.100000+00:00                                knowledge_timestamp         NaN             NaN             0.1583             0.1582\n",
    "    2023-09-01 10:35:00.900000+00:00  _submit_twap_child_order::child_order.limit_pr...    0.158245           160.0                NaN                NaN\n",
    "    2023-09-01 10:35:05+00:00           _submit_twap_child_order::child_order.submitted         NaN             NaN                NaN                NaN\n",
    "    2023-09-01 10:35:23.600000+00:00                                            trade.1    0.158200           160.0                NaN                NaN\n",
    "    2023-09-01 10:36:00+00:00                                             end_timestamp         NaN             NaN                NaN                NaN\n",
    "    \"\"\"\n",
    "    #\n",
    "    execution_timestamps = process_child_order_execution_timestamps(child_order)\n",
    "    #\n",
    "    trade_timestamps = process_child_order_trades_timestamps(\n",
    "        fills_df, child_order.ccxt_id\n",
    "    )\n",
    "    # TODO(Danya): Move this section to both functions above so that they can\n",
    "    # be called separately with the same kind of output.\n",
    "    df_subset = pd.concat([execution_timestamps, trade_timestamps])\n",
    "    df_subset = df_subset.reset_index()\n",
    "    df_subset = df_subset.set_index(\"timestamp\")\n",
    "    # Sort index, since trades are not located in the correct order with\n",
    "    # respect to order submission events.\n",
    "    df_subset = df_subset.sort_index()\n",
    "    # Resample.\n",
    "    df_subset.index = df_subset.index.ceil(resample_freq)\n",
    "    out_df = pd.concat({child_order[\"asset_id\"]: df_subset}, axis=1)\n",
    "    out_df = out_df.swaplevel(axis=1).sort_index(axis=1)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def process_child_order_execution_timestamps(\n",
    "    child_order: pd.Series,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Organize timestamps associated to order book snapshot and limit order.\n",
    "\n",
    "    :param child_order: a single child order from `oms_child_order_df`\n",
    "        Example:\n",
    "\n",
    "        creation_timestamp                         2023-08-29 12:35:00.939164+00:00\n",
    "        asset_id                                                         1030828978\n",
    "        type_                                                                 limit\n",
    "        start_timestamp                            2023-08-29 12:35:00.939164+00:00\n",
    "        end_timestamp                                     2023-08-29 12:36:00+00:00\n",
    "        curr_num_shares                                                         0.0\n",
    "        diff_num_shares                                                      -162.0\n",
    "        tz                                                         America/New_York\n",
    "        extra_params              {'stats': {'_submit_twap_child_order::wave_id'...\n",
    "        passivity_factor                                                       0.55\n",
    "        latest_bid_price                                                     0.1552\n",
    "        latest_ask_price                                                     0.1553\n",
    "        bid_price_mean                                                       0.1552\n",
    "        ask_price_mean                                                       0.1553\n",
    "        used_bid_price                                             latest_bid_price\n",
    "        used_ask_price                                             latest_ask_price\n",
    "        exchange_timestamp                         2023-08-29 12:34:58.561000+00:00\n",
    "        knowledge_timestamp                        2023-08-29 12:34:58.710276+00:00\n",
    "        end_download_timestamp                     2023-08-29 12:34:58.685410+00:00\n",
    "        limit_price                                                        0.155255\n",
    "        ccxt_id                                                         12049653207\n",
    "        name                                                                      9\n",
    "        Name: 9, dtype: object\n",
    "    :return: DataFrame of submission event timestamps and related prices, e.g.:\n",
    "                                                                            timestamp  limit_price  signed_quantity  snapshot_bid_price  snapshot_ask_price\n",
    "    event\n",
    "    knowledge_timestamp                                2023-09-01 10:34:58.050280+00:00          NaN              NaN              0.1582              0.1583\n",
    "    _submit_twap_child_order::child_order.limit_pri... 2023-09-01 10:35:00.803694+00:00     0.158245            160.0                 NaN                 NaN\n",
    "    _submit_twap_child_order::child_order.submitted    2023-09-01 10:35:04.905545+00:00          NaN              NaN                 NaN                 NaN\n",
    "    end_timestamp                                             2023-09-01 10:36:00+00:00          NaN              NaN                 NaN                 NaN\n",
    "    \"\"\"\n",
    "    # Get data and child order timestamps.\n",
    "    events = child_order[\n",
    "        [\n",
    "            \"start_timestamp\",\n",
    "            \"end_timestamp\",\n",
    "            \"exchange_timestamp\",\n",
    "            \"end_download_timestamp\",\n",
    "            \"knowledge_timestamp\",\n",
    "        ]\n",
    "    ]\n",
    "    events = pd.to_datetime(events)\n",
    "    # Get timestamps of order submission events.\n",
    "    stats = child_order[\"extra_params\"][\"stats\"].copy()\n",
    "    # Remove non-timestamp stats not related to time profiling.\n",
    "    # The non-timestamp stats include:\n",
    "    # `wave_id` (`int`);\n",
    "    # `attempt_num` (`int`);\n",
    "    # `exception_on_retry` (`str`, optional for cases when order was not submitted).\n",
    "    for key in list(stats):\n",
    "        if not isinstance(stats[key], pd.Timestamp):\n",
    "            stats.pop(key)\n",
    "            _LOG.debug(\"Popped %s from `stats`\", key)\n",
    "    stats_events = pd.Series(stats.keys(), stats.values(), name=\"event\")\n",
    "    stats_events = stats_events.tz_convert(\"UTC\")\n",
    "    stats_events = pd.Series(\n",
    "        stats_events.index, stats_events.values, name=\"timestamp\"\n",
    "    )\n",
    "    # Combine into a DataFrame.\n",
    "    all_events = (\n",
    "        pd.concat([events, stats_events]).sort_values().rename(\"timestamp\")\n",
    "    )\n",
    "    all_events = all_events.to_frame()\n",
    "    # Add order and order book data.\n",
    "    limit_price = pd.Series(\n",
    "        child_order[\"limit_price\"],\n",
    "        [\"_submit_twap_child_order::child_order.limit_price_calculated\"],\n",
    "        name=\"limit_price\",\n",
    "    )\n",
    "    signed_quantity = pd.Series(\n",
    "        child_order[\"diff_num_shares\"],\n",
    "        [\"_submit_twap_child_order::child_order.limit_price_calculated\"],\n",
    "        name=\"signed_quantity\",\n",
    "    )\n",
    "    snapshot_bid_price = pd.Series(\n",
    "        child_order[\"latest_bid_price\"],\n",
    "        [\"knowledge_timestamp\"],\n",
    "        name=\"snapshot_bid_price\",\n",
    "    )\n",
    "    snapshot_ask_price = pd.Series(\n",
    "        child_order[\"latest_ask_price\"],\n",
    "        [\"knowledge_timestamp\"],\n",
    "        name=\"snapshot_ask_price\",\n",
    "    )\n",
    "    prices = pd.concat(\n",
    "        [limit_price, signed_quantity, snapshot_bid_price, snapshot_ask_price],\n",
    "        axis=1,\n",
    "    )\n",
    "    df = pd.concat([all_events, prices], axis=1)\n",
    "    df.index.name = \"event\"\n",
    "    #\n",
    "    # Combine event timestamps and price data into a single DataFrame.\n",
    "    # Currently we are subselecting only certain columns for analysis.\n",
    "    # Other columns available are:\n",
    "    #  \"exchange_timestamp\",\n",
    "    #  \"end_download_timestamp\",\n",
    "    #  \"_submit_twap_child_order::bid_ask_market_data.start\",\n",
    "    #  \"_submit_twap_child_order::bid_ask_market_data.done\",\n",
    "    #  \"_submit_twap_child_order::get_open_positions.done\",\n",
    "    #  \"_submit_twap_child_order::child_order.created\",\n",
    "    #  \"_submit_single_order_to_ccxt_with_retry::start.timestamp\",\n",
    "    #  \"_submit_single_order_to_ccxt_with_retry::end.timestamp\"\n",
    "    execution_timestamps_and_prices = df.loc[\n",
    "        [\n",
    "            \"knowledge_timestamp\",\n",
    "            \"_submit_twap_child_order::child_order.limit_price_calculated\",\n",
    "            \"_submit_twap_child_order::child_order.submitted\",\n",
    "            \"end_timestamp\",\n",
    "        ]\n",
    "    ]\n",
    "    return execution_timestamps_and_prices\n",
    "\n",
    "\n",
    "def process_child_order_trades_timestamps(\n",
    "    fills_df: pd.DataFrame, ccxt_id: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process timestamp and price of trades related to a single child order.\n",
    "\n",
    "    :param fills_df: output of `load_ccxt_trades_df`\n",
    "    :param ccxt_id: ccxt_id of the child order\n",
    "    :return: timestamp, price and signed quantity of trades for a given order\n",
    "    Example:\n",
    "\n",
    "                                    timestamp  limit_price  signed_quantity\n",
    "    event\n",
    "    trade.1 2023-09-01 10:35:23.508000+00:00       0.1582            160.0\n",
    "    \"\"\"\n",
    "    # Add timestamps for trades.\n",
    "    # TODO(Danya): CMTask5287.\n",
    "    child_order_trades = fills_df.loc[fills_df[\"order\"] == ccxt_id]\n",
    "    if not child_order_trades.empty:\n",
    "        trade_timestamps = []\n",
    "        limit_prices = []\n",
    "        signed_quantities = []\n",
    "        # Process trades for the given child order.\n",
    "        for trade_n in range(len(child_order_trades)):\n",
    "            trade = child_order_trades.iloc[trade_n]\n",
    "            # Get timestamp of a trade.\n",
    "            trade_timestamp = pd.Series(\n",
    "                trade.timestamp, [f\"trade.{trade_n+1}\"], name=\"timestamp\"\n",
    "            )\n",
    "            trade_timestamps.append(trade_timestamp)\n",
    "            # Get price of a trade\n",
    "            limit_price = pd.Series(\n",
    "                trade.price, [f\"trade.{trade_n+1}\"], name=\"limit_price\"\n",
    "            )\n",
    "            limit_prices.append(limit_price)\n",
    "            # Get quanitity of a trade.\n",
    "            signed_quantity = trade.amount\n",
    "            if trade.side == \"sell\":\n",
    "                signed_quantity = -signed_quantity\n",
    "            signed_quantity = pd.Series(\n",
    "                signed_quantity, [f\"trade.{trade_n+1}\"], name=\"signed_quantity\"\n",
    "            )\n",
    "            signed_quantities.append(signed_quantity)\n",
    "        # Combine into a single DataFrame.\n",
    "        trade_timestamps = pd.concat(trade_timestamps)\n",
    "        limit_prices = pd.concat(limit_prices)\n",
    "        signed_quantities = pd.concat(signed_quantities)\n",
    "        #\n",
    "        trade_timestamps = pd.concat(\n",
    "            [trade_timestamps, limit_prices, signed_quantities], axis=1\n",
    "        )\n",
    "        trade_timestamps.index.name = \"event\"\n",
    "    else:\n",
    "        # Return an empty DataFrame if no related trades are found.\n",
    "        trade_timestamps = pd.DataFrame()\n",
    "    return trade_timestamps\n",
    "\n",
    "\n",
    "def process_child_order_timestamps_and_prices_for_single_asset(\n",
    "    oms_child_order_df: pd.DataFrame,\n",
    "    fills_df: pd.DataFrame,\n",
    "    asset_id: int,\n",
    "    resample_freq: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply child order and trade timestamp processing to multiple child orders\n",
    "    belonging a single asset.\n",
    "\n",
    "    :param oms_child_order_df: OMS child order DataFrame\n",
    "    :param asset_id: asset_id to apply the processing to\n",
    "    :param resample_freq: see `process_child_order_timestamps` docstring\n",
    "    :return: timestamps and prices for a single asset, e.g.:\n",
    "                                                                                    event limit_price signed_quantity snapshot_ask_price snapshot_bid_price\n",
    "                                                                            1467591036  1467591036      1467591036         1467591036         1467591036\n",
    "    timestamp\n",
    "    2023-09-01 10:34:58.100000+00:00                                knowledge_timestamp         NaN             NaN            26000.0            25999.9\n",
    "    2023-09-01 10:35:00.800000+00:00  _submit_twap_child_order::child_order.limit_pr...   25999.955          -0.001                NaN                NaN\n",
    "    2023-09-01 10:35:04.900000+00:00    _submit_twap_child_order::child_order.submitted         NaN             NaN                NaN                NaN\n",
    "    2023-09-01 10:35:12.200000+00:00                                            trade.1   26000.000          -0.001                NaN                NaN\n",
    "    2023-09-01 10:36:00+00:00                                             end_timestamp         NaN             NaN                NaN                NaN\n",
    "    2023-09-01 10:36:00+00:00                                       knowledge_timestamp         NaN             NaN            25996.9            25996.8\n",
    "    2023-09-01 10:36:02.300000+00:00  _submit_twap_child_order::child_order.limit_pr...   25996.855          -0.001                NaN                NaN\n",
    "    2023-09-01 10:36:02.600000+00:00    _submit_twap_child_order::child_order.submitted         NaN             NaN                NaN                NaN\n",
    "    2023-09-01 10:36:20.700000+00:00                                            trade.1   25996.900          -0.001                NaN                NaN\n",
    "    2023-09-01 10:36:58.300000+00:00                                knowledge_timestamp         NaN             NaN            25998.7            25998.6\n",
    "    \"\"\"\n",
    "    oms_child_order_df = oms_child_order_df[\n",
    "        oms_child_order_df[\"asset_id\"] == asset_id\n",
    "    ]\n",
    "    processed_orders = []\n",
    "    for idx in oms_child_order_df.index:\n",
    "        child_order = oms_child_order_df.loc[idx]\n",
    "        processed_order = process_timestamps_and_prices_for_a_single_order(\n",
    "            child_order, fills_df, resample_freq\n",
    "        )\n",
    "        processed_orders.append(processed_order)\n",
    "    result = pd.concat(processed_orders, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ca2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
