<!--toc -->

# Amazon Elastic File System (EFS)

Elastic File System (EFS) is a scalable UNIX-compatible filesystem offering
automated growth and shrinking of persistent storage.

We currently use two separate EFSs, one dedicated to production and one
dedicated to development/pre-prod.

The advantage of EFS over S3 is that S3 is not a fully compatible filesystem (it only supports a subset of feature of a standard filesystem), one needs to build an interface in order to work with it or use a command line tool.


## Use cases (within our organization)

### Shared filesystem for developers

All dev servers have the pre-prod EFS mounted to facilitate collaboration,
simple file sharing, and moving between dev servers.
    - A developer can simply store results of an experiment in shared location so others can pick up the files for further analyses.
    - If a dev server crashes a developer/researcher can (partially) resume their work, if the data they need are stored in a shared location instead of their local filesystem.

### Airflow Pre-prod DAG storage

The Airflow pre-prod EC2 instance has the pre-prod EFS mounted and Airflow DAG
directory (the directory where Airflow looks for DAGs) pointed to a location on
the EFS. Using this technique developers can quickly deploy experimental DAGs.
A developer can simply access the EFS, add a new DAG to `/data/shared/airflow_preprod_new/dags` to test out and run it via
Airflow UI.

### Volume for container deployed via ECS

We run all of the workloads in containers encapsulated by an ECS task
definition. It is possible to add a volume to the container by specifying inside
the ECS task definition. This neat feature allows us to store the results of
experiments such as paper trading system run logs in an easy-to-access way.

## Data archival

Since we work with relatively large volumes of data, we transfer data older than
`n` days to a cheaper S3 storage

There is a DAG `prod.periodical_daily_archive_efs_to_s3` running once a day. Its
job is to archive all data generated by ECS to the backup bucket -
`s3://cryptokaizen-data-backup`. With proper data storage lifecycle rules we can
achieve almost an order of magnitude cheaper storage for older data, because of low-cost storage tiers supported by S3 (S3 glacier and its hybrids).
